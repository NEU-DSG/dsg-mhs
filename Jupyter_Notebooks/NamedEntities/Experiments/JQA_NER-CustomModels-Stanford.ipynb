{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Custom Stanford NER Model\n",
    "\n",
    "#### Sources\n",
    "\n",
    "\n",
    "Bochet, Charles, “[Python: How to Train your Own Model with NLTK and Stanford NER \n",
    "Tagger?](https://www.sicara.ai/blog/2018-04-25-python-train-model-NTLK-stanford-ner-tagger),” <i>Sicara</i>, Accessed 10/16/2020.\n",
    "\n",
    "“DataTurks,” “[Stanford CoreNLP: Training your own custom NER tagger](https://medium.com/swlh/stanford-corenlp-training-your-own-custom-ner-tagger-8119cc7dfc06),” <i>medium</i>, Accessed 10/26/2020.\n",
    "\n",
    "Christina, \"[Named Entity Recognition in Python with Stanford-NER and Spacy](https://lvngd.com/blog/named-entity-recognition-in-python-with-stanford-ner-and-spacy/),\" <i>LVNG</i>, Accessed 10/26/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, glob, random,  csv, sys, os, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 258 ms, sys: 3.68 ms, total: 261 ms\n",
      "Wall time: 263 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>text</th>\n",
       "      <th>element</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-01</td>\n",
       "      <td>1 V:15. Tuesday. W. A. Schoolfield at the Offi...</td>\n",
       "      <td>persName</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>Adams</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>A. Gallatin</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>La Forêt</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>Canning</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file                      entry  \\\n",
       "0  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-01   \n",
       "1  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "2  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "3  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "4  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "\n",
       "                                                text   element  \\\n",
       "0  1 V:15. Tuesday. W. A. Schoolfield at the Offi...  persName   \n",
       "1  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "2  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "3  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "4  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "\n",
       "              entity   label  \n",
       "0  W. A. Schoolfield  PERSON  \n",
       "1              Adams  PERSON  \n",
       "2        A. Gallatin  PERSON  \n",
       "3           La Forêt  PERSON  \n",
       "4            Canning  PERSON  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Declare regex to simplify file paths below\n",
    "regex = re.compile(r'.*/(.*).xml')\n",
    "\n",
    "# Get plain text of every element (designated by first argument).\n",
    "def get_textContent(ancestor, xpath_as_string, namespace):\n",
    "    text_list = []\n",
    "    for elem in ancestor.findall(xpath_as_string, namespace):\n",
    "        text = ''.join(ET.tostring(elem, encoding='unicode', method='text'))\n",
    "\n",
    "#         Add text (cleaned of additional whitespace) to text_list.\n",
    "        text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "\n",
    "#     Return concetanate text list.\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "\n",
    "# Choose either all .xml files or training set by select dataset = 'all' or 'training'.\n",
    "# Selection will parse the XML for different elements.\n",
    "\n",
    "# dataset = 'all'\n",
    "dataset = 'training'\n",
    "\n",
    "# Conditionally choose directory and create dataframe.\n",
    "if dataset == 'all':\n",
    "    # Gather all .xml files using glob.\n",
    "    list_of_files = glob.glob(abs_dir + \"Data/JQA/*/*.xml\")\n",
    "    \n",
    "    # Create dataframe to store results.\n",
    "    data = pd.DataFrame(columns = ['file', 'entry', 'text',\n",
    "                                       'element', 'refKey', 'entity'])\n",
    "\n",
    "elif dataset == \"training\":\n",
    "    # Or, use training document(s) alone.\n",
    "    list_of_files = glob.glob(abs_dir + \"Data/TestEncoding/TrainingData/*.xml\")\n",
    "    \n",
    "    # Create dataframe to store results.\n",
    "    data = pd.DataFrame(columns = ['file', 'entry', 'text',\n",
    "                                       'element', 'entity'])\n",
    "\n",
    "else:\n",
    "    print ('Dataset not found.')\n",
    "\n",
    "    \n",
    "# Loop through each file within a directory.\n",
    "for file in list_of_files:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    reFile = str(regex.match(file).group(1))\n",
    "    \n",
    "    for eachDoc in root.findall('.//ns:div/[@type=\"entry\"]', ns):\n",
    "        entry = eachDoc.get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "        text = get_textContent(eachDoc, './ns:div/[@type=\"docbody\"]/ns:p', ns)\n",
    "        \n",
    "        if dataset == 'all':\n",
    "            for elem in eachDoc.findall('.//ns:p/ns:persRef/[@ref]', ns):\n",
    "                name = elem.text\n",
    "                try:\n",
    "                    entity = re.sub(r'\\s+', ' ', name)\n",
    "                except TypeError:\n",
    "                    entity = name\n",
    "\n",
    "                data = data.append({'file':reFile,\n",
    "                                'entry':entry,\n",
    "                                'text':text,\n",
    "                                'element':re.sub(r'.*}(.*)', '\\\\1', elem.tag),\n",
    "                                'refKey':elem.get('ref'),\n",
    "                                'entity':entity},\n",
    "                               ignore_index = True)\n",
    "\n",
    "        elif dataset == 'training':\n",
    "            for xpath in ['.//ns:p//ns:persName', './/ns:p//ns:placeName']:\n",
    "                for elem in eachDoc.findall(xpath, ns):\n",
    "                    name = elem.text\n",
    "                    try:\n",
    "                        entity = re.sub(r'\\s+', ' ', name)\n",
    "                    except TypeError:\n",
    "                        entity = name\n",
    "\n",
    "                    data = data.append({'file':reFile,\n",
    "                                    'entry':entry,\n",
    "                                    'text':text,\n",
    "                                    'element':re.sub(r'.*}(.*)', '\\\\1', elem.tag),\n",
    "                                    'entity':entity},\n",
    "                                   ignore_index = True)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print ('Selected dataset not found.')\n",
    "        \n",
    "\n",
    "        \n",
    "# Create a dictionary to change element tags to NER labels.\n",
    "element_ner_dictionary = {'persName':'PERSON', 'placeName':'LOC'}\n",
    "\n",
    "# Change elements to NER labels.\n",
    "labels_for_sentences = (data['element'].map(element_ner_dictionary))\n",
    "\n",
    "# Attach labels as a column.\n",
    "data['label'] = labels_for_sentences\n",
    "                \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Entries and Shape Data for Custom Model\n",
    "\n",
    "Custom Model should be a tab-separated file (.tsv) with a token column and NER label column (no header).\n",
    "\n",
    "|No Header | No Header|\n",
    "|----------|----------|\n",
    "|En |O|\n",
    "|2017 |DATE|\n",
    "|, |O|\n",
    "|Une |O|\n",
    "|intelligence |O|\n",
    "|artificielle |O|\n",
    "|est |O|\n",
    "|en |O|\n",
    "|mesure |O|\n",
    "|de |O|\n",
    "|développer |O|\n",
    "|par |O|\n",
    "|elle-même |O|\n",
    "|Super |PERSON|\n",
    "|Mario |PERSON|\n",
    "|Bros |PERSON|\n",
    "|. |O |\n",
    "\n",
    "For initials and titles ('W. A. Schoolfield'), one row might be ('W. PERSON'). The example file provided by Stanford is [here](https://nlp.stanford.edu/software/crf-faq.shtml#b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.4 ms, sys: 2.44 ms, total: 86.8 ms\n",
      "Wall time: 85.7 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V:15.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuesday.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W.</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text   label\n",
       "0         1       0\n",
       "0     V:15.       0\n",
       "0  Tuesday.       0\n",
       "0        W.  PERSON\n",
       "0        A.  PERSON"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens = data\n",
    "\n",
    "# Function to link entity-tree (e.g., first name & last name) with underscores.\n",
    "def link_entity_trees(text_column, entity_column):\n",
    "    sentence = text_column\n",
    "    if entity_column in sentence:\n",
    "        re_entity = re.sub('\\s', '_', entity_column)\n",
    "        sentence = re.sub(entity_column, re_entity, sentence)\n",
    "    return sentence\n",
    "\n",
    "# Apply link_entities() to text column & tokenize text.\n",
    "tokens['text'] = tokens \\\n",
    "    .apply(lambda row: link_entity_trees(row['text'], row['entity']), axis = 1) \\\n",
    "    .str.split(' ')\n",
    "\n",
    "# Unnest text column.\n",
    "tokens = tokens.explode('text')\n",
    "\n",
    "# Replace underscores with whitespace to match 'entity' column.\n",
    "tokens['text'] = tokens['text'].str.replace('_', ' ')\n",
    "\n",
    "# Define function to properly label PERSONs and LOCs.\n",
    "def correct_entity_label(text_column, entity_column, label_column):\n",
    "    if text_column != entity_column:\n",
    "        label = '0'\n",
    "    else:\n",
    "        label = label_column\n",
    "    return label\n",
    "\n",
    "tokens['label'] = tokens \\\n",
    "    .apply(lambda row: correct_entity_label(row['text'], row['entity'], row['label']),\n",
    "           axis = 1)\n",
    "\n",
    "# Further tokenize to match example data.\n",
    "tokens['text'] = tokens['text'].str.split(' ')\n",
    "tokens = tokens.explode('text')\n",
    "\n",
    "# Remove whitespace and rows with only whitespace.\n",
    "tokens['text'] = tokens['text'].str.replace('\\s+', '') \\\n",
    "    .replace('', np.nan, regex = True)\n",
    "\n",
    "tokens = tokens.dropna()\n",
    "\n",
    "# Subset dataframe by columns\n",
    "tokens = tokens[['text', 'label']]\n",
    "\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus and Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 ms, sys: 1.87 ms, total: 5.79 ms\n",
      "Wall time: 5.85 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create variable for file path.\n",
    "PATH_TO_STANFORD_FOLDER = \"/Users/quinn.wi/stanfordNLP/stanford-ner-4.0.0/custom-models/\"\n",
    "\n",
    "# Write training corpus to folder.\n",
    "tokens.to_csv(PATH_TO_STANFORD_FOLDER + 'jqa-ner-corpus.tsv',\n",
    "              sep = '\\t', index = False, header = False)\n",
    "\n",
    "\n",
    "# Save model parameters/properties as txt.\n",
    "params = \"\"\"trainFile = custom-models/jqa-ner-corpus.tsv\n",
    "serializeTo = jqa-ner-model.ser.gz\n",
    "map = word=0,answer=1\n",
    "\n",
    "useClassFeature=true\n",
    "useWord=true\n",
    "useNGrams=true\n",
    "noMidNGrams=true\n",
    "maxNGramLeng=6\n",
    "usePrev=true\n",
    "useNext=true\n",
    "useSequences=true\n",
    "usePrevSequences=true\n",
    "maxLeft=1\n",
    "useTypeSeqs=true\n",
    "useTypeSeqs2=true\n",
    "useTypeySequences=true\n",
    "wordShape=chris2useLC\n",
    "useDisjunctive=true\"\"\"\n",
    "\n",
    "with open(PATH_TO_STANFORD_FOLDER + \"jqa-prop.txt\", \"w\") as writeFile:\n",
    "    writeFile.write(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Custom Model\n",
    "\n",
    "To train custom model, write following code in Terminal\n",
    "\n",
    "```code\n",
    "cd stanfordNLP/stanford-ner-4.0.0 [from Home directory]\n",
    "\n",
    "java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop custom-models/jqa-prop.txt\n",
    "```\n",
    "\n",
    "File path must be correct: https://stackoverflow.com/questions/59681871/customized-stanfordner\n",
    "\n",
    "In order to call custom model in Python (NLTK):\n",
    "\n",
    "```python\n",
    "jar = './stanford-ner-4.0.0/stanford-ner.jar'\n",
    "model = './stanford-ner-4.0.0/jqa-ner-model.ser'\n",
    "\n",
    "ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
