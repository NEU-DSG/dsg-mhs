{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Encoded & Found (NER) Entities\n",
    "\n",
    "Joining NER with parsed XML to create list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, warnings, csv, sys, os, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\"\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in Data and Find Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 1.65 s, total: 1min 9s\n",
      "Wall time: 1min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>date</th>\n",
       "      <th>refKey</th>\n",
       "      <th>found_entities</th>\n",
       "      <th>encoded_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>herkimer john</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>adams parmenio</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>ketchum unknown</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>meyer unknown3</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>adams daniel</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file                      entry  \\\n",
       "9  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "9  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "9  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "9  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "9  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "\n",
       "         date           refKey  \\\n",
       "9  1825-01-10    herkimer john   \n",
       "9  1825-01-10   adams parmenio   \n",
       "9  1825-01-10  ketchum unknown   \n",
       "9  1825-01-10   meyer unknown3   \n",
       "9  1825-01-10     adams daniel   \n",
       "\n",
       "                                      found_entities  \\\n",
       "9  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "9  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "9  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "9  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "9  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "\n",
       "                                    encoded_entities  \n",
       "9  herkimer-john,adams-parmenio,ketchum-unknown,m...  \n",
       "9  herkimer-john,adams-parmenio,ketchum-unknown,m...  \n",
       "9  herkimer-john,adams-parmenio,ketchum-unknown,m...  \n",
       "9  herkimer-john,adams-parmenio,ketchum-unknown,m...  \n",
       "9  herkimer-john,adams-parmenio,ketchum-unknown,m...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read in file; select columns; drop rows with NA values (entries without a named person).\n",
    "df = pd.read_csv(abs_dir + 'Output/ParsedXML/JQA_dataframe.txt',\n",
    "                 sep = '\\t') \\\n",
    "    .dropna() \\\n",
    "    .rename(columns = {'people':'refKey'})\n",
    "\n",
    "# Add or substract labels to list for NER to find.\n",
    "label_list = ['PERSON', 'LOC']\n",
    "\n",
    "# spaCy\n",
    "def get_spacy_entities(text, label_list):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in label_list:\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "# Apply function to find spaCy-based entities.\n",
    "df['found_entities'] = df.apply(lambda row: get_spacy_entities(row['text'], label_list),\n",
    "                                axis = 1)\n",
    "\n",
    "# Keep encoded entities as a list for matching further down.\n",
    "df['encoded_entities'] = df['refKey']\n",
    "\n",
    "# Split string of people into individuals.\n",
    "df['refKey'] = df['refKey'].str.split(r',|;')\n",
    "\n",
    "# Explode list so that each list value becomes a row.\n",
    "df = df.explode('refKey')\n",
    "\n",
    "# Clean up entities (refKeys) and found entities.\n",
    "df['refKey'] = df['refKey'].str.replace('-', ' ')\n",
    "\n",
    "# Lowercase found entity (i) in each tuple (x) in 'found_entity' column.\n",
    "df['found_entities'] = df['found_entities'].apply(lambda x: [(i.lower(), j) for i,j in x])\n",
    "\n",
    "# Drop 'text' column to reduce dataframe size.\n",
    "df = df.drop(columns = ['text'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine & Conflate Encoded-Found Entity Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 16s, sys: 9.15 s, total: 6min 25s\n",
      "Wall time: 6min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>date</th>\n",
       "      <th>refKey</th>\n",
       "      <th>found_entities</th>\n",
       "      <th>encoded_entities</th>\n",
       "      <th>matches</th>\n",
       "      <th>added_entity</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>match_quality</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>herkimer john</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "      <td>(john-herkimer, PERSON, 100, match)</td>\n",
       "      <td>john-herkimer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>100.0</td>\n",
       "      <td>match</td>\n",
       "      <td>herkimer john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>adams parmenio</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "      <td>(parmenio-adams-members-h.r., PERSON, 100, match)</td>\n",
       "      <td>parmenio-adams-members-h.r.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>100.0</td>\n",
       "      <td>match</td>\n",
       "      <td>adams parmenio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>ketchum unknown</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "      <td>(a-mr-ketchum, PERSON, 64, match)</td>\n",
       "      <td>a-mr-ketchum</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>64.0</td>\n",
       "      <td>match</td>\n",
       "      <td>ketchum unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>meyer unknown3</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "      <td>(meyer, PERSON, 62, match)</td>\n",
       "      <td>meyer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>62.0</td>\n",
       "      <td>match</td>\n",
       "      <td>meyer unknown3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('JQADiaries-v49-1825-01-p795.xml',)</td>\n",
       "      <td>jqadiaries-v49-1825-01-10</td>\n",
       "      <td>1825-01-10</td>\n",
       "      <td>adams daniel</td>\n",
       "      <td>[(john herkimer, PERSON), (parmenio adams memb...</td>\n",
       "      <td>herkimer-john,adams-parmenio,ketchum-unknown,m...</td>\n",
       "      <td>(parmenio-adams-members-h.r., PERSON, 59, match)</td>\n",
       "      <td>parmenio-adams-members-h.r.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>59.0</td>\n",
       "      <td>match</td>\n",
       "      <td>adams daniel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file                      entry  \\\n",
       "0  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "1  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "2  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "3  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "4  ('JQADiaries-v49-1825-01-p795.xml',)  jqadiaries-v49-1825-01-10   \n",
       "\n",
       "         date           refKey  \\\n",
       "0  1825-01-10    herkimer john   \n",
       "1  1825-01-10   adams parmenio   \n",
       "2  1825-01-10  ketchum unknown   \n",
       "3  1825-01-10   meyer unknown3   \n",
       "4  1825-01-10     adams daniel   \n",
       "\n",
       "                                      found_entities  \\\n",
       "0  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "1  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "2  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "3  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "4  [(john herkimer, PERSON), (parmenio adams memb...   \n",
       "\n",
       "                                    encoded_entities  \\\n",
       "0  herkimer-john,adams-parmenio,ketchum-unknown,m...   \n",
       "1  herkimer-john,adams-parmenio,ketchum-unknown,m...   \n",
       "2  herkimer-john,adams-parmenio,ketchum-unknown,m...   \n",
       "3  herkimer-john,adams-parmenio,ketchum-unknown,m...   \n",
       "4  herkimer-john,adams-parmenio,ketchum-unknown,m...   \n",
       "\n",
       "                                             matches  \\\n",
       "0                (john-herkimer, PERSON, 100, match)   \n",
       "1  (parmenio-adams-members-h.r., PERSON, 100, match)   \n",
       "2                  (a-mr-ketchum, PERSON, 64, match)   \n",
       "3                         (meyer, PERSON, 62, match)   \n",
       "4   (parmenio-adams-members-h.r., PERSON, 59, match)   \n",
       "\n",
       "                  added_entity ner_label  confidence match_quality  \\\n",
       "0                john-herkimer    PERSON       100.0         match   \n",
       "1  parmenio-adams-members-h.r.    PERSON       100.0         match   \n",
       "2                 a-mr-ketchum    PERSON        64.0         match   \n",
       "3                        meyer    PERSON        62.0         match   \n",
       "4  parmenio-adams-members-h.r.    PERSON        59.0         match   \n",
       "\n",
       "             label  \n",
       "0    herkimer john  \n",
       "1   adams parmenio  \n",
       "2  ketchum unknown  \n",
       "3   meyer unknown3  \n",
       "4     adams daniel  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create function to find matches and their confidence scores.\n",
    "def get_matches_and_quality(entity_string, entity_list):\n",
    "    match = process.extract(entity_string,\n",
    "                            entity_list,\n",
    "                            limit = 1,\n",
    "                            scorer = fuzz.token_set_ratio)\n",
    "    refKey = re.sub(' ', '-', match[0][0][0])\n",
    "    label = match[0][0][1]\n",
    "    confidence = match[0][1]\n",
    "    return (refKey, label, confidence)\n",
    "\n",
    "# Determining whether an encoded entity matches a found one and vice-versa.\n",
    "# 1. Function to return an encoded entity and its best match, then declare quality of match.\n",
    "# 2. (Inverse) function to find if a found entity has no matches (and therefore is new data).\n",
    "\n",
    "# 1.\n",
    "def get_entity2found_matches(column_one, column_two, confidence_threshold):\n",
    "    try:\n",
    "        match = get_matches_and_quality(column_one, column_two)\n",
    "\n",
    "        refKey = re.sub(' ', '-', match[0])\n",
    "        label = match[1]\n",
    "        confidence = match[2]\n",
    "        \n",
    "        if confidence >= confidence_threshold:\n",
    "            match_quality = 'match'\n",
    "        else:\n",
    "            match_quality = 'only_encoded'\n",
    "\n",
    "        return (refKey, label, confidence, match_quality)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "matches_df = df.assign(matches = df \\\n",
    "    .apply(lambda row: get_entity2found_matches(row['refKey'],\n",
    "                                                row['found_entities'], 50), axis = 1)\n",
    "                   )\n",
    "\n",
    "# Untuple Results.\n",
    "# Matched_entity should not declare matched; new isn't true either; added_entity?\n",
    "matches_df[['added_entity', 'ner_label', 'confidence', 'match_quality']] = pd \\\n",
    "    .DataFrame(matches_df['matches'].values.tolist(),\n",
    "               index = matches_df.index)\n",
    "\n",
    "# 2. \n",
    "def get_found2entity_matches(column_one, column_two, confidence_threshold):\n",
    "    for entity in column_one:\n",
    "\n",
    "        refKey = re.sub(' ', '-', entity[0])\n",
    "        label = entity[1]\n",
    "        confidence = fuzz.token_sort_ratio(entity[0], column_two)\n",
    "        \n",
    "        if confidence >= confidence_threshold:\n",
    "            continue\n",
    "        else:\n",
    "            match_quality = 'only_found'\n",
    "            return (refKey, label, confidence, match_quality)\n",
    "\n",
    "# Unlike previous function, this inverse function needs to add rows (rather than columns).\n",
    "for index, row in matches_df.iterrows():\n",
    "    match = get_found2entity_matches(row['found_entities'], row['refKey'], 50)\n",
    "    \n",
    "    if match is not None:\n",
    "        refKey = re.sub(' ', '-', match[0])\n",
    "        label = match[1]\n",
    "        confidence = match[2]\n",
    "        match_quality = match[3]\n",
    "\n",
    "        matches_df = matches_df.append({'file':row['file'], 'entry':row['entry'],\n",
    "                                        'date':row['date'], 'refKey':row['refKey'],\n",
    "                                        'found_entities':row['found_entities'],\n",
    "                                        'encoded_entities':row['encoded_entities'],\n",
    "                                        'matches':row['matches'],\n",
    "                                        'added_entity':refKey,\n",
    "                                        'ner_label':label,\n",
    "                                        'confidence':confidence,\n",
    "                                        'match_quality': match_quality},\n",
    "                                       ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "        \n",
    "# Converge 'refKey' and 'added_entity' based on 'match_quality'.\n",
    "# If 'match_quality' equals 'match,' then refKey becomes the referent (to disambiguate).\n",
    "# The same is true if 'match_quality' equals 'only_encoded'.\n",
    "# If 'match_quality' equal 'not_match,' then 'added_entity' becomes the referent.\n",
    "def converge_entities(match_quality, refKey_column, added_entity_column):\n",
    "    if match_quality == 'match' or match_quality == 'only_encoded':\n",
    "        value = refKey_column\n",
    "    else:\n",
    "        value = added_entity_column\n",
    "    return value\n",
    "\n",
    "# Create new column, 'referent'\n",
    "# Selects appropriate choice between refKey and added_entity to be referent.\n",
    "matches_df['label'] = matches_df.apply(lambda row: converge_entities(row['match_quality'],\n",
    "                                                                     row['refKey'],\n",
    "                                                                     row['added_entity']),\n",
    "                                       axis = 1)\n",
    "\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Subset data by columns.\n",
    "# corr_df = matches_df[['entry', 'label']]\n",
    "\n",
    "# # Create entry-person matrix.\n",
    "# corr_df = pd.crosstab(corr_df['entry'], corr_df['label'])\n",
    "\n",
    "# # Convert entry-person matrix into an adjacency matrix of persons.\n",
    "# corr_df = corr_df.T.dot(corr_df)\n",
    "\n",
    "# # Change diagonal values to zero. That is, a person cannot co-occur with themself.\n",
    "# np.fill_diagonal(corr_df.values, 0)\n",
    "\n",
    "# # Simple correlation matrix from dataframe.\n",
    "# corr_df = corr_df.corr()\n",
    "\n",
    "# # Create new 'source' column that corresponds to index (person).\n",
    "# corr_df['label_src'] = corr_df.index\n",
    "\n",
    "# # Reshape dataframe to focus on source, target, and weight.\n",
    "# # Remove same-person pairs (weight = 1) and low correlations (weight >= 0.7).\n",
    "# # 0.4 Correlation Coefficient (weigh) considered 'moderate' in Dancey & Reidy (psychology)\n",
    "# # and 'strong' in Quinnipiac Univeristy (politics).\n",
    "# corr_df = pd.melt(corr_df, id_vars = ['label_src'], value_name = 'weight') \\\n",
    "#     .query('(weight < 1.00) & (weight >= 0.4)')  \\\n",
    "#     .rename(columns = {'label':'target', 'label_src':'label'})\n",
    "\n",
    "# # Rejoin source with its ner label.\n",
    "# corr_df = corr_df \\\n",
    "#     .merge(matches_df[['label', 'ner_label', 'match_quality']],\n",
    "#            on = 'label', how = 'left') \\\n",
    "#     .drop_duplicates()\n",
    "\n",
    "\n",
    "# print (corr_df.shape)\n",
    "# corr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data to Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Create list of unique entities from source and target columns.\n",
    "# nodes = pd.DataFrame(corr_df['label'].values.tolist() + corr_df['target'].values.tolist()) \\\n",
    "#     .rename(columns = {0:'label'}) \\\n",
    "#     .drop_duplicates()\n",
    "\n",
    "# # Create identifying codes for labels.\n",
    "# nodes = nodes \\\n",
    "#     .assign(source = nodes['label'].astype('category').cat.codes) \\\n",
    "#     .dropna() \\\n",
    "#     .sort_values(['source'], ascending = True) # Sorting matches labels with source codes.\n",
    "\n",
    "# # Create dictionary to map values to codes.\n",
    "# nodes_dictionary = nodes.set_index('label')['source'].to_dict()\n",
    "\n",
    "# # Create links dataframe and map links to nodes' codes.\n",
    "# links = corr_df \\\n",
    "#     .assign(source = corr_df['label'].map(nodes_dictionary),\n",
    "#             target = corr_df['target'].map(nodes_dictionary))\n",
    "\n",
    "# # Add data to nodes dataframe.\n",
    "# nodes = nodes.merge(links[['label', 'ner_label', 'match_quality']],\n",
    "#            on = 'label', how = 'left') \\\n",
    "#     .drop_duplicates()\n",
    "\n",
    "# print (links.shape)\n",
    "# links.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 72.5 ms, total: 1.62 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "matches_df.to_csv(abs_dir + \"Output/Graphs/JQA_Network_mergedEntities-correlation/network-mergedEntities.csv\",\n",
    "             sep = ',', index = False)\n",
    "\n",
    "# nodes.to_csv(abs_dir + \"Output/Graphs/JQA_Network_mergedEntities-correlation/nodes.csv\",\n",
    "#              sep = ',', index = False)\n",
    "\n",
    "# links.to_csv(abs_dir + \"Output/Graphs/JQA_Network_mergedEntities-correlation/links.csv\",\n",
    "#              sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
