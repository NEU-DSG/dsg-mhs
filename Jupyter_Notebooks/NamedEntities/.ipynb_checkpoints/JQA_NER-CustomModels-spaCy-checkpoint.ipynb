{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Custom spaCy NER Models\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Import parsed plain XML as dataframe.\n",
    "2. \"Senticize\" and tokenize texts.\n",
    "3. Count distance from beginning of sentence (in tokens).\n",
    "\n",
    "\n",
    "#### Sources\n",
    "\n",
    "Christina, \"[Named Entity Recognition in Python with Stanford-NER and Spacy](https://lvngd.com/blog/named-entity-recognition-in-python-with-stanford-ner-and-spacy/),\" <i>LVNG</i>, Accessed 10/26/2020.\n",
    "\n",
    "Nishanth, N. \"[Training Custom NER](https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7),\" <i>towards data science</i>, Accessed 10/26/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, glob, random, csv, sys, os, warnings\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser', 'tagger'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 2.72 ms, total: 252 ms\n",
      "Wall time: 255 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>text</th>\n",
       "      <th>element</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-01</td>\n",
       "      <td>1 V:15. Tuesday. W. A. Schoolfield at the Offi...</td>\n",
       "      <td>persName</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>Adams</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>A. Gallatin</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>La Forêt</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-02</td>\n",
       "      <td>2. VI: Mrs. Adams unwell. Despatches to A. Gal...</td>\n",
       "      <td>persName</td>\n",
       "      <td>Canning</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file                      entry  \\\n",
       "0  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-01   \n",
       "1  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "2  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "3  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "4  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-02   \n",
       "\n",
       "                                                text   element  \\\n",
       "0  1 V:15. Tuesday. W. A. Schoolfield at the Offi...  persName   \n",
       "1  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "2  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "3  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "4  2. VI: Mrs. Adams unwell. Despatches to A. Gal...  persName   \n",
       "\n",
       "              entity   label  \n",
       "0  W. A. Schoolfield  PERSON  \n",
       "1              Adams  PERSON  \n",
       "2        A. Gallatin  PERSON  \n",
       "3           La Forêt  PERSON  \n",
       "4            Canning  PERSON  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Declare regex to simplify file paths below\n",
    "regex = re.compile(r'.*/(.*).xml')\n",
    "\n",
    "# Get plain text of every element (designated by first argument).\n",
    "def get_textContent(ancestor, xpath_as_string, namespace):\n",
    "    text_list = []\n",
    "    for elem in ancestor.findall(xpath_as_string, namespace):\n",
    "        text = ''.join(ET.tostring(elem, encoding='unicode', method='text'))\n",
    "\n",
    "#         Add text (cleaned of additional whitespace) to text_list.\n",
    "        text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "\n",
    "#     Return concetanate text list.\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "\n",
    "# Choose either all .xml files or training set by select dataset = 'all' or 'training'.\n",
    "# Selection will parse the XML for different elements.\n",
    "\n",
    "# dataset = 'all'\n",
    "dataset = 'training'\n",
    "\n",
    "# Conditionally choose directory and create dataframe.\n",
    "if dataset == 'all':\n",
    "    # Gather all .xml files using glob.\n",
    "    list_of_files = glob.glob(abs_dir + \"Data/JQA/*/*.xml\")\n",
    "    \n",
    "    # Create dataframe to store results.\n",
    "    data = pd.DataFrame(columns = ['file', 'entry', 'text',\n",
    "                                       'element', 'refKey', 'entity'])\n",
    "\n",
    "elif dataset == \"training\":\n",
    "    # Or, use training document(s) alone.\n",
    "    list_of_files = glob.glob(abs_dir + \"Data/TestEncoding/TrainingData/*.xml\")\n",
    "    \n",
    "    # Create dataframe to store results.\n",
    "    data = pd.DataFrame(columns = ['file', 'entry', 'text',\n",
    "                                       'element', 'entity'])\n",
    "\n",
    "else:\n",
    "    print ('Dataset not found.')\n",
    "\n",
    "    \n",
    "# Loop through each file within a directory.\n",
    "for file in list_of_files:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    reFile = str(regex.match(file).group(1))\n",
    "    \n",
    "    for eachDoc in root.findall('.//ns:div/[@type=\"entry\"]', ns):\n",
    "        entry = eachDoc.get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "        text = get_textContent(eachDoc, './ns:div/[@type=\"docbody\"]/ns:p', ns)\n",
    "        \n",
    "        if dataset == 'all':\n",
    "            for elem in eachDoc.findall('.//ns:p/ns:persRef/[@ref]', ns):\n",
    "                name = elem.text\n",
    "                try:\n",
    "                    entity = re.sub(r'\\s+', ' ', name)\n",
    "                except TypeError:\n",
    "                    entity = name\n",
    "\n",
    "                data = data.append({'file':reFile,\n",
    "                                'entry':entry,\n",
    "                                'text':text,\n",
    "                                'element':re.sub(r'.*}(.*)', '\\\\1', elem.tag),\n",
    "                                'refKey':elem.get('ref'),\n",
    "                                'entity':entity},\n",
    "                               ignore_index = True)\n",
    "\n",
    "        elif dataset == 'training':\n",
    "            for xpath in ['.//ns:p//ns:persName', './/ns:p//ns:placeName']:\n",
    "                for elem in eachDoc.findall(xpath, ns):\n",
    "                    name = elem.text\n",
    "                    try:\n",
    "                        entity = re.sub(r'\\s+', ' ', name)\n",
    "                    except TypeError:\n",
    "                        entity = name\n",
    "\n",
    "                    data = data.append({'file':reFile,\n",
    "                                    'entry':entry,\n",
    "                                    'text':text,\n",
    "                                    'element':re.sub(r'.*}(.*)', '\\\\1', elem.tag),\n",
    "                                    'entity':entity},\n",
    "                                   ignore_index = True)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print ('Selected dataset not found.')\n",
    "\n",
    "\n",
    "# Create a dictionary to change element tags to NER labels.\n",
    "element_ner_dictionary = {'persName':'PERSON', 'placeName':'LOC'}\n",
    "\n",
    "# Change elements to NER labels.\n",
    "labels_for_sentences = (data['element'].map(element_ner_dictionary))\n",
    "\n",
    "# Attach labels as a column.\n",
    "data['label'] = labels_for_sentences\n",
    "                \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \"Senticize\" and Tokenize Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 5.23 ms, total: 109 ms\n",
      "Wall time: 113 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>sents</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-28</td>\n",
       "      <td>Bailey here .</td>\n",
       "      <td>[(0, 6, PERSON)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-28</td>\n",
       "      <td>Mrs. Adams to Alexandria .</td>\n",
       "      <td>[(5, 10, PERSON), (14, 24, LOC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-28</td>\n",
       "      <td>P.U.S. to Loudoun .</td>\n",
       "      <td>[(10, 17, LOC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TrainCopy_JQADiaries-v23-1821-05-p359</td>\n",
       "      <td>jqadiaries-v23-1821-05-28</td>\n",
       "      <td>S. Thompson , Parish , Calhoun at Office .</td>\n",
       "      <td>[(14, 20, PERSON), (23, 30, PERSON)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file                      entry  \\\n",
       "42  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-28   \n",
       "43  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-28   \n",
       "44  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-28   \n",
       "45  TrainCopy_JQADiaries-v23-1821-05-p359  jqadiaries-v23-1821-05-28   \n",
       "\n",
       "                                         sents  \\\n",
       "42                               Bailey here .   \n",
       "43                  Mrs. Adams to Alexandria .   \n",
       "44                         P.U.S. to Loudoun .   \n",
       "45  S. Thompson , Parish , Calhoun at Office .   \n",
       "\n",
       "                                entities  \n",
       "42                      [(0, 6, PERSON)]  \n",
       "43      [(5, 10, PERSON), (14, 24, LOC)]  \n",
       "44                       [(10, 17, LOC)]  \n",
       "45  [(14, 20, PERSON), (23, 30, PERSON)]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Parse plain text for sentences.\n",
    "data['sents'] = data['text'].apply(lambda x: list(nlp(x).sents))\n",
    "\n",
    "# Unnest list of sentences.\n",
    "sentences = data.explode('sents')\n",
    "\n",
    "# Subset dataframe by columns. (Unless way to automate, leave out 'element', 'entity')\n",
    "sentences = sentences[['file', 'entry', 'element', 'entity', 'label', 'sents']]\n",
    "\n",
    "# Join nlp.sents into strings.\n",
    "sentences['sents'] = sentences['sents'].apply(lambda x: ' '.join(i.text for i in x))\n",
    "\n",
    "# Replace m-dash with space for tokenizing words.\n",
    "sentences['sents'] = sentences['sents'].str.replace(r'—', ' ', regex = True)\n",
    "\n",
    "# Decalare function to tokenize sentences.\n",
    "# Then, return entities and their start and end position within a string.\n",
    "def tokenize_sents(sentence_column, entity_column, label_column):\n",
    "    token_l = []\n",
    "    sent = sentence_column\n",
    "    \n",
    "#     Replace whitespace with underscore in entity strings so they're considered continuous.\n",
    "    if entity_column in sentence_column:\n",
    "        re_entity = re.sub('\\s', '_', entity_column)\n",
    "        sent = re.sub(entity, re_entity, sent)\n",
    "        \n",
    "#     Find string positions of each word.\n",
    "    for w in sent.split(' '):\n",
    "        token_start = sent.find(w)\n",
    "        token_end = token_start + len(w)\n",
    "        \n",
    "#         If condition to filter out words that are not entities.\n",
    "        if re.sub('_', ' ', w) == entity_column:\n",
    "            token_l.append((token_start, token_end, label_column))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return token_l\n",
    "        \n",
    "\n",
    "# Tokenize sentences with string position (beginning and end).\n",
    "sentences['tokens'] = sentences \\\n",
    "    .apply(lambda row: tokenize_sents(row['sents'], row['entity'], row['label']),\n",
    "           axis = 1)\n",
    "\n",
    "# Unnest 'tokens' columns so only tuples remain.\n",
    "sentences = sentences.explode('tokens')\n",
    "\n",
    "# Drop null values (rows without an entity).\n",
    "sentences = sentences.dropna()\n",
    "\n",
    "# For each sentences, gather 'tokens' tuples into single list.\n",
    "sentences = sentences.groupby(['file', 'entry', 'sents'])['tokens'].apply(list) \\\n",
    "    .reset_index(name = 'entities')\n",
    "\n",
    "\n",
    "sentences.head()\n",
    "# sentences.query('sents == \"Mrs. Adams to Alexandria .\"')\n",
    "sentences.query('entry == \"jqadiaries-v23-1821-05-28\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape Data for spaCy NER\n",
    "\n",
    "\n",
    "#### Data Sample for Custom NER (spaCy)\n",
    "\n",
    "```json\n",
    "TRAIN_DATA = [\n",
    "    ('Who is Nishanth?', {\n",
    "        'entities': [(7, 15, 'PERSON')]\n",
    "    }),\n",
    "     ('Who is Kamal Khumar?', {\n",
    "        'entities': [(7, 19, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.08 ms, sys: 30 µs, total: 6.11 ms\n",
      "Wall time: 6.12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Notes to Canning & c.', {'entities': [(9, 16, 'PERSON')]}),\n",
       " ('VI : Mrs. Adams unwell .', {'entities': [(10, 15, 'PERSON')]}),\n",
       " ('Mrs. A. to Alexandria .',\n",
       "  {'entities': [(5, 7, 'PERSON'), (11, 21, 'LOC')]}),\n",
       " ('Philip went away .', {'entities': [(0, 6, 'PERSON')]}),\n",
       " ('E. Wyer at the Office with his Report   P. Lanman , and E. Patterson   Letter to Carysfort .',\n",
       "  {'entities': [(81, 90, 'LOC')]}),\n",
       " ('Poletica , Wyer , Rodgers at the Office .',\n",
       "  {'entities': [(0, 8, 'PERSON'), (11, 15, 'PERSON'), (18, 25, 'PERSON')]}),\n",
       " ('Heard Little , at the Bath Room .', {'entities': [(6, 12, 'PERSON')]}),\n",
       " ('Hyde here   Seamen from Martinique .', {'entities': [(0, 4, 'PERSON')]}),\n",
       " ('7 VI : Jacob Adams , Wyer , Connell at the Office .',\n",
       "  {'entities': [(21, 25, 'PERSON'), (28, 35, 'PERSON')]}),\n",
       " ('Dr Tucker .', {'entities': [(3, 9, 'PERSON')]}),\n",
       " ('9 VI : S. Kean here   Connell at the Office .',\n",
       "  {'entities': [(22, 29, 'PERSON')]}),\n",
       " ('Draft of Note to Hyde .', {'entities': [(17, 21, 'PERSON')]}),\n",
       " ('10 VI : Forsyth at the Office .', {'entities': [(8, 15, 'PERSON')]}),\n",
       " ('Tazewell Commissioner .', {'entities': [(0, 8, 'PERSON')]}),\n",
       " ('11 VI : Crawford at the Office   Statements of French trade .',\n",
       "  {'entities': [(8, 16, 'PERSON')]}),\n",
       " ('Eve at Hyde ’s .', {'entities': [(7, 11, 'PERSON')]}),\n",
       " ('Note for Hyde   Letter to Griswold .',\n",
       "  {'entities': [(9, 13, 'PERSON'), (26, 34, 'PERSON')]}),\n",
       " ('G 13 V : Heard Little .', {'entities': [(15, 21, 'PERSON')]}),\n",
       " ('Mrs Adams passed the eve at Smith ’s .', {'entities': [(4, 9, 'PERSON')]}),\n",
       " ('At P.U.S. Hyde .', {'entities': [(10, 14, 'PERSON')]}),\n",
       " ('Forsyth to take leave .', {'entities': [(0, 7, 'PERSON')]}),\n",
       " ('Mrs. Adams , eve at Mrs Brown ’s .', {'entities': [(5, 10, 'PERSON')]}),\n",
       " ('15 V:30.Thornton , Miller , Forsyth , Freeman , Connell at the Office .',\n",
       "  {'entities': [(19, 25, 'PERSON'),\n",
       "    (28, 35, 'PERSON'),\n",
       "    (38, 45, 'PERSON'),\n",
       "    (48, 55, 'PERSON')]}),\n",
       " ('Last sitting to C. King   Long Note from Hyde   Louisiana .',\n",
       "  {'entities': [(41, 45, 'PERSON'), (48, 57, 'LOC')]}),\n",
       " ('Pensacola .', {'entities': [(0, 9, 'LOC')]}),\n",
       " ('17 V : Gales at the Office   Calvin .',\n",
       "  {'entities': [(7, 12, 'PERSON'), (29, 35, 'PERSON')]}),\n",
       " ('Despatch to Hughes   Ball at Canning ’s .',\n",
       "  {'entities': [(12, 18, 'PERSON')]}),\n",
       " ('Stackelberg .', {'entities': [(0, 11, 'PERSON')]}),\n",
       " ('Levy and Miller at the Office .',\n",
       "  {'entities': [(0, 4, 'PERSON'), (9, 15, 'PERSON')]}),\n",
       " ('19 VI : Wyer at the Office , and Bulfinch with E. Mauri   Sinking fund .',\n",
       "  {'entities': [(8, 12, 'PERSON'), (33, 41, 'PERSON')]}),\n",
       " ('Heard M’Cormick .', {'entities': [(6, 15, 'PERSON')]}),\n",
       " ('Met P.U.S. and Hay .', {'entities': [(15, 18, 'PERSON')]}),\n",
       " ('Watkins , Leonard , Worthington , Macauley at the Office .',\n",
       "  {'entities': [(0, 7, 'PERSON'),\n",
       "    (10, 17, 'PERSON'),\n",
       "    (20, 31, 'PERSON'),\n",
       "    (34, 42, 'PERSON')]}),\n",
       " ('Met Calhoun there .', {'entities': [(4, 11, 'PERSON')]}),\n",
       " ('Note from P.U.S. He is going to Loudoun .',\n",
       "  {'entities': [(32, 39, 'LOC')]}),\n",
       " ('Mrs. A. eve at Smith ’s .', {'entities': [(5, 7, 'PERSON')]}),\n",
       " ('Rotch here .', {'entities': [(0, 5, 'PERSON')]}),\n",
       " ('25 VI : Calls on Rotch and Hammond .',\n",
       "  {'entities': [(17, 22, 'PERSON'), (27, 34, 'PERSON')]}),\n",
       " ('P.U.S.Scott , Brown , Mason at the Office .',\n",
       "  {'entities': [(14, 19, 'PERSON'), (22, 27, 'PERSON')]}),\n",
       " ('Hammond , W. G. D. Worthington at the Office   At P.U.S. Met Calhoun .',\n",
       "  {'entities': [(0, 7, 'PERSON'), (61, 68, 'PERSON')]}),\n",
       " ('G 27 VII : Heard Belt at the Treasury .',\n",
       "  {'entities': [(17, 21, 'PERSON')]}),\n",
       " ('Visit to Martineng .', {'entities': [(9, 18, 'PERSON')]}),\n",
       " ('Bailey here .', {'entities': [(0, 6, 'PERSON')]}),\n",
       " ('Mrs. Adams to Alexandria .',\n",
       "  {'entities': [(5, 10, 'PERSON'), (14, 24, 'LOC')]}),\n",
       " ('P.U.S. to Loudoun .', {'entities': [(10, 17, 'LOC')]}),\n",
       " ('S. Thompson , Parish , Calhoun at Office .',\n",
       "  {'entities': [(14, 20, 'PERSON'), (23, 30, 'PERSON')]}),\n",
       " ('29 V : Cranch ’s and Greenleaf here .', {'entities': [(21, 30, 'PERSON')]}),\n",
       " ('Mauri and Wyer at the Office .',\n",
       "  {'entities': [(0, 5, 'PERSON'), (10, 14, 'PERSON')]}),\n",
       " ('30 VI:30.Mauri , Tazewell , Watkins , Hyde at the Office .',\n",
       "  {'entities': [(17, 25, 'PERSON'), (28, 35, 'PERSON'), (38, 42, 'PERSON')]}),\n",
       " ('Ironside to New - York .', {'entities': [(0, 8, 'PERSON')]}),\n",
       " ('V : Josiah Meigs and Fellows , Tazewell , White and Omaley at the Office .',\n",
       "  {'entities': [(31, 39, 'PERSON'), (42, 47, 'PERSON'), (52, 58, 'PERSON')]})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather tuples for each sentence into single list.\n",
    "training_data = sentences[['sents', 'entities']]\n",
    "\n",
    "# Convert dataframe to json format.\n",
    "TRAIN_DATA = []\n",
    "\n",
    "for index, row in training_data.iterrows():\n",
    "    sentence_data = (row['sents'], {'entities': row['entities']})\n",
    "    TRAIN_DATA.append(sentence_data)\n",
    "    \n",
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 300 ms, sys: 5.6 ms, total: 306 ms\n",
      "Wall time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "        \n",
    "nlp.to_disk(abs_dir + 'Output/NER/ner-spacyCustom-' + str(dataset) + '-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
