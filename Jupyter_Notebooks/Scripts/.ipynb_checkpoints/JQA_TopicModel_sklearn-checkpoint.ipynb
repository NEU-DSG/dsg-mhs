{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, warnings, pickle, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import NLTK packages.\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import and append stopwords.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Import sklearn packages.\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Import project-specific functions.\n",
    "# Python files (.py) have to be in same folder to work.\n",
    "from xml_ET_parse_functions import *\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        print ('Expected command: topic_model_gensim.py <input> <output> <numberOfTopics>')\n",
    "        exit(-1)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Declare variables.\n",
    "    \"\"\"\n",
    "    # Declare regex to simplify file paths below\n",
    "    regex = re.compile(r'.*/\\d{4}/(.*)')\n",
    "\n",
    "    # Declare document level of file. Requires root starting point ('.').\n",
    "    doc_as_xpath = './/ns:div/[@type=\"entry\"]'\n",
    "\n",
    "    # Declare date element of each document.\n",
    "    date_path = './ns:bibl/ns:date/[@when]'\n",
    "\n",
    "    # Declare person elements in each document.\n",
    "    person_path = './/ns:p/ns:persRef/[@ref]'\n",
    "    \n",
    "    # Declare subject elements in each document.\n",
    "    subject_path = './/ns:bibl/ns:note[@type=\"subject\"]'\n",
    "\n",
    "    # Declare text level within each document.\n",
    "    text_path = './ns:div/[@type=\"docbody\"]/ns:p'\n",
    "\n",
    "    print ('Variables declared.')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Build dataframe.\n",
    "    \"\"\"\n",
    "    dataframe = []\n",
    "\n",
    "    for file in glob.glob(str(sys.argv[1]) + '/*xml'):\n",
    "    #         Call functions to create necessary variables and grab content.\n",
    "        root = get_root(file)\n",
    "        ns = get_namespace(root)\n",
    "\n",
    "\n",
    "        for eachDoc in root.findall(doc_as_xpath, ns):\n",
    "    #             Call functions.\n",
    "            entry = get_document_id(eachDoc, '{http://www.w3.org/XML/1998/namespace}id')\n",
    "            date = get_date_from_attrValue(eachDoc, date_path, 'when', ns)\n",
    "            people = get_peopleList_from_attrValue(eachDoc, person_path, 'ref', ns)\n",
    "            subject = get_subject_from_attrValue(eachDoc, subject_path, ns)\n",
    "            text = get_textContent(eachDoc, text_path, ns)\n",
    "\n",
    "            dataframe.append([str(regex.search(file).groups()), entry, date, people, subject, text])\n",
    "\n",
    "    dataframe = pd.DataFrame(dataframe, columns = ['file', 'entry', 'date', 'people', 'subject', 'text'])\n",
    "    print ('Dataframe built.')\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Lemmatize & Stem Text\n",
    "    \"\"\"\n",
    "    # Lowercase text field\n",
    "    dataframe['text'] = dataframe['text'].str.lower()\n",
    "\n",
    "    # Tokenize text field.\n",
    "    dataframe['text'] = dataframe['text'].apply(word_tokenize)\n",
    "\n",
    "    # Lemmatize and stem text field.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
    "\n",
    "    def lemma_and_stem(list_of_words):\n",
    "        return [stemmer.stem(lemmatizer.lemmatize(w)) for w in list_of_words if w not in stop_words]\n",
    "\n",
    "    dataframe['text'] = dataframe['text'].apply(lemma_and_stem)\n",
    "\n",
    "    # Convert list of words to string for LDA model.\n",
    "    dataframe['text'] = dataframe['text'].apply(' '.join)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train Topic Model\n",
    "    \"\"\"\n",
    "    # Remove duplicate text rows (caused from unnesting headings) by subsetting & de-duplicating.\n",
    "    topics = dataframe[['entry', 'text']].drop_duplicates(subset = ['entry'])\n",
    "\n",
    "    # Initialise the vectorizer with English stop words.\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the processed texts.\n",
    "    features = vectorizer.fit_transform(topics['text'])\n",
    "    \n",
    "    # Set parameters (topics set to number of unique subject headings found).\n",
    "    number_topics = sys.argv[3]\n",
    "    number_words = 10\n",
    "\n",
    "    # Create and fit the LDA model\n",
    "    lda = LDA(n_components = number_topics, n_jobs=-1)\n",
    "    lda.fit(features)\n",
    "    \n",
    "    # Create a document-topic matrix.\n",
    "    dtm = lda.transform(features)\n",
    "\n",
    "    # Convert document-topic matrix to dataframe.\n",
    "    dtm = pd.DataFrame(dtm, index = topics.index)\n",
    "\n",
    "    # Join document-topic dataframe with metadata on shared indices.\n",
    "    dtm = pd.merge(dataframe[['file', 'entry', 'date', 'subject']],\n",
    "                   dtm,\n",
    "                   left_index = True, right_index = True)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Save topics dataframe.\n",
    "    \"\"\"\n",
    "    with open(sys.argv[2], \"w\") as f:\n",
    "        f.write(dtm)\n",
    "        print ('Topics dataframe saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
