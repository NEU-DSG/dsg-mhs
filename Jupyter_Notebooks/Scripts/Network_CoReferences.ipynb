{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-monroe",
   "metadata": {},
   "source": [
    "# Network Analysis: Co-References within Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, json, glob, re # Import global modules (ones that appear in multiple locations) with 'as'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.algorithms import community\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "from xml_ET_parse_functions import *\n",
    "from network_prep_functions import *\n",
    "\n",
    "# Set warnings filter.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Declare absolute directory path.\n",
    "abs_dir = '/Users/quinn.wi/Documents/'\n",
    "\n",
    "# Declare XML directory path.\n",
    "xml_dir = glob.glob(abs_dir + 'Data/JQA/1821/*.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-choir",
   "metadata": {},
   "source": [
    "## Declare Project-Specific XPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "global-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare regex to simplify file paths below\n",
    "regex = re.compile(r'.*/\\d{4}/(.*)')\n",
    "\n",
    "# Declare document level of file. Requires root starting point ('.').\n",
    "doc_as_xpath = './/ns:div/[@type=\"entry\"]'\n",
    "\n",
    "# Declare date element of each document.\n",
    "date_path = './ns:bibl/ns:date/[@when]'\n",
    "\n",
    "# Declare person elements in each document.\n",
    "person_path = './/ns:p/ns:persRef/[@ref]'\n",
    "\n",
    "# Declare text level within each document.\n",
    "text_path = './ns:div/[@type=\"docbody\"]/ns:p'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-thunder",
   "metadata": {},
   "source": [
    "## Build Nodes, Links, and Dictionary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blank-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 280 ms, total: 29.1 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataframe = []\n",
    "\n",
    "for file in xml_dir:\n",
    "#         Call functions to create necessary variables and grab content.\n",
    "    root = get_root(file)\n",
    "    ns = get_namespace(root)\n",
    "\n",
    "    for eachDoc in root.findall(doc_as_xpath, ns):\n",
    "#             Call functions.\n",
    "        entry = get_document_id(eachDoc, '{http://www.w3.org/XML/1998/namespace}id')\n",
    "        date = get_date_from_attrValue(eachDoc, date_path, 'when', ns)\n",
    "        people = get_peopleList_from_attrValue(eachDoc, person_path, 'ref', ns)\n",
    "        text = get_textContent(eachDoc, text_path, ns)\n",
    "        \n",
    "        dataframe.append([str(regex.search(file).groups()), entry, date, people, text])\n",
    "        \n",
    "dataframe = pd.DataFrame(dataframe, columns = ['file', 'entry', 'date', 'people', 'text'])\n",
    "\n",
    "# Split string of people into individuals.\n",
    "dataframe['people'] = dataframe['people'].str.split(r',|;')\n",
    "\n",
    "# Explode list so that each list value becomes a row.\n",
    "dataframe = dataframe.explode('people')\n",
    "\n",
    "# Create entry-person matrix.\n",
    "dataframe = pd.crosstab(dataframe['entry'], dataframe['people'])\n",
    "\n",
    "# Convert entry-person matrix into an adjacency matrix of persons.\n",
    "dataframe = dataframe.T.dot(dataframe)\n",
    "\n",
    "# Change diagonal values to zero. That is, a person cannot co-occur with themself.\n",
    "np.fill_diagonal(dataframe.values, 0)\n",
    "\n",
    "# Simple correlation matrix from dataframe.\n",
    "dataframe = dataframe.corr(method = 'pearson')\n",
    "\n",
    "# Create new 'source' column that corresponds to index (person).\n",
    "dataframe['source'] = dataframe.index\n",
    "\n",
    "# Reshape dataframe to focus on source, target, and weight.\n",
    "# Remove same-person pairs (weight = 1) and weak correlations (weight < 4).\n",
    "# Rename 'people' column name to 'target'.\n",
    "dataframe = pd.melt(dataframe, id_vars = ['source'], value_name = 'weight') \\\n",
    "    .query('(weight < 1.00) & (weight > 0.4)') \\\n",
    "    .rename(columns = {'people':'target'})\n",
    "\n",
    "nodes, links, nodes_dictionary = get_nodes_and_links(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-missouri",
   "metadata": {},
   "source": [
    "## Build Network and Measure Network Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "obvious-objective",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Map labels back onto source and target.\n",
    "edges = links.replace({'source':nodes_dictionary, 'target':nodes_dictionary})\n",
    "\n",
    "# Convert edges dataframe to edges tuple (compatible with graph object below).\n",
    "edges = [tuple(x) for x in edges[['source', 'target']].to_numpy()]\n",
    "\n",
    "# Initialize graph object.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges to graph object.\n",
    "G.add_nodes_from(nodes['label'])\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print (nx.info(G))\n",
    "\n",
    "# Measure network density.\n",
    "density = nx.density(G)\n",
    "print (f\"Network density: {density:.3f}\")\n",
    "\n",
    "# Related to diameter, check if network is connected and, therefore, can have a diameter.\n",
    "print (f\"Is the network connected? {nx.is_connected(G)}\")\n",
    "\n",
    "# Get a list of network components (communities).\n",
    "# Find the largest component.\n",
    "components = nx.connected_components(G)\n",
    "largest_component = max(components, key = len)\n",
    "\n",
    "# Create a subgraph of the largest component and measure its diameter.\n",
    "subgraph = G.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print (f\"Network diameter of the largest component: {diameter:.3f}\")\n",
    "\n",
    "# Find triadic closure (similar to density).\n",
    "triadic_closure = nx.transitivity(G)\n",
    "print (f\"Triadic closure: {triadic_closure:.3f}\\n\")\n",
    "\n",
    "# Find centrality measures.\n",
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each centrality measure to an attribute.\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "nx.set_node_attributes(G, dict(G.degree(G.nodes())), 'degree')\n",
    "\n",
    "# Find communities.\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Create a dictionary that maps nodes to their community.\n",
    "modularity_dict = {}\n",
    "for i, c in enumerate(communities):\n",
    "    for name in c:\n",
    "        modularity_dict[name] = i\n",
    "        \n",
    "# Add modularity information to graph object.\n",
    "nx.set_node_attributes(G, modularity_dict, 'modularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-colleague",
   "metadata": {},
   "source": [
    "## Save Network Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert graph object into a dictionary.\n",
    "data = json_graph.node_link_data(G)\n",
    "\n",
    "# Serialize dictionary with json.\n",
    "class NPEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "    \n",
    "data_json = json.dumps(data, cls=NPEncoder)\n",
    "\n",
    "with open(abs_dir + \"Output/Graphs/JQA_Network_correlation/network.json\", \"w\") as f:\n",
    "    \n",
    "    f.write(data_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
