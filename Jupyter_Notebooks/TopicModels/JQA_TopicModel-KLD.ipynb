{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "productive-satisfaction",
   "metadata": {},
   "source": [
    "# Topic Model — Kullback-Leibler Divergence\n",
    "\n",
    "This notebook measures Kullback-Leibler Divergence (KLD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sharp-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-01</td>\n",
       "      <td>1817-10-01</td>\n",
       "      <td>1. IV:30. Wednesday. Wrote a Letter to J. L. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-02</td>\n",
       "      <td>1817-10-02</td>\n",
       "      <td>2. IV: Continued drafting instructions for Rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-03</td>\n",
       "      <td>1817-10-03</td>\n",
       "      <td>3. IV: I had visits this morning from Mr Levet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-04</td>\n",
       "      <td>1817-10-04</td>\n",
       "      <td>4. IV: I waked before three and had afterwards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-05</td>\n",
       "      <td>1817-10-05</td>\n",
       "      <td>5. V: The Ladies went this morning to St. John...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              file                      entry        date  \\\n",
       "0  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-01  1817-10-01   \n",
       "1  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-02  1817-10-02   \n",
       "2  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-03  1817-10-03   \n",
       "3  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-04  1817-10-04   \n",
       "4  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-05  1817-10-05   \n",
       "\n",
       "                                                text  \n",
       "0  1. IV:30. Wednesday. Wrote a Letter to J. L. S...  \n",
       "1  2. IV: Continued drafting instructions for Rus...  \n",
       "2  3. IV: I had visits this morning from Mr Levet...  \n",
       "3  4. IV: I waked before three and had afterwards...  \n",
       "4  5. V: The Ladies went this morning to St. John...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries.\n",
    "import re, nltk, warnings, csv, sys, os, gensim, tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from scipy import stats\n",
    "\n",
    "# Import NLTK packages.\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import sklearn packages.\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Import and append stopwords.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append('mr')\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/Data/\"\n",
    "\n",
    "# Read in file; select columns; drop rows with NA values (entries without a named person).\n",
    "df = pd.read_csv(abs_dir + 'Output/ParsedXML/JQA_Subjects-dataframe.txt', sep = '\\t') \\\n",
    "    .drop(columns = ['subject']) \\\n",
    "    .dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-premium",
   "metadata": {},
   "source": [
    "## Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crazy-cholesterol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 217 ms, total: 29.3 s\n",
      "Wall time: 29.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>entry</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-01</td>\n",
       "      <td>1817-10-01</td>\n",
       "      <td>1. iv:30. wednesday . wrote letter j. l. sulli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-02</td>\n",
       "      <td>1817-10-02</td>\n",
       "      <td>2. iv : continu draft instruct rush . subject ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-03</td>\n",
       "      <td>1817-10-03</td>\n",
       "      <td>3. iv : visit morn levett harri , nours regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-04</td>\n",
       "      <td>1817-10-04</td>\n",
       "      <td>4. iv : wake three afterward sleep . inconveni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JQADiaries-v30-1817-10-p260.xml</td>\n",
       "      <td>jqadiaries-v30-1817-10-05</td>\n",
       "      <td>1817-10-05</td>\n",
       "      <td>5. v : ladi went morn st. john ’ church ; retu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              file                      entry        date  \\\n",
       "0  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-01  1817-10-01   \n",
       "1  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-02  1817-10-02   \n",
       "2  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-03  1817-10-03   \n",
       "3  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-04  1817-10-04   \n",
       "4  JQADiaries-v30-1817-10-p260.xml  jqadiaries-v30-1817-10-05  1817-10-05   \n",
       "\n",
       "                                                text  \n",
       "0  1. iv:30. wednesday . wrote letter j. l. sulli...  \n",
       "1  2. iv : continu draft instruct rush . subject ...  \n",
       "2  3. iv : visit morn levett harri , nours regist...  \n",
       "3  4. iv : wake three afterward sleep . inconveni...  \n",
       "4  5. v : ladi went morn st. john ’ church ; retu...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # Unnest subject headings.\n",
    "# df['subject'] = df['subject'].str.split(',')\n",
    "# df = df.explode('subject')\n",
    "\n",
    "# Lowercase text field\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Tokenize text field.\n",
    "df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# Lemmatize and stem text field.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
    "\n",
    "def lemma_and_stem(list_of_words):\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(w)) for w in list_of_words if w not in stop_words]\n",
    "\n",
    "df['text'] = df['text'].apply(lemma_and_stem)\n",
    "\n",
    "# Convert list of words to string for LDA model.\n",
    "df['text'] = df['text'].apply(' '.join)\n",
    "\n",
    "# print ('Number of unique subject headings:', len(df['subject'].unique()), '\\n')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-seventh",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-sleep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 837 ms, total: 3.1 s\n",
      "Wall time: 13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=20, n_jobs=-1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove duplicate text rows (caused from unnesting headings) by subsetting & de-duplicating.\n",
    "topics = df[['entry', 'text']].drop_duplicates(subset = ['entry'])\n",
    "\n",
    "# Initialise the vectorizer with English stop words.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed texts.\n",
    "features = vectorizer.fit_transform(topics['text'])\n",
    "\n",
    "# Set parameters (topics set to number of unique subject headings found).\n",
    "number_topics = 20\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components = number_topics, n_jobs=-1)\n",
    "lda.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-homework",
   "metadata": {},
   "source": [
    "## Document Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appropriate-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.1 ms, sys: 139 ms, total: 202 ms\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create a document-topic matrix.\n",
    "doc_tops = lda.transform(features)\n",
    "\n",
    "# Convert document-topic matrix to dataframe.\n",
    "doc_tops = pd.DataFrame(doc_tops, index = topics.index)\n",
    "\n",
    "# Join document-topic dataframe with metadata on shared indices.\n",
    "doc_tops = pd.merge(df[['entry', 'date']],\n",
    "                    doc_tops,\n",
    "                    left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-alias",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "closed-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 ms, sys: 1.84 ms, total: 8.06 ms\n",
      "Wall time: 6.95 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "matrix = doc_tops.drop(columns = ['entry', 'date'])\n",
    "\n",
    "kld = stats.entropy(matrix.T, matrix.T)\n",
    "\n",
    "kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "informative-independence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix.T\n",
    "\n",
    "# stats.entropy([0.000485, 0.000266, 0.000299], [0.302791, 0.146970, 0.061189])\n",
    "\n",
    "max(kld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-surgeon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
