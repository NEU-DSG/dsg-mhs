{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c0522",
   "metadata": {},
   "source": [
    "# Topic Model -- Richards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca86511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, nltk, warnings, csv, sys, os, pickle, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from scipy import stats\n",
    "\n",
    "# Import NLTK packages.\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import sklearn packages.\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Import LDA visualizer.\n",
    "import pyLDAvis, pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Import and append stopwords.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append(['mr', 'dear'])\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('Correspondence_XML_parser.py'), '../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "from Correspondence_XML_parser import *\n",
    "\n",
    "# Read in config.py (git ignored file) for API username and pw.\n",
    "config_path = os.path.abspath(os.path.join(os.path.dirname('config.py'), '../Scripts'))\n",
    "sys.path.append(config_path)\n",
    "import config\n",
    "\n",
    "# url = 'https://dsg.xmldb-dev.northeastern.edu/basex/psc/' :: old\n",
    "url = 'https://dsg.xmldb-dev.northeastern.edu/BaseX964/rest/psc/'\n",
    "user = config.username\n",
    "pw = config.password\n",
    "\n",
    "# Get the correct file path to navigate to the github repository.\n",
    "abs_dir = os.getcwd() + '/../../'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aac6bc",
   "metadata": {},
   "source": [
    "## Gather XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7f626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 855 µs, sys: 849 µs, total: 1.7 ms\n",
      "Wall time: 959 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove this cell when files are in BaseX.\n",
    "# Declare directory location to shorten filepaths later.\n",
    "files = glob.glob(abs_dir + \"../../Data/PSC/Richards/ESR-XML-Files-MHS/*.xml\")\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8175a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Must be connected to Northeastern's VPN.\n",
    "# r = requests.get(url, \n",
    "#                  auth = (user, pw), \n",
    "#                  headers = {'Content-Type': 'application/xml'}\n",
    "#                 )\n",
    "\n",
    "# # Check status of URL\n",
    "# print (r.status_code)\n",
    "\n",
    "# # Read in contents of pipeline.\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# # Split soup's content by \\n (each line is a file path to an XML doc).\n",
    "# # Use filter() to remove empty strings ('').\n",
    "# # Convert back to list using list().\n",
    "# files = list(filter(None, soup.text.split('\\n')))\n",
    "\n",
    "# # # Filter list and retrieve only jqa/ files.\n",
    "# # files = [i for i in files if 'jqa/' in i]\n",
    "\n",
    "# # len(files)\n",
    "# files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126022e7",
   "metadata": {},
   "source": [
    "## Build DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7138ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/GitHub/dsg-mhs/Jupyter_Notebooks/TopicModels/../../../../Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "CPU times: user 8.11 ms, sys: 2.59 ms, total: 10.7 ms\n",
      "Wall time: 12.2 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>subjects</th>\n",
       "      <th>references</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESR-EDA-1892-01-08.xml</td>\n",
       "      <td>1892-01-08</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>1893 Chicago World's Fair,Aladdin Oven,New Eng...</td>\n",
       "      <td>palmer-bertha,hovey-e,daniells-unknown</td>\n",
       "      <td>Boston Jan 8 1892 My dear Mr Atkinson I enclo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESR-EDA-1892-04-12.xml</td>\n",
       "      <td>1892-04-12</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>Aladdin Oven,nutrition,cooking</td>\n",
       "      <td>abel-mary</td>\n",
       "      <td>April 12— Dear Mr Atkinson I expect Mrs Abel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESR-EDA-1892-04-07.xml</td>\n",
       "      <td>1892-04-07</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>Aladdin Oven,Nutrition,cooking</td>\n",
       "      <td>conro-emma,abel-mary</td>\n",
       "      <td>Boston, April 7, 1892 My dear Mr. Atkinson I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file        date          source           target  \\\n",
       "0  ESR-EDA-1892-01-08.xml  1892-01-08  richards-ellen  atkinson-edward   \n",
       "1  ESR-EDA-1892-04-12.xml  1892-04-12  richards-ellen  atkinson-edward   \n",
       "2  ESR-EDA-1892-04-07.xml  1892-04-07  richards-ellen  atkinson-edward   \n",
       "\n",
       "                                            subjects  \\\n",
       "0  1893 Chicago World's Fair,Aladdin Oven,New Eng...   \n",
       "1                     Aladdin Oven,nutrition,cooking   \n",
       "2                     Aladdin Oven,Nutrition,cooking   \n",
       "\n",
       "                               references  \\\n",
       "0  palmer-bertha,hovey-e,daniells-unknown   \n",
       "1                               abel-mary   \n",
       "2                    conro-emma,abel-mary   \n",
       "\n",
       "                                                text  \n",
       "0   Boston Jan 8 1892 My dear Mr Atkinson I enclo...  \n",
       "1   April 12— Dear Mr Atkinson I expect Mrs Abel ...  \n",
       "2   Boston, April 7, 1892 My dear Mr. Atkinson I ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build dataframe from XML files.\n",
    "# build_dataframe() called from Correspondence_XML_parser\n",
    "df = build_dataframe(files)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd701142",
   "metadata": {},
   "source": [
    "## Clean Data & Prepare for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d7ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89.9 ms, sys: 5 ms, total: 94.9 ms\n",
      "Wall time: 93.8 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Drop duplicate texts (created from unnested subject headings) & count words.\n",
    "doc_len = df['text'].str.split(' ').str.len() \\\n",
    "    .reset_index() \\\n",
    "    .drop_duplicates()\n",
    "\n",
    "# Round word count.\n",
    "doc_len = np.around(doc_len['text'], decimals = -1)\n",
    "\n",
    "doc_len = pd.DataFrame(doc_len)\n",
    "\n",
    "# Plot graph.\n",
    "sns.set(rc = {\"figure.figsize\": (12, 6)})\n",
    "sns.set_style(\"dark\")\n",
    "ax = sns.histplot(doc_len['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c056801",
   "metadata": {},
   "source": [
    "### Chunk texts into equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e063dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 ms, sys: 1.52 ms, total: 16.8 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>subjects</th>\n",
       "      <th>references</th>\n",
       "      <th>text</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESR-EDA-1892-04-07.xml</td>\n",
       "      <td>1892-04-07</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>Aladdin Oven,Nutrition,cooking</td>\n",
       "      <td>conro-emma,abel-mary</td>\n",
       "      <td>Boston, April 7, 1892 My dear Mr. Atkinson I ...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESR-EDA-1890-09-29.xml</td>\n",
       "      <td>1890-09-29</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>Teaching,Nutrition</td>\n",
       "      <td>abel-mary,abel-john,palmer-alice</td>\n",
       "      <td>Boston Sept 29 1890 Dear Mr Atkinson I answer...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ESR-EDA-1893-05-23.xml</td>\n",
       "      <td>1893-05-23</td>\n",
       "      <td>richards-ellen</td>\n",
       "      <td>atkinson-edward</td>\n",
       "      <td>Bread</td>\n",
       "      <td>morton-julius,shapleigh-unknown,sedgwick-willi...</td>\n",
       "      <td>Boston May 23 1893 My dear Mr Atkinson I glad...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file        date          source           target  \\\n",
       "2  ESR-EDA-1892-04-07.xml  1892-04-07  richards-ellen  atkinson-edward   \n",
       "3  ESR-EDA-1890-09-29.xml  1890-09-29  richards-ellen  atkinson-edward   \n",
       "5  ESR-EDA-1893-05-23.xml  1893-05-23  richards-ellen  atkinson-edward   \n",
       "\n",
       "                         subjects  \\\n",
       "2  Aladdin Oven,Nutrition,cooking   \n",
       "3              Teaching,Nutrition   \n",
       "5                           Bread   \n",
       "\n",
       "                                          references  \\\n",
       "2                               conro-emma,abel-mary   \n",
       "3                   abel-mary,abel-john,palmer-alice   \n",
       "5  morton-julius,shapleigh-unknown,sedgwick-willi...   \n",
       "\n",
       "                                                text  wordCount  \n",
       "2   Boston, April 7, 1892 My dear Mr. Atkinson I ...        177  \n",
       "3   Boston Sept 29 1890 Dear Mr Atkinson I answer...        167  \n",
       "5   Boston May 23 1893 My dear Mr Atkinson I glad...        158  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = 200\n",
    "\n",
    "def splitText(string):\n",
    "    words = string.split(' ')\n",
    "    removed_stopwords = [w for w in words if w not in stop_words]\n",
    "    grouped_words = [removed_stopwords[i: i + chunk_size] for i in range(0, len(removed_stopwords), chunk_size)]\n",
    "    return grouped_words\n",
    "\n",
    "df['text'] = df['text'].apply(splitText)\n",
    "\n",
    "df = df.explode('text')\n",
    "\n",
    "# Add word count field.\n",
    "df['wordCount'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Join list of words into single string.\n",
    "df['text'] = df['text'].apply(' '.join)\n",
    "\n",
    "# Remove rows without text.\n",
    "df = df.dropna(subset = ['text'])\n",
    "\n",
    "# Remove texts with too few words (chunk_size - 50).\n",
    "df = df.query('wordCount >= (@chunk_size - 50)')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5129067",
   "metadata": {},
   "source": [
    "## Train Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3321889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #1:\n",
      "work kitchen rumford centennial shall session certain mr make week\n",
      "\n",
      "Topic #2:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #3:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #4:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #5:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #6:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #7:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #8:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #9:\n",
      "people know think gumption statements little good qualifying present use\n",
      "\n",
      "Topic #10:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #11:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #12:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #13:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #14:\n",
      "letter sept food dinner mrs americans especially extract question abel\n",
      "\n",
      "Topic #15:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #16:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #17:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #18:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #19:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #20:\n",
      "working say good miss week little page think man food\n",
      "\n",
      "Topic #21:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #22:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #23:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #24:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #25:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #26:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #27:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #28:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #29:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #30:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #31:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #32:\n",
      "oven shall women feb better pail mr miss club roast\n",
      "\n",
      "Topic #33:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #34:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #35:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #36:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #37:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #38:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "\n",
      "Topic #39:\n",
      "years kind especially evidently exave expect expensive explains extract eye\n",
      "CPU times: user 106 ms, sys: 108 ms, total: 214 ms\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove duplicate text rows (caused from unnesting headings) by subsetting & de-duplicating.\n",
    "topics = df[['file', 'text']].drop_duplicates(subset = ['file'])\n",
    "\n",
    "# Initialise the vectorizer with English stop words.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed texts.\n",
    "features = vectorizer.fit_transform(topics['text'])\n",
    "\n",
    "# Helper function (from Kapadia).\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Set parameters (topics set to number of unique subject headings found).\n",
    "number_topics = 40\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components = number_topics, n_jobs=-1)\n",
    "lda.fit(features)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d76aeb",
   "metadata": {},
   "source": [
    "## Save pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904492e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 437 ms, sys: 16.3 ms, total: 454 ms\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "p = pyLDAvis.sklearn.prepare(lda, features, vectorizer, mds='mmds')\n",
    "\n",
    "pyLDAvis.save_html(p, abs_dir + \"lab_space/projects/richards/topics/richards_topics-40_pyLDAvis.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0186c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
