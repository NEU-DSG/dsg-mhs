{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model & Subject Headings\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "Kapadia, Shashank, \"[Topic Modeling in Python: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0),\" <i>towards data science</i>, Accessed 10/09/2020.\n",
    "\n",
    "ALollz, \"[How to calculate p-values for pairwise correlation of columns in Pandas?](https://stackoverflow.com/questions/52741236/how-to-calculate-p-values-for-pairwise-correlation-of-columns-in-pandas),\" <i>StackOverflow</i>, Accessed 10/13/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, nltk, warnings, csv, sys, os, pickle, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from scipy import stats\n",
    "\n",
    "# Import NLTK packages.\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import sklearn packages.\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Import LDA visualizer.\n",
    "import pyLDAvis, pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Import and append stopwords.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append('mr')\n",
    "\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('JQA_XML_parser.py'), '../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "from JQA_XML_parser import *\n",
    "\n",
    "# Read in config.py (git ignored file) for API username and pw.\n",
    "config_path = os.path.abspath(os.path.join(os.path.dirname('config.py'), '../Scripts'))\n",
    "sys.path.append(config_path)\n",
    "import config\n",
    "\n",
    "# url = 'https://dsg.xmldb-dev.northeastern.edu/basex/psc/' :: old\n",
    "url = 'https://dsg.xmldb-dev.northeastern.edu/BaseX964/rest/psc/'\n",
    "user = config.username\n",
    "pw = config.password\n",
    "\n",
    "# Ignore warnings related to deprecated functions.\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Get the correct file path to navigate to the github repository.\n",
    "abs_dir = os.getcwd() + '/../../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.31 ms, sys: 3.97 ms, total: 7.28 ms\n",
      "Wall time: 7.77 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove this cell when files are in BaseX.\n",
    "# Declare directory location to shorten filepaths later.\n",
    "files = glob.glob(abs_dir + \"../../Data/PSC/JQA/*/*.xml\")\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Must be connected to Northeastern's VPN.\n",
    "# r = requests.get(url, \n",
    "#                  auth = (user, pw), \n",
    "#                  headers = {'Content-Type': 'application/xml'}\n",
    "#                 )\n",
    "\n",
    "# # Check status of URL\n",
    "# print (r.status_code)\n",
    "\n",
    "# # Read in contents of pipeline.\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# # Split soup's content by \\n (each line is a file path to an XML doc).\n",
    "# # Use filter() to remove empty strings ('').\n",
    "# # Convert back to list using list().\n",
    "# files = list(filter(None, soup.text.split('\\n')))\n",
    "\n",
    "# # Filter list and retrieve only jqa/ files.\n",
    "# files = [i for i in files if 'jqa/' in i]\n",
    "\n",
    "# # len(files)\n",
    "# files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_dataframe() missing 3 required positional arguments: 'url', 'user', and 'pw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_dataframe() missing 3 required positional arguments: 'url', 'user', and 'pw'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build dataframe from XML files.\n",
    "# build_dataframe() called from Correspondence_XML_parser\n",
    "df = build_dataframe(files)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data & Prepare for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Drop duplicate texts (created from unnested subject headings) & count words.\n",
    "doc_len = df['text'].str.split(' ').str.len() \\\n",
    "    .reset_index() \\\n",
    "    .drop_duplicates()\n",
    "\n",
    "# Round word count.\n",
    "doc_len = np.around(doc_len['text'], decimals = -1)\n",
    "\n",
    "doc_len = pd.DataFrame(doc_len)\n",
    "\n",
    "# Plot graph.\n",
    "sns.set(rc = {\"figure.figsize\": (12, 6)})\n",
    "sns.set_style(\"dark\")\n",
    "ax = sns.histplot(doc_len['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Document Length\n",
    "\n",
    "Topic modeling is sensitive to document length. Longer documents, which discuss multiple topics, might water down the end results. It might be good to shorten and normalize document lengths.\n",
    "\n",
    "How will multiple subject headings relate to splitting entries? Will splitting entries wash down/skew results of topic correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk texts into equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = 200\n",
    "\n",
    "def splitText(string):\n",
    "    words = string.split(' ')\n",
    "    removed_stopwords = [w for w in words if w not in stop_words]\n",
    "    grouped_words = [removed_stopwords[i: i + chunk_size] for i in range(0, len(removed_stopwords), chunk_size)]\n",
    "    return grouped_words\n",
    "\n",
    "df['text'] = df['text'].apply(splitText)\n",
    "\n",
    "df = df.explode('text')\n",
    "\n",
    "# Add word count field.\n",
    "df['wordCount'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Join list of words into single string.\n",
    "df['text'] = df['text'].apply(' '.join)\n",
    "\n",
    "# Remove rows without text.\n",
    "df = df.dropna(subset = ['text'])\n",
    "\n",
    "# Remove texts with too few words (chunk_size - 50).\n",
    "df = df.query('wordCount >= (@chunk_size - 50)')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Topic Model\n",
    "\n",
    ">Count Vectorizer or Tfidf? Create two models and compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove duplicate text rows (caused from unnesting headings) by subsetting & de-duplicating.\n",
    "topics = df[['entry', 'text']].drop_duplicates(subset = ['entry'])\n",
    "\n",
    "# Initialise the vectorizer with English stop words.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed texts.\n",
    "features = vectorizer.fit_transform(topics['text'])\n",
    "\n",
    "# Helper function (from Kapadia).\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Set parameters (topics set to number of unique subject headings found).\n",
    "number_topics = 40\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components = number_topics, n_jobs=-1)\n",
    "lda.fit(features)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Topics\n",
    "\n",
    "#### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "pyLDAvis adjusts topic numbers by +1. \n",
    "Topic 1 in visualization is actually topic 0 (zero) in the model.\n",
    "'''\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda, features, vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<unknown>, line 3)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/var/folders/t2/01ffpgd11p9_t88s5bg2d2fm0000gp/T/ipykernel_21118/1313769416.py\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\np = pyLDAvis.sklearn.prepare(lda, features, vectorizer, mds=\\'mmds\\')\\n\\npyLDAvis.save_html(p, abs_dir + \"lab_space/projects/jqa/topics/jqa_topics-40_pyLDAvis.html\\')\\n')\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2403\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.8/site-packages/decorator.py\"\u001b[0m, line \u001b[1;32m232\u001b[0m, in \u001b[1;35mfun\u001b[0m\n    return caller(func, *(extras + args), **kw)\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1277\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pyLDAvis.save_html(p, abs_dir + \"lab_space/projects/jqa/topics/jqa_topics-40_pyLDAvis.html')\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "p = pyLDAvis.sklearn.prepare(lda, features, vectorizer, mds='mmds')\n",
    "\n",
    "pyLDAvis.save_html(p, abs_dir + \"lab_space/projects/jqa/topics/jqa_topics-40_pyLDAvis.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
