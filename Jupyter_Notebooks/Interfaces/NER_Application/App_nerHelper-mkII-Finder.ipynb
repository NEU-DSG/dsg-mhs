{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b83b5b3",
   "metadata": {},
   "source": [
    "# nerHelper -- part 1\n",
    "\n",
    "First half of app: finds and makes NER suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745a41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# I'm using lxml because it has getparent(), which is critical for accessing multiple xml:id of docs within a single file.\n",
    "from lxml import etree\n",
    "\n",
    "# I'm using ET in get_encoding() only.\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/GitHub/dsg-mhs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564817a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(child):\n",
    "    if child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "        ancestor = child.getparent().tag\n",
    "        xml_id = child.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "\n",
    "        abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "        return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('<[/]?w>', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "#     encoding = etree.tostring(elem, pretty_print = True).decode('UTF-8') # this line failed to return single elem.\n",
    "    \n",
    "#     This troubleshoots an error that emerged with etree.tostring above:\n",
    "    encoding = ET.tostring(elem, method = 'xml').decode('utf-8') # convert xml to string with ET\n",
    "#     encoding = etree.fromstring(encoding) # convert string back to xml encoding with etree.\n",
    "#     encoding = etree.tostring(encoding).decode('utf-8') # convert back to string with etree.\n",
    "    \n",
    "    encoding = xml_cleanup(encoding)\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    encoding = re.sub('[:]?ns0[:]?', '', encoding)\n",
    "    return encoding\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "#     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "    try:\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "        \n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            return \"Already Encoded\"\n",
    "        \n",
    "#         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "            new_encoding = re.sub('ns0:', '', new_encoding)\n",
    "\n",
    "            return new_encoding\n",
    "\n",
    "        else:\n",
    "            return 'Error Making NER Suggestions'\n",
    "    \n",
    "#     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "#     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "#     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(child, df, ns, subset_ner, filename, descendant_order):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    entities, previous_encoding = get_contents(child, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "        'abridged_xpath':abridged_xpath,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, ner_values): # date, \n",
    "    ner_values = ner_values#.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    # Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "            \n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                    df['abridged_xpath'] = abridged_xpath\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "            \n",
    "            df['new_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entity'],\n",
    "                                                        row['label'],\n",
    "                                                        subset_ner, 4, banned_list),\n",
    "                       axis = 1)\n",
    "\n",
    "            \n",
    "            # Add additional columns for user input.\n",
    "            df['uniq_id'] = ''\n",
    "            \n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "            df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return df # filename, date, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f46307",
   "metadata": {},
   "source": [
    "## APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120b908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n",
      "CPU times: user 26.3 ms, sys: 10.2 ms, total: 36.5 ms\n",
      "Wall time: 53.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # External JavaScript files\n",
    "# external_scripts = [\n",
    "#     'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js',\n",
    "#     {'src':'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js'}\n",
    "# ]\n",
    "\n",
    "app = JupyterDash(__name__) \n",
    "#                   external_scripts = external_scripts)\n",
    "\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "\n",
    "# Preset variables.\n",
    "ner_labels = ['LOC','GPE']\n",
    "# ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW','DATE']\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef', 'date']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "    \n",
    "#     Title\n",
    "    html.Header(\n",
    "        className=\"app-header\",\n",
    "        children = [\n",
    "            html.Div('nerHelper Application', className = \"app-header--title\")\n",
    "        ]),\n",
    "    \n",
    "    \n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('NER Labels & Definitions'),\n",
    "    \n",
    "#     Add legend & checklist for ner_labels.\n",
    "    html.Table([\n",
    "        html.Thead([\n",
    "            html.Tr([\n",
    "                html.Th('Label'),\n",
    "                html.Th('Definition'),\n",
    "            ]),\n",
    "        ]),\n",
    "        html.Tbody([\n",
    "#             html.Tr([\n",
    "#                 html.Td('PERSON'),\n",
    "#                 html.Td('A person\\'s name (proper noun)' ),\n",
    "#             ]),\n",
    "            html.Tr([\n",
    "                html.Td('LOC'),\n",
    "                html.Td('Non-GPE locations, mountain ranges, bodies of water.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('GPE'),\n",
    "                html.Td('Countries, cities, states.' ),\n",
    "            ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('FAC'),\n",
    "#                 html.Td('Buildings, airports, highways, bridges, etc.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('ORG'),\n",
    "#                 html.Td('Companies, agencies, institutions, etc.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('NORP'),\n",
    "#                 html.Td('Nationalities or religious or political groups.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('EVENT'),\n",
    "#                 html.Td('Named hurricanes, battles, wars, sports events, etc.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('WORK_OF_ART'),\n",
    "#                 html.Td('Titles of books, songs, etc.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('LAW'),\n",
    "#                 html.Td('Named documents made into laws.' ),\n",
    "#             ]),\n",
    "#             html.Tr([\n",
    "#                 html.Td('DATE'),\n",
    "#                 html.Td('Absolute or relative dates or periods.' ),\n",
    "#             ]),\n",
    "        ]),\n",
    "    ]),\n",
    "    \n",
    "    #     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    \n",
    "    dcc.Checklist(\n",
    "        className = 'ner-checklist',\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        className = 'upload-data',\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=False # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    \n",
    "#     Store uploaded data.\n",
    "    dcc.Store(id = 'data-upload-store'),\n",
    "    \n",
    "#     Display pane for file information.\n",
    "    html.Div(className = 'file-information', id = 'file-information'),\n",
    "    \n",
    "    \n",
    "#     Display pane for data as table.\n",
    "    dash_table.DataTable(id = 'data-table-container',\n",
    "                         row_selectable=\"single\",\n",
    "                         selected_rows = [0],\n",
    "                         editable = True,\n",
    "                         page_size=10,\n",
    "                        ),\n",
    "    \n",
    "    html.Div(id = 'download-button-container'),\n",
    "    \n",
    "    html.Div(id = 'file-downloaded-container')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "######### Callbacks ################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Upload data & create table.\n",
    "@app.callback([Output('file-information', 'children'),\n",
    "               Output('data-upload-store', 'data')],\n",
    "              [Input('upload-data', 'contents'),\n",
    "               Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'),\n",
    "               State('upload-data', 'last_modified')])\n",
    "def upload_data(contents, ner_values, filename, date):\n",
    "    if contents is None:\n",
    "        raise PreventUpdate\n",
    "            \n",
    "    data = parse_contents(contents, filename, ner_values)\n",
    "    \n",
    "    file_information = html.Div([html.P(f'File name: {filename}'),\n",
    "                                 html.P(f'Last modified: {datetime.datetime.fromtimestamp(date)}')])\n",
    "    \n",
    "    return file_information, data.to_dict('rows')\n",
    "\n",
    "\n",
    "# Generate table with data from store.\n",
    "@app.callback([Output('data-table-container', 'data'),\n",
    "               Output('data-table-container', 'columns')],\n",
    "              Input('data-upload-store', 'data'))\n",
    "def populate_data_table(data):\n",
    "\n",
    "    df = pd.DataFrame(data)[['file', 'entity', 'label']]\n",
    "    cols = [{'name':i, 'id': i} for i in df.columns]\n",
    "\n",
    "    return df.to_dict('rows'), cols\n",
    "\n",
    "\n",
    "# After last revision (or whenever one change completed), provide button to commit changes to XML.\n",
    "@app.callback(Output('download-button-container', 'children'),\n",
    "              Input('data-upload-store', 'data'))\n",
    "def provide_download_button(data):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    return html.Button('Download NER Suggestions as CSV.', \n",
    "                       id = 'download-button', className = 'download-button')\n",
    "\n",
    "\n",
    "@app.callback(Output('file-downloaded-container', 'children'),\n",
    "              Input('download-button-container', 'n_clicks'),\n",
    "              [State('data-upload-store', 'data'),\n",
    "               State('upload-data', 'filename')])\n",
    "def download_csv(n_clicks, data, filename):\n",
    "    download_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "    \n",
    "    if download_id != 'download-button-container.n_clicks':\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    reFile = re.match(r'(.*).xml', filename).group(1)\n",
    "    \n",
    "    path = f\"{reFile}.csv\"\n",
    "    with open(path, \"w\") as file:\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "#         Create accept column.\n",
    "        df['accept'] = ''\n",
    "#         Re-organize column order.\n",
    "        df = df[['accept', 'entity', 'label', 'uniq_id', 'previous_encoding', 'new_encoding', \n",
    "                 'entities', 'abridged_xpath', 'descendant_order', 'file']]\n",
    "    \n",
    "        df.to_csv(file, sep = ',')\n",
    "        \n",
    "    return html.P(f'{reFile}.csv downloaded!')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash\n",
    "    app.run_server(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e0d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
