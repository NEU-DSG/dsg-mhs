{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from urllib.parse import quote as urlquote\n",
    "from flask import Flask, send_from_directory, send_file\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 0 ns, total: 14 µs\n",
      "Wall time: 16 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(elem):\n",
    "    while elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is None:\n",
    "        elem = elem.getparent()\n",
    "        \n",
    "        if elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "            ancestor = elem.getparent().tag\n",
    "            xml_id = elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "            \n",
    "            abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]'\n",
    "            return abridged_xpath\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Intersperse Entity with Likely TEI Information for Capacious Regex\n",
    "\"\"\"\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 - 0)\n",
    "    result[0::2] = lst\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entities, label_dict):\n",
    "    previous_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    entity = entities[0]\n",
    "    label = label_dict[entities[1]]\n",
    "    \n",
    "    try:\n",
    "    #     Create regex that anticipates additional encoding anywhere in tag content.\n",
    "    #     Break up entity by character to intersperse possible TEI interruptions.\n",
    "        expanded_entity = [c for c in entity]\n",
    "        expanded_regex = '[' + \"|\".join(['<.*>', '</.*>', '\\s*']) + ']*'\n",
    "\n",
    "    #     Intersperse possible encoding within entity.\n",
    "        expanded_regex =  r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "        match = re.search(expanded_regex, previous_encoding, re.VERBOSE|re.DOTALL)\n",
    "\n",
    "    #     If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        if match:\n",
    "            new_encoding = re.sub(f'{match.group(0)}',\n",
    "                                  f'<{label}>{match.group(0)}</{label}>',\n",
    "                                  previous_encoding)\n",
    "\n",
    "            return new_encoding # Check if encoding is well formed?\n",
    "\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, label_dict):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in label_dict.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, label_dict) # Get named entities from plain text.\n",
    "    \n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    accept_change\n",
    "    make_hand_edits\n",
    "    add_unique_identifier\n",
    "\"\"\"\n",
    "def revise_with_hand_edits(label_dict, row):\n",
    "    previous_encoding = re.sub('\\s+', ' ', row['previous_encoding'], re.MULTILINE)\n",
    "    \n",
    "#     Using re.sub because pandas is converting tuple to string.\n",
    "    entity = re.sub(\"\\(\\'(.*)\\',\\'(.*)'\\)\", '\\g<1>', row['entities'])\n",
    "    label = label_dict[re.sub(\"\\(\\'(.*)\\',\\'(.*)'\\)\", '\\g<2>', row['entities'])]\n",
    "\n",
    "#     Create regex that anticipates additional encoding anywhere in tag content.\n",
    "#     Break up entity by character to intersperse possible TEI interruptions.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['<.*>', '</.*>', '\\s*']) + ']*'\n",
    "\n",
    "#     Intersperse possible encoding within entity.\n",
    "#     row['previous_encoding'] requires [0] to grab contents.\n",
    "    expanded_regex =  r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    match = re.search(expanded_regex, previous_encoding, re.VERBOSE|re.DOTALL)\n",
    "    \n",
    "#     If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "    if match != None:\n",
    "\n",
    "#             If there is a unique id to add & hand edits...\n",
    "        if row['add_unique_identifier'] != '' and row['make_hand_edits'] != '':\n",
    "            identifier_regex = re.search('(<.+)>.+</.+>', row['make_hand_edits'], re.VERBOSE|re.DOTALL)\n",
    "            new_edit = identifier_regex.group(1) + 'xml:id=\"{}\"'.format(row['add_unique_identifier'])\n",
    "\n",
    "            new_match = re.sub(f'{identifier_regex.group(1)}',\n",
    "                               f'{new_edit}',\n",
    "                               row['make_hand_edits'])\n",
    "            \n",
    "            revised_encoding = re.sub(f'{match.group(0)}',\n",
    "                          new_match + ' ',\n",
    "                          row['previous_encoding'])\n",
    "\n",
    "    #             Clean up any additional whitespace.\n",
    "            revised_encoding = re.sub('\\s+', ' ', revised_encoding, re.MULTILINE)\n",
    "\n",
    "            return revised_encoding # Check if encoding is well formed?\n",
    "\n",
    "\n",
    "\n",
    "#             If there are ONLY unique ids to add an NO hand edits...\n",
    "        elif row['add_unique_identifier'] != '' and row['make_hand_edits'] == '':\n",
    "            identifier_regex = re.search('(<.+)>.+</.+>', match.group(0), re.VERBOSE|re.DOTALL)\n",
    "            new_edit = identifier_regex.group(1) + 'xml:id=\"{}\"'.format(row['add_unique_identifier'])\n",
    "\n",
    "            new_match = re.sub(f'{identifier_regex.group(1)}',\n",
    "                               f'{new_edit}',\n",
    "                               identifier_regex.group(0))\n",
    "            \n",
    "            revised_encoding = re.sub(f'{match.group(0)}',\n",
    "                                      new_match + ' ',\n",
    "                                      row['previous_encoding'])\n",
    "\n",
    "    #             Clean up any additional whitespace.\n",
    "            revised_encoding = re.sub('\\s+', ' ', revised_encoding, re.MULTILINE)\n",
    "\n",
    "            return revised_encoding # Check if encoding is well formed?\n",
    "    \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    accept_change\n",
    "    make_hand_edits\n",
    "    add_unique_identifier\n",
    "\"\"\"\n",
    "def inherit_changes(label_dict, dataframe):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "#         If HAND changes are accepted...\n",
    "        if row['accept_change'] == 'y' and (row['make_hand_edits'] != '' or row['add_unique_identifier'] != ''):\n",
    "        \n",
    "            revised_by_hand = revise_with_hand_edits(label_dict, row)\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "            \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "        \n",
    "#         If NER suggestions are accepted as-is...\n",
    "        elif row['accept_change'] == 'y' and row['make_hand_edits'] == '' and row['add_unique_identifier'] == '':\n",
    "        \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "                \n",
    "#         If changes are rejected...\n",
    "        else:\n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "\n",
    "        \n",
    "    dataframe = dataframe.groupby('abridged_xpath').tail(1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Write New XML File with Accepted Revisions\n",
    "Expects:\n",
    "    XML File with Original Encoding\n",
    "    CSV File with Accepted Changes\n",
    "    Label Dictionary\n",
    "\"\"\"\n",
    "def revise_xml(xml_in, csv_df, label_dict):\n",
    "    with open(xml_in, 'r') as xml_in: #, open(output_filename, 'wb') as xml_out:\n",
    "    #     First, update data to reflect accepted changes.\n",
    "        new_data = inherit_changes(label_dict, csv_df)\n",
    "\n",
    "        tree = etree.parse(xml_in)\n",
    "        root = tree.getroot()\n",
    "        ns = get_namespace(root)\n",
    "\n",
    "        tree_as_string = etree.tostring(tree, pretty_print = True).decode('utf-8')\n",
    "        tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "\n",
    "    #     Declare accepted encoding to be written.\n",
    "    #     For each entry in file...\n",
    "        for child in root.findall('.//ns:p', ns):\n",
    "\n",
    "    #         Store original encoding.\n",
    "            original_encoding_as_string = get_encoding(child)\n",
    "    #         Removing namespace information embedded in <p> tags.\n",
    "            original_encoding_as_string = re.sub('(<p)(.*1.0\")(>)',\n",
    "                                                 '\\\\1\\\\3',\n",
    "                                                 original_encoding_as_string)\n",
    "\n",
    "    #         Get xpath of child and write full xpath with namespaces using dictionary\n",
    "            abridged_xpath = get_abridged_xpath(child)\n",
    "            for key, value in ns.items():\n",
    "                full_xpath = re.sub('(.*)(xml:)(.*)', '\\\\1{http://www.w3.org/XML/1998/namespace}\\\\3', abridged_xpath)\n",
    "\n",
    "            accepted_encoding_as_string = new_data.loc[new_data['abridged_xpath'] == abridged_xpath, 'new_encoding'][1]\n",
    "            accepted_encoding_as_string = re.sub('(<p)(.*1.0\")(>)',\n",
    "                                                 '\\\\1\\\\3',\n",
    "                                                 accepted_encoding_as_string)\n",
    "\n",
    "\n",
    "            tree_as_string = re.sub(original_encoding_as_string,\n",
    "                                    accepted_encoding_as_string,\n",
    "                                    tree_as_string)\n",
    "\n",
    "    #     Check well-formedness (will fail if not well-formed)\n",
    "        doc = etree.fromstring(tree_as_string)\n",
    "\n",
    "    #     Write changed XML.\n",
    "        et = etree.ElementTree(doc)\n",
    "        return (et)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "App Function: Write Filename when Uploaded\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename):\n",
    "    return html.Div([\n",
    "        html.H4(f'{filename} succesfully uploaded')\n",
    "    ])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "App Function: Make XML Revisions and Provide Download Link\n",
    "\"\"\"\n",
    "def make_xml_revisions(xml_contents, csv_contents):\n",
    "    try:\n",
    "#         XML Contents\n",
    "        xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "        xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "        xml_file = xml_decoded.encode('utf-8')\n",
    "    \n",
    "    except Exception as e:\n",
    "        return html.Div([f'There was an error processing {xml_filename}: {e}.'])\n",
    "    \n",
    "    try:    \n",
    "#         CSV Contents\n",
    "        csv_content_type, csv_content_string = csv_contents.split(',')\n",
    "        csv_decoded = base64.b64decode(csv_content_string).decode('utf-8')\n",
    "        csv_df = pd.read_csv(io.StringIO(csv_decoded))\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([f'There was an error processing {csv_filename}: {e}.'])\n",
    "    \n",
    "    \n",
    "    revised_data = revise_xml(xml_file, csv_file, label_dict)\n",
    "    \n",
    "    return revised_data\n",
    "# #     Return HTML with outputs.\n",
    "#     return html.Div([\n",
    "#         html.A(\n",
    "#         html.Button('Download New XML File'),\n",
    "#         id = 'download-link',\n",
    "#         download = 'New Data...',\n",
    "#         href = \"\",\n",
    "#         target = \"_blank\"\n",
    "#     ),\n",
    "        \n",
    "# #         Break & Horizontal line\n",
    "#         html.Br(),\n",
    "#         html.Hr(),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc1427fe370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 3.98 ms, total: 24.7 ms\n",
      "Wall time: 367 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_dict = {\n",
    "    'PERSON':'persName',\n",
    "    'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "    'GPE':'placeName', # Countries, cities, states.\n",
    "    'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "    'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "    'NORP':'name', # Nationalities or religious or political groups.\n",
    "    'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "    'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "    'LAW':'name', # Named documents made into laws.\n",
    "}\n",
    "\n",
    "server = Flask(__name__)\n",
    "app = JupyterDash(__name__, server = server)\n",
    "\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "#     Title\n",
    "    html.H1('XML & NER Revision'),\n",
    "\n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload Original XML File'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-xml',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    html.Div(id = 'xml-data-upload'),\n",
    "    \n",
    "    \n",
    "    #     Upload Data Area.\n",
    "    html.H2('Upload CSV File with Accepted Changes'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-csv',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    html.Div(id = 'csv-data-upload'),\n",
    "    \n",
    "    html.Div(id = 'csv-table'),\n",
    "    \n",
    "    html.A(\n",
    "        html.Button('Download New XML File'),\n",
    "        id = 'download-link',\n",
    "        download = 'New Data...',\n",
    "        href = \"revisions\",\n",
    "        target = \"_blank\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# App Callbacks\n",
    "# Write XML Filename when Uploaded\n",
    "@app.callback(\n",
    "    Output('xml-data-upload', 'children'),\n",
    "    Input('upload-xml', 'contents'),\n",
    "    State('upload-xml', 'filename')\n",
    ")\n",
    "def update_xml_file(xml_contents, xml_filename):\n",
    "    if xml_contents is not None:\n",
    "        xml_children = [\n",
    "            parse_contents(xc, xf) for xc, xf in zip(xml_contents, xml_filename)\n",
    "        ]\n",
    "        return xml_children\n",
    "\n",
    "# Write CSV Filename when Uploaded\n",
    "@app.callback(\n",
    "    Output('csv-data-upload', 'children'),\n",
    "    Input('upload-csv', 'contents'),\n",
    "    State('upload-csv', 'filename')\n",
    ")\n",
    "def update_csv_file(csv_contents, csv_filename):\n",
    "    if csv_contents is not None:\n",
    "        csv_children = [\n",
    "            parse_contents(cc, cf) for cc, cf in zip(csv_contents, csv_filename)\n",
    "        ]\n",
    "        return csv_children\n",
    "\n",
    "# # Create DATATABLE TO VERIFY CONTENTS\n",
    "# @app.callback(\n",
    "#     Output('csv-table', 'children'),\n",
    "#     Input('upload-csv', 'contents')\n",
    "# )\n",
    "# def update_table(csv_contents):\n",
    "#     if csv_contents is not None:\n",
    "#         return dash_table.DataTable(\n",
    "#             data = pd.read_csv(csv_contents, sep = ',').to_dict('records'),\n",
    "#             columns = [{'name':i, 'id':i} for i in df.columns],\n",
    "#             page_size=5,\n",
    "#             export_format = 'csv',            \n",
    "#         )\n",
    "    \n",
    "    \n",
    "# # Create NEW XML File with Revisions.\n",
    "# @app.callback(\n",
    "#     Output('download-link', 'href'),\n",
    "#     [Input('upload-xml', 'xml-contents'), Input('upload-csv', 'csv-contents')]\n",
    "# )\n",
    "\n",
    "# def provide_download_link(xml_contents, csv_contents):\n",
    "#     if xml_contents is not None and csv_contents is not None:\n",
    "#         revised_children = [\n",
    "#             make_xml_revisions(xc, cc) for xc, cc in zip(xml_contents, csv_contents)\n",
    "#         ]\n",
    "        \n",
    "#         download_string = \"data:text/csv;charset=utf-8,\" + urllib.quote(revised_children)\n",
    "        \n",
    "#         return download_string\n",
    "\n",
    "@server.route(\"/downloadable/<path>\")\n",
    "def download_file (path = None):\n",
    "    return send_file(\"downloadable/\" + path, as_attachment=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test With Dynamically Generated BUTTON & TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
