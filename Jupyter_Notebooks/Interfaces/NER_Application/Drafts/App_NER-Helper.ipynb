{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: XML Uploader & NER Helper\n",
    "\n",
    "https://dash.plotly.com/dash-core-components/upload\n",
    "\n",
    "Susan Li: https://towardsdatascience.com/parsing-xml-named-entity-recognition-in-one-shot-629a8b9846ee\n",
    "\n",
    "StackOverflow: https://stackoverflow.com/questions/54443531/downloading-dynamically-generated-files-from-a-dash-flask-app\n",
    "\n",
    "Dash Recipes: https://github.com/plotly/dash-recipes/blob/master/dash-download-file-link-server.py\n",
    "\n",
    "Faculty Platform: https://docs.faculty.ai/user-guide/apps/examples/dash_file_upload_download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(elem):\n",
    "    while elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is None:\n",
    "        elem = elem.getparent()\n",
    "        \n",
    "        if elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "            ancestor = elem.getparent().tag\n",
    "            xml_id = elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "            \n",
    "            abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]'\n",
    "            return abridged_xpath\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Intersperse Entity with Likely TEI Information for Capacious Regex\n",
    "\"\"\"\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 - 0)\n",
    "    result[0::2] = lst\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entities, label_dict):\n",
    "    previous_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    entity = entities[0]\n",
    "    label = label_dict[entities[1]]\n",
    "    \n",
    "    try:\n",
    "    #     Create regex that anticipates additional encoding anywhere in tag content.\n",
    "    #     Break up entity by character to intersperse possible TEI interruptions.\n",
    "        expanded_entity = [c for c in entity]\n",
    "        expanded_regex = '[' + \"|\".join(['<.*>', '</.*>', '\\s*']) + ']*'\n",
    "\n",
    "    #     Intersperse possible encoding within entity.\n",
    "        expanded_regex =  r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "        match = re.search(expanded_regex, previous_encoding, re.VERBOSE|re.DOTALL)\n",
    "\n",
    "    #     If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        if match:\n",
    "            new_encoding = re.sub(f'{match.group(0)}',\n",
    "                                  f'<{label}>{match.group(0)}</{label}>',\n",
    "                                  previous_encoding)\n",
    "\n",
    "            return new_encoding # Check if encoding is well formed?\n",
    "\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'      \n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 0 ns, total: 10 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    label_dict = {'PERSON':'persName',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "\n",
    "            for child in root.findall('.//ns:p', ns):\n",
    "\n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                entities, previous_encoding = get_contents(child, './/ns:p', ns, subset_ner)\n",
    "\n",
    "                df = df.append({\n",
    "                    'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "                    'abridged_xpath':abridged_xpath,\n",
    "                    'previous_encoding': previous_encoding,\n",
    "                    'entities':entities,\n",
    "                },\n",
    "                    ignore_index = True)\n",
    "\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "            \n",
    "\n",
    "            df['ner_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entities'],\n",
    "                                                        subset_ner),\n",
    "                       axis = 1)\n",
    "            \n",
    "\n",
    "            # Add additional columns for user input.\n",
    "            df['accept_changes?'] = ''\n",
    "            df['make_hand_edits'] = ''\n",
    "            df['add_unique_identifier'] = ''\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return html.Div([\n",
    "        \n",
    "#         Print file info.\n",
    "        html.Div([\n",
    "            html.H4('File Information'),\n",
    "            html.P(f'{filename}, {datetime.datetime.fromtimestamp(date)}'),\n",
    "        ]),\n",
    "        \n",
    "        html.Br(),\n",
    "        \n",
    "#         Return data table of element and attribute info.\n",
    "        dash_table.DataTable(\n",
    "            data = df.to_dict('records'),\n",
    "            columns = [{'name':i, 'id':i} for i in df.columns],\n",
    "            page_size=5,\n",
    "            export_format = 'csv',\n",
    "\n",
    "            style_cell_conditional=[\n",
    "                {\n",
    "                    'if': {'column_id': c},\n",
    "                    'textAlign': 'left'\n",
    "                } for c in ['Date', 'Region']\n",
    "            ],\n",
    "            style_data_conditional=[\n",
    "                {\n",
    "                    'if': {'row_index': 'odd'},\n",
    "                    'backgroundColor': 'rgb(248, 248, 248)'\n",
    "                }\n",
    "            ],\n",
    "            style_header={\n",
    "                'backgroundColor': 'rgb(230, 230, 230)',\n",
    "                'fontWeight': 'bold'\n",
    "            }\n",
    "        ),\n",
    "\n",
    "# #         Horizontal line\n",
    "#         html.Hr(),\n",
    "        \n",
    "# #         For debugging, display the raw contents provided by the web browser\n",
    "#         html.Div('Raw Content'),\n",
    "#         html.Pre(contents[0:200] + '...', style={\n",
    "#             'whiteSpace': 'pre-wrap',\n",
    "#             'wordBreak': 'break-all'\n",
    "#         })\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP\n",
    "\n",
    "Currently assumes every child is a possible document (if they aren't, it shouldn't matter...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd6b8af66d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 3.5 ms, total: 20.9 ms\n",
      "Wall time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "# Preset Variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "#     Title\n",
    "    html.H1('XML Uploader & NER Helper'),\n",
    "\n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    dcc.Checklist(\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['PERSON', 'LOC']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    html.Div(id = 'output-data-upload'),\n",
    "\n",
    "])\n",
    "\n",
    "# Callbacks.\n",
    "# Upload callback variables & function.\n",
    "@app.callback(Output('output-data-upload', 'children'),\n",
    "              [Input('upload-data', 'contents'), Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'), State('upload-data', 'last_modified')])\n",
    "\n",
    "def update_output(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents is not None:\n",
    "        children = [\n",
    "            parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "        ]\n",
    "        return children\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
