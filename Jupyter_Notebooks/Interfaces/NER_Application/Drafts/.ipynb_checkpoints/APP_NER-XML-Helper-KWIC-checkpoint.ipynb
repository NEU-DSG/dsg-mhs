{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/SemanticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# XML Parsing Function: Retrieve XPaths\n",
    "# \"\"\"\n",
    "# def get_abridged_xpath(child):\n",
    "#     while child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is None:\n",
    "#         elem = child.getparent()\n",
    "        \n",
    "#         if elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "#             ancestor = elem.getparent().tag\n",
    "#             xml_id = elem.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "            \n",
    "#             abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "#             return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\n",
    "DOES NOT USE WHILE LOOP, which was breaking after adjustments.\n",
    "\"\"\"\n",
    "def get_abridged_xpath(child):\n",
    "    if child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "        ancestor = child.getparent().tag\n",
    "        xml_id = child.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "\n",
    "        abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "        return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Function: Build KWIC of Found Entities in Up Converted Encoding\n",
    "\"\"\"\n",
    "def get_kwic_encoding(converted_entity, converted_encoding, kwic_range):\n",
    "#     Create keyword-in-context encoding fragments.\n",
    "    c_entity_sequence = converted_entity.split(' ')\n",
    "    c_encoding_sequence = converted_encoding.split(' ')\n",
    "    \n",
    "#     If tokenized sequence of converted entity is a subset in tokenized encoding...\n",
    "    if c_encoding_sequence <= c_entity_sequence:\n",
    "        sequence_list = []\n",
    "        \n",
    "#         Create a list of index values where c_entity_sequence occurs in converted_encoding\n",
    "        for i in c_entity_sequence:\n",
    "            for idx, item in enumerate(c_encoding_sequence):\n",
    "                if item == i:\n",
    "                    sequence_list.append(idx)\n",
    "            \n",
    "                else:\n",
    "#                     Create regex to account for possible encoding within converted entity.\n",
    "                    conv_ent_re = re.sub('<w>(.*)</w>', '<w>.*\\\\1.*</w>', i)\n",
    "                    if re.match(conv_ent_re, item):\n",
    "                        sequence_list.append(idx)\n",
    "        \n",
    "#         Return KWIC using list range.\n",
    "        if sequence_list[0] - kwic_range < 0:\n",
    "#             If first item in KWIC - kwic range is less than zero, start KWIC view at beginning of string.\n",
    "#             Last item in list = sequence_list[-1] + 1\n",
    "            kwic_encoding = c_encoding_sequence[0: sequence_list[-1]  + 1 + kwic_range]\n",
    "        else:\n",
    "            kwic_encoding = c_encoding_sequence[sequence_list[0] - kwic_range: sequence_list[-1] + 1 + kwic_range]\n",
    "\n",
    "#         Convert list to string.\n",
    "        kwic_encoding = ' '.join(kwic_encoding)\n",
    "\n",
    "        return kwic_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "#     Get KWIC Encoding.\n",
    "    prev_kwic_encoding = get_kwic_encoding(converted_entity, converted_encoding, kwic_range)\n",
    "    prev_kwic_encoding = re.sub('<[/]?w>', '', prev_kwic_encoding)\n",
    "\n",
    "#     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "    try:\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "        \n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            return prev_kwic_encoding, \"Already Encoded\", \"Already Encoded\"\n",
    "        \n",
    "#         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "    \n",
    "#             Repeate process for KWIC\n",
    "            new_kwic_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  prev_kwic_encoding)\n",
    "            \n",
    "            new_kwic_encoding = re.sub('<[/]?w>', '', new_kwic_encoding)\n",
    "            new_kwic_encoding = re.sub('_', ' ', new_kwic_encoding)\n",
    "\n",
    "            return prev_kwic_encoding, new_encoding, new_kwic_encoding\n",
    "\n",
    "        else:\n",
    "            return prev_kwic_encoding, 'Error Making NER Suggestions', 'Error Making NER Suggestions'\n",
    "    \n",
    "#     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "#     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "#     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(child, df, ns, subset_ner, filename, descendant_order):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    entities, previous_encoding = get_contents(child, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "#         'abridged_xpath':abridged_xpath,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    label_dict = {'PERSON':'persName',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "            \n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                    df['abridged_xpath'] = abridged_xpath\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "            \n",
    "            df['new_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entity'],\n",
    "                                                        row['label'],\n",
    "                                                        subset_ner, 4, banned_list),\n",
    "                       axis = 1)\n",
    "            \n",
    "            df[['prev_kwic', 'new_encoding', 'new_kwic']] = pd.DataFrame(df['new_encoding'].tolist(),\n",
    "                                                                         index = df.index)\n",
    "\n",
    "            \n",
    "            # Add additional columns for user input.\n",
    "            df['accept_changes'] = ''\n",
    "            df['make_hand_edits'] = ''\n",
    "            \n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "            df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return html.Div([\n",
    "        \n",
    "#         Print file info.\n",
    "        html.Div([\n",
    "            html.H4('File Information'),\n",
    "            html.P(f'{filename}, {datetime.datetime.fromtimestamp(date)}'),\n",
    "        ]),\n",
    "        \n",
    "        html.Br(),\n",
    "        \n",
    "#         Return data table of element and attribute info.\n",
    "        dash_table.DataTable(\n",
    "            data = df.to_dict('records'),\n",
    "            columns = [{'name':i, 'id':i} for i in df.columns],\n",
    "            page_size=5,\n",
    "            export_format = 'csv',\n",
    "\n",
    "            style_cell_conditional=[\n",
    "                {\n",
    "                    'if': {'column_id': c},\n",
    "                    'textAlign': 'left'\n",
    "                } for c in ['Date', 'Region']\n",
    "            ],\n",
    "            style_data_conditional=[\n",
    "                {\n",
    "                    'if': {'row_index': 'odd'},\n",
    "                    'backgroundColor': 'rgb(248, 248, 248)'\n",
    "                }\n",
    "            ],\n",
    "            style_header={\n",
    "                'backgroundColor': 'rgb(230, 230, 230)',\n",
    "                'fontWeight': 'bold'\n",
    "            }\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa8f2cff310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 ms, sys: 4.81 ms, total: 27 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "# Preset Variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW']\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "#     Title\n",
    "    html.H1('Named Entity Recognition Helper'),\n",
    "\n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    dcc.Checklist(\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['PERSON', 'LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    html.Div(id = 'output-data-upload'),\n",
    "\n",
    "])\n",
    "\n",
    "# Callbacks.\n",
    "# Upload callback variables & function.\n",
    "@app.callback(Output('output-data-upload', 'children'),\n",
    "              [Input('upload-data', 'contents'), Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'), State('upload-data', 'last_modified')])\n",
    "\n",
    "def update_output(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents is not None:\n",
    "        children = [\n",
    "            parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "        ]\n",
    "        return children\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 784 µs, sys: 800 µs, total: 1.58 ms\n",
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filename = abs_dir + \"Data/TestEncoding/EditingData/test_xml-before.xml\"\n",
    "# filename = abs_dir + \"Data/TestEncoding/EditingData/JQADiaries-v33-1821-12-p001_copy.xml\"\n",
    "xml_file = open(filename).read()\n",
    "root = etree.fromstring(xml_file.encode('utf-8'))\n",
    "ns = get_namespace(root)\n",
    "\n",
    "subset_ner = {'PERSON':'persName', 'LOC':'placeName'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Function: Build KWIC of Found Entities in Up Converted Encoding\n",
    "\"\"\"\n",
    "def get_kwic_encoding(entity, converted_encoding, kwic_range):\n",
    "#     Create keyword-in-context encoding fragments.\n",
    "\n",
    "    expanded_conv_regex =  '<w>' + ''.join([e + '(<.*?>)*' for e in list(entity)]) + '.*?</w>'\n",
    "\n",
    "    c_entity_sequence = expanded_conv_regex.split(' ')\n",
    "    c_encoding_sequence = converted_encoding.split(' ')\n",
    "\n",
    "\n",
    "#     Create a list of index values\n",
    "    sequence_list = []\n",
    "\n",
    "    for i in c_entity_sequence:\n",
    "        for idx, item in enumerate(c_encoding_sequence):\n",
    "            if re.search(i, item):\n",
    "                sequence_list.append(idx)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "#     Return KWIC using list range.\n",
    "    if sequence_list:\n",
    "        if sequence_list[0] - kwic_range < 0:\n",
    "#             If first item in KWIC - kwic range is less than zero, start KWIC view at beginning of string.\n",
    "#             Last item in list = sequence_list[-1] + 1\n",
    "            kwic_encoding = c_encoding_sequence[0: sequence_list[-1] + 1 + kwic_range]\n",
    "        else:\n",
    "            kwic_encoding = c_encoding_sequence[sequence_list[0] - kwic_range: sequence_list[-1] + 1 + kwic_range]\n",
    "\n",
    "#         Convert list to string.\n",
    "        kwic_encoding = ' '.join(kwic_encoding)\n",
    "        return kwic_encoding\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    \n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)    \n",
    "\n",
    "    try:\n",
    "# #     Get KWIC Encoding.\n",
    "#         prev_kwic_encoding = get_kwic_encoding(entity, converted_encoding, kwic_range)\n",
    "#         prev_kwic_encoding = re.sub('<[/]?w>', '', prev_kwic_encoding)\n",
    "        \n",
    "        \n",
    "    #     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "        expanded_conv_regex =  '<w>' + ''.join([e + '(<.*?>)*' for e in list(entity)]) + '.*?</w>'\n",
    "        entity_match = re.search(expanded_conv_regex, converted_encoding)\n",
    "        print (expanded_conv_regex, '\\t', entity_match)\n",
    "\n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            print ('Already Encoded')\n",
    "#             return prev_kwic_encoding, \"Already Encoded\", \"Already Encoded\"\n",
    "            \n",
    "        #         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "    \n",
    "# #             Repeate process for KWIC\n",
    "#             new_kwic_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "#                                   f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "#                                   prev_kwic_encoding)\n",
    "            \n",
    "#             new_kwic_encoding = re.sub('<[/]?w>', '', new_kwic_encoding)\n",
    "#             new_kwic_encoding = re.sub('_', ' ', new_kwic_encoding)\n",
    "\n",
    "#             return prev_kwic_encoding, new_encoding, new_kwic_encoding\n",
    "            print ('New Encoding')\n",
    "\n",
    "\n",
    "        else:\n",
    "            print ('NER Suggestions Error')\n",
    "#             return prev_kwic_encoding, 'Error Making NER Suggestions', 'Error Making NER Suggestions'\n",
    "    \n",
    "# #     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "# #     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "# #     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        print ('Error', converted_encoding)\n",
    "        return f'Error Occurred with Regex: {entity}', f'Error Occurred with Regex: {entity}', f'Error Occurred with Regex: {entity}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<w>W(<.*?>)*.(<.*?>)* (<.*?>)*A(<.*?>)*.(<.*?>)* (<.*?>)*S(<.*?>)*c(<.*?>)*h(<.*?>)*o(<.*?>)*o(<.*?>)*l(<.*?>)*f(<.*?>)*i(<.*?>)*e(<.*?>)*l(<.*?>)*d(<.*?>)*.*?</w> \t <re.Match object; span=(173, 221), match='<w>W.</w> <w>A.</w> <w>Schoolfield</w> <w>at</w>'>\n",
      "New Encoding\n",
      "<w>A(<.*?>)*b(<.*?>)*e(<.*?>)*l(<.*?>)*.*?</w> \t <re.Match object; span=(1522, 1553), match='<w>Abel</persRef></w> <w>is</w>'>\n",
      "Already Encoded\n",
      "<w>W(<.*?>)*.(<.*?>)* (<.*?>)*A(<.*?>)*.(<.*?>)* (<.*?>)*S(<.*?>)*c(<.*?>)*h(<.*?>)*o(<.*?>)*o(<.*?>)*l(<.*?>)*f(<.*?>)*i(<.*?>)*e(<.*?>)*l(<.*?>)*d(<.*?>)*.*?</w> \t <re.Match object; span=(244, 283), match='<w>W.</w> <w>A.</w> <w>Schoolfield.</w>'>\n",
      "New Encoding\n",
      "<w>R(<.*?>)*.(<.*?>)* (<.*?>)*B(<.*?>)*.(<.*?>)* (<.*?>)*T(<.*?>)*a(<.*?>)*n(<.*?>)*e(<.*?>)*y(<.*?>)*.*?</w> \t None\n",
      "Error <w><closer_xmlns=\"http://www.tei-c.org/ns/1.0\"_xmlns:mhs=\"http://www.masshist.org/ns/1.0\"_xmlns:tei=\"http://www.tei-c.org/ns/1.0\"></w> <w><salute>I</w> <w>am</w> <w>dear</w> <w>sir</w> <w>/</w> <w>very</w> <w>truly</w> <w>your</w> <w>/</w> <w>friend</w> <w></salute></w> <w><signed>R.</w> <w>B.</w> <w>Taney</signed></w> <w></closer></w> <w></w>\n",
      "CPU times: user 74 ms, sys: 2.02 ms, total: 76 ms\n",
      "Wall time: 76 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>abridged_xpath</th>\n",
       "      <th>previous_encoding</th>\n",
       "      <th>entities</th>\n",
       "      <th>descendant_order</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "      <th>new_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_xml-before.xml</td>\n",
       "      <td>.//ns:body//{http://www.tei-c.org/ns/1.0}div[@...</td>\n",
       "      <td>&lt;p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...</td>\n",
       "      <td>(W. A. Schoolfield, PERSON)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_xml-before.xml</td>\n",
       "      <td>.//ns:body//{http://www.tei-c.org/ns/1.0}div[@...</td>\n",
       "      <td>&lt;p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...</td>\n",
       "      <td>(Abel, PERSON)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Abel</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_xml-before.xml</td>\n",
       "      <td>.//ns:body//{http://www.tei-c.org/ns/1.0}div[@...</td>\n",
       "      <td>&lt;p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...</td>\n",
       "      <td>(W. A. Schoolfield, PERSON)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_xml-before.xml</td>\n",
       "      <td>.//ns:body//{http://www.tei-c.org/ns/1.0}div[@...</td>\n",
       "      <td>&lt;closer xmlns=\"http://www.tei-c.org/ns/1.0\" xm...</td>\n",
       "      <td>(R. B. Taney, PERSON)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>R. B. Taney</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>(Error Occurred with Regex: R. B. Taney, Error...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file                                     abridged_xpath  \\\n",
       "2  test_xml-before.xml  .//ns:body//{http://www.tei-c.org/ns/1.0}div[@...   \n",
       "2  test_xml-before.xml  .//ns:body//{http://www.tei-c.org/ns/1.0}div[@...   \n",
       "3  test_xml-before.xml  .//ns:body//{http://www.tei-c.org/ns/1.0}div[@...   \n",
       "4  test_xml-before.xml  .//ns:body//{http://www.tei-c.org/ns/1.0}div[@...   \n",
       "\n",
       "                                   previous_encoding  \\\n",
       "2  <p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...   \n",
       "2  <p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...   \n",
       "3  <p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:m...   \n",
       "4  <closer xmlns=\"http://www.tei-c.org/ns/1.0\" xm...   \n",
       "\n",
       "                      entities  descendant_order             entity   label  \\\n",
       "2  (W. A. Schoolfield, PERSON)               3.0  W. A. Schoolfield  PERSON   \n",
       "2               (Abel, PERSON)               3.0               Abel  PERSON   \n",
       "3  (W. A. Schoolfield, PERSON)               4.0  W. A. Schoolfield  PERSON   \n",
       "4        (R. B. Taney, PERSON)               5.0        R. B. Taney  PERSON   \n",
       "\n",
       "                                        new_encoding  \n",
       "2                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4  (Error Occurred with Regex: R. B. Taney, Error...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "\n",
    "# Search through elements for entities.\n",
    "desc_order = 0\n",
    "for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    for descendant in child:\n",
    "        desc_order = desc_order + 1\n",
    "        df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "        df['abridged_xpath'] = abridged_xpath\n",
    "        \n",
    "df = df \\\n",
    "    .explode('entities') \\\n",
    "    .dropna()\n",
    "\n",
    "df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "\n",
    "df['new_encoding'] = df \\\n",
    "    .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                            row['entity'],\n",
    "                                            row['label'],\n",
    "                                            subset_ner, 4, banned_list),\n",
    "           axis = 1)\n",
    "\n",
    "\n",
    "# df[['prev_kwic', 'new_encoding', 'new_kwic']] = pd.DataFrame(df['new_encoding'].tolist(),\n",
    "#                                                                          index = df.index)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:mhs=\"http://www.masshist.org/ns/1.0\" xmlns:tei=\"http://www.tei-c.org/ns/1.0\"><date>31. V:45. </date>Edwards of Connecticut, with R. Ingersoll&#8212; <persName>None</persName> the Treasury.</p> '"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_encoding\n",
    "\n",
    "new_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
