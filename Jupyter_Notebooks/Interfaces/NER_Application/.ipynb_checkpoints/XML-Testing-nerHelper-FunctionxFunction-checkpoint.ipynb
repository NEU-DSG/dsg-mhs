{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c94fa3f",
   "metadata": {},
   "source": [
    "# nerHelper - Function by Function Testing\n",
    "\n",
    "Below is each function separated into an individual cell for testing. Scrolling down should mimic the steps the nerHelper takes. Some of the user-interaction functions might not appear here as they would need to be de-bugged while app is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54c7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xml.etree.ElementTree as ET ## not called in app.\n",
    "\n",
    "from lxml import etree, isoschematron\n",
    "from lxml.html import fragment_fromstring\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb89945",
   "metadata": {},
   "source": [
    "## Function Order\n",
    "\n",
    "0. [parse_contents()](#parse_contents)\n",
    "    * get_namespace\n",
    "    * get_abridged_xpath\n",
    "    * get_text\n",
    "    * get_encoding\n",
    "    * get_spacy_entities\n",
    "    * get_contents\n",
    "    * make_dataframe\n",
    "    * make_ner_suggestions\n",
    "    \n",
    "parse_contents() and the functions it calls reads in an existing XML file, converts it to plain text, finds named entities, and creates a dataframe of results with their metadata (filename, parent node, etc.).\n",
    "\n",
    "make_ner_suggestions(), also part of parse_contents(), does the important job of transforming plaintext and the found entity back into XML using regular expressions. The old encoding can then be \"found & replaced\" with the new encoding (again, using regular expressions).\n",
    "\n",
    "1. [keyword-in-context](#keyword-in-context)\n",
    "    * keyword-in-context\n",
    "    \n",
    "2. [highlighter()](#highlighter)\n",
    "    * highlighter\n",
    "    \n",
    "\n",
    "3. [revisions](#revisions)\n",
    "    * inherit_changes\n",
    "        * up_convert_encoding\n",
    "        * xml_cleanup\n",
    "        * revise_without_uniq_id\n",
    "\n",
    "4. [update header](#update_header)\n",
    "    * append_change_to_revisionDesc\n",
    "    * append_app_to_appInfo\n",
    "\n",
    "5. [writing](#writing)\n",
    "    * revise_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41bec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 1.13 ms, total: 2.19 ms\n",
      "Wall time: 1.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preset/User-defined variables: static here.\n",
    "label_dict = {'PERSON':'persName',\n",
    "              'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "              'GPE':'placeName', # Countries, cities, states.\n",
    "              'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "              'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "              'NORP':'name', # Nationalities or religious or political groups.\n",
    "              'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "              'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "              'LAW':'name', # Named documents made into laws.\n",
    "              'DATE':'date' # Absolute or relative dates or periods.\n",
    "             }\n",
    "\n",
    "ner_values = ['LOC', 'GPE']\n",
    "subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "# filename = \"JQADiaries-v28-1809-12-p047.xml\"\n",
    "filename = \"CMS1819-03-08-toRobertSedgwickIF.xml\"\n",
    "banned_list = ['persRef', 'date']\n",
    "\n",
    "\n",
    "# xml_string = open(abs_dir + \"Data/PSC/JQA/1809/JQADiaries-v28-1809-12-p047.xml\", \"r+\").read().strip()\n",
    "xml_string = open(abs_dir + \"Data/PSC/Sedgwick/CMS1819-03-08-toRobertSedgwickIF.xml\", \"r+\").read().strip()\n",
    "\n",
    "# Base lxml function: parse xml from a string.\n",
    "root = etree.fromstring(xml_string.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c6a42",
   "metadata": {},
   "source": [
    "<a id=\"get_namespace\">parse_contents</a>\n",
    "\n",
    "## parse_contents\n",
    "\n",
    "The test version here differs slightly than the app. This test reads files in with a local filepath rather than an upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03b6c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_namespace() is working.\n",
      "abridged_xpath(), all functions in get_contents(), and make_dataframe() are working.\n",
      "CPU times: user 202 ms, sys: 3.35 ms, total: 205 ms\n",
      "Wall time: 206 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>abridged_xpath</th>\n",
       "      <th>previous_encoding</th>\n",
       "      <th>entities</th>\n",
       "      <th>descendant_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:opener xmlns:ns0=\"http://www.tei-c.org/ns...</td>\n",
       "      <td>[(Albany, GPE)]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>[(Northampton, GPE)]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file abridged_xpath  \\\n",
       "0  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "1  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "2  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "3  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "4  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "\n",
       "                                   previous_encoding              entities  \\\n",
       "0  <ns0:opener xmlns:ns0=\"http://www.tei-c.org/ns...       [(Albany, GPE)]   \n",
       "1  <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...                    []   \n",
       "2  <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...  [(Northampton, GPE)]   \n",
       "3  <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...                    []   \n",
       "4  <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...                    []   \n",
       "\n",
       "   descendant_order  \n",
       "0               1.0  \n",
       "1               2.0  \n",
       "2               3.0  \n",
       "3               4.0  \n",
       "4               5.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create empty dataframe to store results.\n",
    "df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "ns = get_namespace(root)\n",
    "print ('get_namespace() is working.')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(child):\n",
    "    if child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "        ancestor = child.getparent().tag\n",
    "        xml_id = child.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "\n",
    "        abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "        return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "#     encoding = etree.tostring(elem, with_tail = False, pretty_print = True).decode('utf-8')\n",
    "    encoding = ET.tostring(descendant, method = 'xml').decode('utf-8')\n",
    "    \n",
    "#     encoding = etree.fromstring(encoding)\n",
    "#     encoding = etree.tostring(encoding).decode('utf-8')\n",
    "    \n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "    \n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(child, df, ns, subset_ner, filename, descendant_order):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    entities, previous_encoding = get_contents(child, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "#         'abridged_xpath':abridged_xpath,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "desc_order = 0\n",
    "for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns): # part of parse_contents()\n",
    "    \n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "    for descendant in child:\n",
    "#         print (descendant.tag)\n",
    "#         print (etree.tostring(descendant, with_tail = False))\n",
    "#         print (ET.tostring(descendant, method = 'xml'))\n",
    "    \n",
    "        desc_order = desc_order + 1\n",
    "        df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "        df['abridged_xpath'] = abridged_xpath\n",
    "        \n",
    "print ('abridged_xpath(), all functions in get_contents(), and make_dataframe() are working.')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62908f4f",
   "metadata": {},
   "source": [
    "## Make NER Suggestions \n",
    "\n",
    "Part of parse_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e15106aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.3 ms, sys: 1.38 ms, total: 9.68 ms\n",
      "Wall time: 8.53 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>abridged_xpath</th>\n",
       "      <th>previous_encoding</th>\n",
       "      <th>entities</th>\n",
       "      <th>descendant_order</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:opener xmlns:ns0=\"http://www.tei-c.org/ns...</td>\n",
       "      <td>(Albany, GPE)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>(Northampton, GPE)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Northampton</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...</td>\n",
       "      <td>(Boston, GPE)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Boston</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CMS1819-03-08-toRobertSedgwickIF.xml</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;ns0:postscript xmlns:ns0=\"http://www.tei-c.or...</td>\n",
       "      <td>(quarrée, GPE)</td>\n",
       "      <td>11.0</td>\n",
       "      <td>quarrée</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file abridged_xpath  \\\n",
       "0   CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "2   CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "6   CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "10  CMS1819-03-08-toRobertSedgwickIF.xml           None   \n",
       "\n",
       "                                    previous_encoding            entities  \\\n",
       "0   <ns0:opener xmlns:ns0=\"http://www.tei-c.org/ns...       (Albany, GPE)   \n",
       "2   <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...  (Northampton, GPE)   \n",
       "6   <ns0:p xmlns:ns0=\"http://www.tei-c.org/ns/1.0\"...       (Boston, GPE)   \n",
       "10  <ns0:postscript xmlns:ns0=\"http://www.tei-c.or...      (quarrée, GPE)   \n",
       "\n",
       "    descendant_order       entity label  \n",
       "0                1.0       Albany   GPE  \n",
       "2                3.0  Northampton   GPE  \n",
       "6                7.0       Boston   GPE  \n",
       "10              11.0      quarrée   GPE  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "#     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "    try:\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "        \n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            return \"Already Encoded\"\n",
    "        \n",
    "#         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "\n",
    "            return new_encoding\n",
    "\n",
    "        else:\n",
    "            return 'Error Making NER Suggestions'\n",
    "    \n",
    "#     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "#     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "#     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "        \n",
    "\n",
    "                    \n",
    "#             Join data\n",
    "df = df \\\n",
    "    .explode('entities') \\\n",
    "    .dropna(subset = ['entities'])\n",
    "\n",
    "df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "\n",
    "df['new_encoding'] = df \\\n",
    "    .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                            row['entity'],\n",
    "                                            row['label'],\n",
    "                                            subset_ner, 4, banned_list),\n",
    "           axis = 1)\n",
    "\n",
    "\n",
    "# Add additional columns for user input.\n",
    "df['uniq_id'] = ''\n",
    "\n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a1378",
   "metadata": {},
   "source": [
    "<a id=\"kwic\">keyword-in-context</a>\n",
    "\n",
    "## Keyword In Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dec5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Intersperse Entity with Likely TEI Information for Capacious Regex\n",
    "\"\"\"\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 - 0)\n",
    "    result[0::2] = lst\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Function: Build KWIC of Found Entities in Up Converted Encoding\n",
    "\"\"\"\n",
    "def get_kwic_encoding(entity, encoding, banned_list, kwic_range):\n",
    "#     Up convert arguments.\n",
    "    converted_encoding = up_convert_encoding(encoding)\n",
    "    converted_entity = up_convert_encoding(entity)\n",
    "\n",
    "#     Intersperse & 'up convert' by hand entity.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['(<.*?>)']) + ']*'\n",
    "\n",
    "    expanded_regex = r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    expanded_entity = re.sub('\\s', '</w> <w>', expanded_regex)\n",
    "    \n",
    "#     <w>(?:(?!<w>).)*\n",
    "#     'Tempered greedy token solution', <w> cannot appear after a <w>, unless within expanded_entity\n",
    "#     entity_regex = re.compile('(<w>(?:(?!<w>).)*' + expanded_entity + '.*?</w>)')\n",
    "    entity_regex = re.compile('([^\\s]*' + expanded_entity + '[^\\s]*)')\n",
    "    \n",
    "    \n",
    "    # Use regex match as final conv. entity.\n",
    "    try:\n",
    "        kwic_dict = {entity: []}\n",
    "        for m in entity_regex.finditer(converted_encoding):\n",
    "            \n",
    "            if any(item in m.group() for item in banned_list):\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "#                 Gather context:\n",
    "#                 Start of match (m.start()) minus kwic_range through end of match plus kwic_range.\n",
    "                context = converted_encoding[ m.start() - kwic_range : m.end() + kwic_range]\n",
    "                kwic_dict[entity].append(context)\n",
    "        \n",
    "        \n",
    "#         For each item in entity list, create new regex and expand until reaches preceeding </w> and trailing <w>.\n",
    "        for n, i in enumerate(kwic_dict[entity]):\n",
    "            complete_kwic = re.search(f'([^\\s]*{i}[^\\s]*)', converted_encoding).group()\n",
    "            kwic_dict[entity][n] = complete_kwic\n",
    "            \n",
    "#         Remove <w> and square brackets.\n",
    "        \n",
    "        \n",
    "#         Return values only\n",
    "        return kwic_dict[entity]\n",
    "            \n",
    "    except AttributeError:\n",
    "        return np.nan\n",
    "    \n",
    "# get_kwic_encoding(entity, encoding, banned_list, kwic_range):\n",
    "df['kwic'] = df \\\n",
    "    .apply(lambda row: get_kwic_encoding(row['entity'],\n",
    "                                         row['previous_encoding'],\n",
    "                                         banned_list, 30),\n",
    "           axis = 1)\n",
    "\n",
    "\n",
    "df.explode('kwic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03ac3d",
   "metadata": {},
   "source": [
    "<a id=\"highlighter\">highlighter</a>\n",
    "\n",
    "## Highlighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60037391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Reading Pane: Highlight Found Entity\n",
    "\"\"\"\n",
    "def highlighter(previous_encoding, entity):\n",
    "    highlighted_text = etree.fromstring(previous_encoding)\n",
    "    highlighted_text = etree.tostring(highlighted_text, method = 'text', encoding = 'UTF-8').decode('UTF-8')\n",
    "    \n",
    "    entity_match = re.search(f'(.*)({entity})(.*)', highlighted_text)\n",
    "    \n",
    "    highlighted_text = html.P([entity_match.group(1), html.Mark(entity_match.group(2)), entity_match.group(3)])\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "# Mimicking row selection from app., which handles one row at a time.\n",
    "reading_df = df.head(1)\n",
    "\n",
    "highlighted_text = highlighter(reading_df['previous_encoding'].squeeze(),\n",
    "                               reading_df['entity'].squeeze())\n",
    "\n",
    "highlighted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f4265",
   "metadata": {},
   "source": [
    "<a id=\"revisions\">revisions</a>\n",
    "\n",
    "## Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('<[/]?w>', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_without_uniq_id(label_dict, uniq_id, \n",
    "                           label, entity, previous_encoding, new_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     Up convert PREVIOUS ENCODING: assumes encoder will supply new encoding and attribute with value.\n",
    "    converted_encoding = up_convert_encoding(previous_encoding)\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "\n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id == '':\n",
    "\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "        revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    descendant_order\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def inherit_changes(label_dict, dataframe):\n",
    "    \n",
    "    dataframe = dataframe.fillna('')\n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "#         If HAND changes are accepted...\n",
    "        if row['uniq_id'] != '':\n",
    "        \n",
    "            revised_by_hand = revise_with_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                  row['label'], row['entity'], \n",
    "                                                  row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "            \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "                    \n",
    "                else:\n",
    "                    dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "                    \n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "        \n",
    "#         If NER suggestions are accepted as-is...\n",
    "        elif row['label'] != '' and row['uniq_id'] == '':\n",
    "        \n",
    "            revised_no_uniq_id = revise_without_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                        row['label'], row['entity'],\n",
    "                                                        row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_no_uniq_id\n",
    "        \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "                \n",
    "                else:\n",
    "                    dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "                \n",
    "#         If changes are rejected...\n",
    "        else:\n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "\n",
    "#     Subset dataframe with finalized revisions.\n",
    "    dataframe = dataframe.groupby(['abridged_xpath', 'descendant_order']).tail(1)\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "\n",
    "# using same data from highlighter \n",
    "for index, row in reading_df.iterrows():\n",
    "    \n",
    "    revised_no_uniq_id = revise_without_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                            row['label'], row['entity'],\n",
    "                                                            row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "# reading_df.loc[0, 'new_encoding'] = revised_no_uniq_id\n",
    "reading_df\n",
    "\n",
    "# # Inheritance update: if change occurred, change next row's \"previous_encoding\" as long as elem. is the same.\n",
    "# dataframe.loc[index, 'new_encoding'] = revised_no_uniq_id\n",
    "\n",
    "# try:\n",
    "#     if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "#     and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "#         dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "\n",
    "#     else:\n",
    "#         dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "\n",
    "# except KeyError as e:\n",
    "#     dataframe.loc[index, 'new_encoding'] = row['new_encoding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec12ca4",
   "metadata": {},
   "source": [
    "<a id=\"update_header\">update_header</a>\n",
    "\n",
    "## Update Header\n",
    "\n",
    "These functions called in <a id=\"writing\">writing</a> section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b78ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML: Write <change> to <revisionDesc>\n",
    "Expects:\n",
    "    XML File (xml_contents in revise_xml())\n",
    "    \n",
    "Output:\n",
    "    Writes changes directly to xml structure (root)\n",
    "\"\"\"\n",
    "def append_change_to_revisionDesc(root, ns):\n",
    "#     Create a change element for revisionDesc.\n",
    "#     If revisionDesc already exists...\n",
    "    if root.find('.//ns:teiHeader/ns:revisionDesc', ns):\n",
    "        revision_desc = root.find('.//ns:teiHeader/ns:revisionDesc', ns)\n",
    "\n",
    "        new_change = etree.SubElement(revision_desc, 'change',\n",
    "                                      when = str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "                                      who = '#nerHelper')\n",
    "                                      \n",
    "        new_change.text = f\"Entities added by NER (spaCy: {spacy.__version__}) application.\"\n",
    "#     Else, create revisionDesc with SubElement, then change.\n",
    "    else:\n",
    "        teiHeader = root.find('.//ns:teiHeader', ns)\n",
    "        revision_desc = etree.SubElement(teiHeader, 'revisionDesc')\n",
    "        new_change = etree.SubElement(revision_desc, 'change',\n",
    "                                      when = str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "                                      who = '#nerHelper')\n",
    "        new_change.text = f\"Entities added by NER (spaCy: {spacy.__version__}) application.\"\n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write <application> to <appInfo>\n",
    "Expects:\n",
    "    XML File (xml_contents in revise_xml())\n",
    "    \n",
    "Output:\n",
    "    Writes changes directly to xml structure (root)\n",
    "\"\"\"\n",
    "def append_app_to_appInfo(root, ns):\n",
    "#     If <appInfo> already exists...\n",
    "    if root.find('.//ns:teiHeader//ns:appInfo', ns):\n",
    "        app_info = root.find('.//ns:teiHeader//ns:appInfo', ns)\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                        ident = 'nerHelper',\n",
    "                                        version = \"0.1\")\n",
    "\n",
    "        # Without saving a variable.\n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n",
    "        \n",
    "#     If <appInfo> missing BUT <encodingDesc> exists...\n",
    "    elif root.find('.//ns:teiHeader/ns:encodingDesc', ns):\n",
    "        encoding_desc = root.find('.//ns:teiHeader/ns:encodingDesc', ns)\n",
    "        \n",
    "        app_info = etree.SubElement(encoding_desc, 'appInfo')\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                ident = 'nerHelper',\n",
    "                                version = \"0.1\")\n",
    "        \n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n",
    "        \n",
    "#     Else <appInfo> and <encodingDesc> missing...\n",
    "    else:\n",
    "        teiHeader = root.find('.//ns:teiHeader', ns)\n",
    "        \n",
    "        encoding_desc = etree.SubElement(teiHeader, 'encodingDesc')\n",
    "        \n",
    "        app_info = etree.SubElement(encoding_desc, 'appInfo')\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                ident = 'nerHelper',\n",
    "                                version = \"0.1\")\n",
    "        \n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90abf574",
   "metadata": {},
   "source": [
    "<a id=\"writing\">writing</a>\n",
    "\n",
    "## Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "new_df = inherit_changes(label_dict, reading_df)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Write New XML File with Accepted Revisions\n",
    "Expects:\n",
    "    XML File with Original Encoding\n",
    "    CSV File with Accepted Changes\n",
    "    Label Dictionary\n",
    "\"\"\"\n",
    "# def revise_xml(xml_contents, csv_df):\n",
    "# #     Label dictionary.\n",
    "#     label_dict = {'PERSON':'persRef',\n",
    "#                   'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "#                   'GPE':'placeName', # Countries, cities, states.\n",
    "#                   'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "#                   'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "#                   'NORP':'name', # Nationalities or religious or political groups.\n",
    "#                   'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "#                   'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "#                   'LAW':'name', # Named documents made into laws.\n",
    "#                   'DATE':'date' # Absolute or relative dates or periods.\n",
    "#                  }\n",
    "    \n",
    "# #     First, update data to reflect accepted changes.\n",
    "#     new_data = inherit_changes(label_dict, csv_df)\n",
    "    \n",
    "#     xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "#     xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "#     xml_file = xml_decoded.encode('utf-8')\n",
    "    \n",
    "# root = ET.fromstring(xml_string)\n",
    "# root = etree.fromstring(xml_string)\n",
    "#     ns = get_namespace(root)\n",
    "    \n",
    "#     Add <change> to <revisionDesc> and add <application> to <appInfo>\n",
    "\n",
    "new_data = reading_df\n",
    "\n",
    "append_change_to_revisionDesc(root, ns)\n",
    "append_app_to_appInfo(root, ns) # Does not need to save as variable; changes written to root.\n",
    "\n",
    "\n",
    "#     Convert XML structure to string for regex processing.\n",
    "\n",
    "tree_as_string = ET.fromstring(xml_string)\n",
    "tree_as_string = ET.tostring(tree_as_string, method = 'xml').decode('utf-8')\n",
    "\n",
    "# tree_as_string = etree.tostring(root).decode('utf-8') #, pretty_print = True\n",
    "tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "\n",
    "#     Write accepted code into XML tree.\n",
    "for index, row in new_data.iterrows():\n",
    "    original_encoding_as_string = row['previous_encoding']\n",
    "\n",
    "    # Remove namespaces within tags to ensure regex matches accurately.\n",
    "    original_encoding_as_string = re.sub('^<(.*?)( xmlns.*?)>(.*)$',\n",
    "                                         '<\\\\1>\\\\3',\n",
    "                                         original_encoding_as_string)\n",
    "# #     Remove namespaces.\n",
    "#     original_encoding_as_string = re.sub('ns0:', '', original_encoding_as_string)\n",
    "\n",
    "    accepted_encoding_as_string = row['new_encoding']\n",
    "    accepted_encoding_as_string = re.sub('<(.*?)( xmlns.*?)>(.*)$',\n",
    "                                         '<\\\\1>\\\\3',\n",
    "                                         accepted_encoding_as_string) # Remove namespaces within tags.\n",
    "    \n",
    "# #     Remove namespaces.\n",
    "#     accepted_encoding_as_string = re.sub('ns0:', '', accepted_encoding_as_string)\n",
    "\n",
    "    tree_as_string = re.sub(original_encoding_as_string,\n",
    "                            accepted_encoding_as_string,\n",
    "                            tree_as_string)\n",
    "    \n",
    "#     Remove namespaces.\n",
    "    tree_as_string = re.sub('ns0:', '', tree_as_string)\n",
    "    \n",
    "\n",
    "#     Check well-formedness (will fail if not well-formed)\n",
    "doc = etree.fromstring(tree_as_string)\n",
    "et = etree.ElementTree(doc)\n",
    "\n",
    "#     Convert to string.\n",
    "et = etree.tostring(et, encoding='unicode', method='xml', pretty_print = True)\n",
    "#     return et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_str = new_df['previous_encoding'].values[0]\n",
    "\n",
    "# prev_str = re.sub('ns0:', '', prev_str)\n",
    "\n",
    "# prev_str = re.sub('^<(.*?)( xmlns.*?)>(.*)$',\n",
    "#                   '<\\\\1>\\\\3',\n",
    "#                   prev_str)\n",
    "\n",
    "# original_encoding_as_string\n",
    "\n",
    "\n",
    "\n",
    "#     Convert XML structure to string for regex processing.\n",
    "# tree_as_string = etree.tostring(root).decode('utf-8') #, pretty_print = True\n",
    "\n",
    "\n",
    "tree_as_string = ET.fromstring(xml_string)\n",
    "tree_as_string = ET.tostring(tree_as_string, method = 'xml').decode('utf-8')\n",
    "tree_as_string = re.sub('\\s+', ' ', tree_as_string)\n",
    "\n",
    "# new_xml_string\n",
    "\n",
    "# tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "\n",
    "# error = 'others&#8212;<persRef ref=\"barney-john2\">Barney</persRef>' # og has space between &#8212; and <persRef\n",
    "\n",
    "# error in tree_as_string\n",
    "# og in tree_as_string\n",
    "\n",
    "tree_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_encoding_as_string in tree_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_as_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29a72b",
   "metadata": {},
   "source": [
    "## App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, ner_values): # date, \n",
    "    ner_values = ner_values#.split(',')\n",
    "    \n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    # Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "            \n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                    df['abridged_xpath'] = abridged_xpath\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "            \n",
    "            df['new_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entity'],\n",
    "                                                        row['label'],\n",
    "                                                        subset_ner, 4, banned_list),\n",
    "                       axis = 1)\n",
    "\n",
    "            \n",
    "            # Add additional columns for user input.\n",
    "            df['uniq_id'] = ''\n",
    "            \n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "            df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return df\n",
    "#     return filename, date, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "app = JupyterDash(__name__) \n",
    "#                   external_scripts = external_scripts)\n",
    "\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "\n",
    "# Preset variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW','DATE']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "    \n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    \n",
    "    dcc.Checklist(\n",
    "        className = 'ner-checklist',\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        className = 'upload-data',\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=False # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    \n",
    "#     Store uploaded data.\n",
    "    dcc.Store(id = 'data-upload-store'),\n",
    "    \n",
    "#     Display pane for file information.\n",
    "    html.Div(className = 'file-information', id = 'file-information'),\n",
    "    \n",
    "#     Display pane for data as table.\n",
    "    dash_table.DataTable(id = 'data-table-container',\n",
    "                         row_selectable=\"single\",\n",
    "                         selected_rows = [0],\n",
    "                         editable = True,\n",
    "                         page_size=1,\n",
    "                        )\n",
    "])\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "######### Callbacks ################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Upload data & create table.\n",
    "@app.callback([Output('file-information', 'children'),\n",
    "               Output('data-upload-store', 'data')],\n",
    "              [Input('upload-data', 'contents'),\n",
    "               Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'),\n",
    "               State('upload-data', 'last_modified')])\n",
    "def upload_data(contents, ner_values, filename, date):\n",
    "    if contents is None:\n",
    "        raise PreventUpdate\n",
    "            \n",
    "    data = parse_contents(contents, filename, ner_values)\n",
    "    \n",
    "    file_information = html.Div([html.P(f'File name: {filename}'),\n",
    "                                 html.P(f'Last modified: {datetime.datetime.fromtimestamp(date)}')])\n",
    "    \n",
    "    return file_information, data.to_dict('rows')\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash\n",
    "#     app.run_server(debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01ddff",
   "metadata": {},
   "source": [
    "## Revise XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ba47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "  \n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_without_uniq_id(label_dict, uniq_id, \n",
    "                           label, entity, previous_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     Up convert PREVIOUS ENCODING: assumes encoder will supply new encoding and attribute with value.\n",
    "    converted_encoding = up_convert_encoding(previous_encoding)\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "    print ('found placeName', 'placeName' in converted_encoding)\n",
    "\n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id == '':\n",
    "\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "        revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    descendant_order\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def inherit_changes(label_dict, dataframe):\n",
    "    print ('starting inherit_changes()...')\n",
    "    \n",
    "    dataframe = dataframe.fillna('')\n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "# #         If HAND changes are accepted...\n",
    "#         if row['uniq_id'] != '':\n",
    "        \n",
    "#             revised_by_hand = revise_with_uniq_id(label_dict, row['uniq_id'],\n",
    "#                                                   row['label'], row['entity'], \n",
    "#                                                   row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "#             dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "            \n",
    "#             try:\n",
    "#                 if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "#                 and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "#                     dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "                    \n",
    "#                 else:\n",
    "#                     dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "                    \n",
    "                    \n",
    "#             except KeyError as e:\n",
    "#                 dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "        \n",
    "#         If NER suggestions are accepted as-is...\n",
    "#         elif row['label'] != '' and row['uniq_id'] == '':\n",
    "        if row['accept'] == 'y':\n",
    "        \n",
    "            revised_no_uniq_id = revise_without_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                        row['label'], row['entity'],\n",
    "                                                        row['previous_encoding'])\n",
    "\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_no_uniq_id\n",
    "        \n",
    "            try:\n",
    "#                 If the next row has the same xpath & descendant order (next row handles the same element),\n",
    "#                 update next row's \"previous_encoding\" to reflect changes.\n",
    "                if (row['abridged_xpath'] == dataframe.loc[index + 1, 'abridged_xpath']) \\\n",
    "                and (row['descendant_order'] == dataframe.loc[index + 1, 'descendant_order']):\n",
    "        \n",
    "                    dataframe.at[index + 1, 'previous_encoding'] = revised_no_uniq_id\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.at[index, 'new_encoding'] = revised_no_uniq_id\n",
    "                \n",
    "                \n",
    "#         If changes are rejected...\n",
    "        else:\n",
    "            try:\n",
    "#                 If next row handles same element, pass current row's \"previous_encoding\"\n",
    "#                 to carry forward accepted changes in previous rows.\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "        \n",
    "                    dataframe.at[index + 1, 'previous_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "                   \n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.at[index, 'new_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "\n",
    "#     Subset dataframe with finalized revisions.\n",
    "    dataframe = dataframe.groupby(['abridged_xpath', 'descendant_order']).tail(1)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "\n",
    "\n",
    "df = pd.read_excel(abs_dir + 'GitHub/dsg-mhs/Jupyter_Notebooks/Interfaces/NER_Application/before-JQADiaries-v36i-1828-12-p065.xlsx', \n",
    "                   index_col = 0)\n",
    "\n",
    "inherit_changes(label_dict, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e85a6",
   "metadata": {},
   "source": [
    "All information needed to revise an xml file is stored in a spreadsheet. \n",
    "\n",
    "The revision function(s) checks one row at a time, the \"current row.\" The \"current row\" provides the entity's label and text (i.e. placeName, Kentucky). \n",
    "\n",
    "Before the function starts making revisions, it checks the \"preceding row.\" If the \"preceding row\" handles the same element, has the same \"abridged_xpath\" and \"descendant_order,\" then the function grabs the \"preceding row's\" \"new_encoding\" field. The reason the function grabs the \"preceding row's\" \"new_encoding\" rather than the \"current row's\" is because the \"current row\" does not know about previous revisions. When new changes are committed, described later, then those changes go into the \"new_encoding\" field, which has to include all previous changes in order for revisions to accumulate (as opposed to being over-written with each new row).\n",
    "\n",
    "If the \"current row\" has different values in either \"abridged_xpath\" or \"descendant_order,\" then the \"current row\" is a different element thant he \"preceding row.\" This means that the current row is essentially starting from scratch and will add the first revision.\n",
    "\n",
    "After the first revision is made (using regular expressions), the revision is stored in the \"new_encoding\" field of the \"current row.\" If the \"following row\" handles the same element, then the \"following row\" must grab the \"current row's\" \"new_encoding.\" Otherwise, the \"following row\" will miss/overlook changes that have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff709237",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('<[/]?w>', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "def revise_without_uniq_id(entity, converted_encoding, label):\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "    entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "    revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                              f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                              converted_encoding)\n",
    "    \n",
    "    cleaned_revisions = xml_cleanup(revised_encoding)\n",
    "    \n",
    "    return cleaned_revisions\n",
    "\n",
    "def revise_with_uniq_id(entity, uniq_id, converted_encoding, label):\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "    entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "    entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "    revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                              f'<{label} ref=\"{uniq_id}\" type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                              converted_encoding)\n",
    "        \n",
    "    revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "    return revised_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Choose revision function based on presence of uniq_id\n",
    "\"\"\"\n",
    "def choose_revision(entity, uniq_id, converted_encoding, label):\n",
    "    if uniq_id != '':\n",
    "        polished_revisions = revise_with_uniq_id(entity, uniq_id, converted_encoding, label)\n",
    "    elif uniq_id == '':\n",
    "        polished_revisions = revise_without_uniq_id(entity, converted_encoding, label)\n",
    "    else:\n",
    "        print ('breaking at choose_revision()')\n",
    "        \n",
    "    return polished_revisions\n",
    "        \n",
    "\n",
    "dataframe = df#.query('descendant_order == 9')#.copy(deep=False)\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    label = label_dict[row['label']]\n",
    "    entity = row['entity']\n",
    "    uniq_id = row['uniq_id']\n",
    "    \n",
    "    if row['accept'] == 'y': # if changes are accepted...\n",
    "        \n",
    "#         If the current row is handling same element as the preceding row...\n",
    "        if (row['abridged_xpath'] == dataframe.loc[index - 1, 'abridged_xpath']) \\\n",
    "        and (row['descendant_order']== dataframe.loc[index - 1, 'descendant_order']):\n",
    "            print ('previous elem is the same')\n",
    "\n",
    "#         Up convert encoding.\n",
    "#         Up convert preceding row's \"new_encoding\" or, if not, up convert current row's \"previous encoding\"\n",
    "            try:\n",
    "                converted_encoding = up_convert_encoding(dataframe.loc[index - 1, 'new_encoding'])\n",
    "            except KeyError:\n",
    "                converted_encoding = up_convert_encoding(row['previous_encoding']) # this KeyError triggers if the\n",
    "            \n",
    "            polished_revisions = choose_revision(entity, uniq_id, converted_encoding, label)\n",
    "        \n",
    "            dataframe.loc[index, 'new_encoding'] = polished_revisions\n",
    "                \n",
    "#         If the current row is handling a different row...\n",
    "        else:\n",
    "            print ('previous elem is NOT the same')\n",
    "#             Up convert current row's \"previous encoding\"\n",
    "            converted_encoding = up_convert_encoding(row['previous_encoding'])\n",
    "    \n",
    "            polished_revisions = choose_revision(entity, uniq_id, converted_encoding, label)\n",
    "        \n",
    "            dataframe.loc[index, 'new_encoding'] = polished_revisions\n",
    "\n",
    "#         else:\n",
    "#             converted_encoding = up_convert_encoding(dataframe.loc[index, 'previous_encoding'])\n",
    "\n",
    "#             converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "#             entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "#             entity_match = re.search(entity_regex, converted_encoding)\n",
    "#             print (entity_match)\n",
    "#             revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "#                                       f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "#                                       converted_encoding)\n",
    "\n",
    "\n",
    "#             dataframe.loc[index, 'new_encoding'] = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print ('changes not accepted')\n",
    "        dataframe.loc[index, 'new_encoding']\n",
    "\n",
    "    \n",
    "# newest_df = new_df.groupby(['abridged_xpath', 'descendant_order']).tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in new_df.iterrows():\n",
    "    if '<placeName type=\"nerHelper-added\">Kentucky' in r['previous_encoding']:\n",
    "        print (i, 'entity added')\n",
    "    else:\n",
    "        print (i, 'entity not added or Kentucy lost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ee802",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.query('entity == \"Tiber\"')['new_encoding'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ebb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({'a': [1, 2], 'b': [0, 3], 'c': [5, 6]})\n",
    "\n",
    "for i, r in test.iterrows():\n",
    "    if r['b'] < r['a']:\n",
    "        test.loc[i + 1, 'b'] = r['a'] + 5\n",
    "        \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec72c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ab' == 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'abcdef'\n",
    "\n",
    "reg = 'cd'\n",
    "\n",
    "re.sub(reg, '12', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_enc = '<w><p_xmlns=\"http://www.tei-c.org/ns/1.0\">Senator</w> <w>from</w> <w>Kentucky&#8212;quite</w> <w>shocked</w> <w>at</w> <w>the</w> <w>virulence</w> <w>of</w> <w>Newspaper</w> <w>Slanders</w> <w>ag<hi_rend=\"superscript\">t.</hi></w> <w>the</w> <w>Administration&#8212;</w> <w><persRef_ref=\"allen-samuel\">Allen</persRef></w> <w>thinks</w> <w>I</w> <w>have</w> <w>suffered,</w> <w>for</w> <w>not</w> <w>turning</w> <w>my</w> <w>enemies</w> <w>out</w> <w>of</w> <w>office;</w> <w>particularly</w> <w>the</w> <w><persRef_ref=\"mclean-john2\">Post-master</w> <w>General</persRef>.</w> <w>Committee</w> <w>of</w> <w>both</w> <w>Houses</w> <w>of</w> <w>Congress,</w> <w>notified</w> <w>me</w> <w>that</w> <w>they</w> <w>had</w> <w>formed</w> <w>Quorums</w> <w>and</w> <w>were</w> <w>ready</w> <w>to</w> <w>receive</w> <w>any</w> <w>Communication</w> <w>from</w> <w>me.</w> <w>Answered</w> <w>that</w> <w>I</w> <w>should</w> <w>make</w> <w>one</w> <w>at</w> <w>12</w> <w>to-morrow&#8212;</w> <w><persRef_key=\"rush-richard\">M<hi_rend=\"superscript\">r</hi></w> <w>Rush</persRef></w> <w>read</w> <w>me</w> <w>the</w> <w>draft</w> <w>of</w> <w>his</w> <w>annual</w> <w>report</w> <w>on</w> <w>the</w> <w>finances&#8212;</w> <w>Very</w> <w>pleasing&#8212;corrected</w> <w>the</w> <w>message</w> <w>by</w> <w>the</w> <w>revised</w> <w>figures</w> <w>of</w> <w>his</w> <w>Report&#8212;</w> <w><persRef_ref=\"sanford-nathan\">Sanford</persRef></w> <w>says</w> <w><persRef_ref=\"vanburen-martin\">Van</w> <w>Burren</persRef></w> <w>is</w> <w>not</w> <w>coming.</w> <w>He</w> <w>is</w> <w>elected</w> <w>Governor</w> <w>by</w> <w>a</w> <w>minority,</w> <w>in</w> <w>New-York.</w> <w><persRef_ref=\"condict-lewis\">Condict</persRef></w> <w>spoke</w> <w>of</w> <w><persRef_ref=\"southard-samuel\">Southard&#8217;s</persRef></w> <w>coming</w> <w>as</w> <w>Senator</w> <w>from</w> <w>New-Jersey.</w> <w>Fears</w> <w>they</w> <w>will</w> <w>make</w> <w>him</w> <w>a</w> <w>Non-resident;</w> <w>as</w> <w>they</w> <w>did</w> <w><persRef_ref=\"bailey-john\">Bailey</persRef>.</w> <w>Asked</w> <w>if</w> <w>Southard</w> <w>could</w> <w>not</w> <w>withdraw</w> <w>and</w> <w>return</w> <w>to</w> <w>New-Jersey&#8212;</w> <w>I</w> <w>thought</w> <w>it</w> <w>unnecessary&#8212;</w> <w>Visit</w> <w>from</w> <w>Maryland</w> <w>members</w> <w>with</w> <w>others&#8212;</w> <w><persRef_ref=\"barney-john2\">Barney</persRef></w> <w>lately</w> <w>at</w> <w>Charleston</w> <w>S.C.&#8212;</w> <w><persRef_ref=\"brent-daniel\">M<hi_rend=\"superscript\">r</hi></w> <w>Brent</persRef></w> <w>took</w> <w><persRef_ref=\"tacon-francisco\">Tacon&#8217;s</persRef></w> <w>Letters</w> <w>complaining</w> <w>of</w> <w><persRef_ref=\"salmon-hilario\">Salmon&#8217;s</persRef></w> <w>quarrels</w> <w>with</w> <w><persRef_ref=\"kirk-william\">Kirk&#8217;s</persRef></w> <w>children</w> <w>and</w> <w>servant</w> <w><unclear>maids</unclear>&#8212;</w> <w><persRef_ref=\"debresson-charles\">Bresson</persRef></w> <w>and</w> <w>the</w> <w><persRef_ref=\"lannes-napoleon\">Duke</w> <w>of</w> <w>Montebello</persRef></w> <w>took</w> <w>leave&#8212;going</w> <w>to-morrow</w> <w>for</w> <w>Mexico&#8212;</w> <w>I</w> <w>rode</w> <w>before</w> <w>dinner</w> <w>on</w> <w>horseback</w> <w>across</w> <w>the</w> <w>Tiber</w> <w>round</w> <w>by</w> <w>the</w> <w>Navy-Yard</w> <w>and</w> <w>Eastern</w> <w>Branch</w> <w>upper</w> <w>bridge,</w> <w>returning</w> <w>by</w> <w>the</w> <w>Capitol&#8212;</w> <w>Evening</w> <w>visits</w> <w>from</w> <w>M<hi_rend=\"superscript\">r</hi></w> <w>Bailey</w> <w>and</w> <w><persRef_ref=\"little-peter\">Col<hi_rend=\"superscript\">l.</hi></w> <w>Little</persRef>&#8212;</w> <w>Read</w> <w>to</w> <w>Bailey,</w> <w>my</w> <w>Letters</w> <w>to</w> <w><persRef_ref=\"bacon-ezekiel\">Bacon</persRef></w> <w>of</w> <w>Dec<hi_rend=\"superscript\">r.</hi></w> <w>1808&#8212;</w> <w>Collated</w> <w>the</w> <w>two</w> <w>Copies</w> <w>of</w> <w>my</w> <w>Message</w> <w>for</w> <w>the</w> <w>two</w> <w>Houses</w> <w>of</w> <w>Congress&#8212;</w> <w>My</w> <w>Son</w> <w><persRef_ref=\"adams-john2\">John&#8217;s</persRef></w> <w><persRef_ref=\"hellen-mary\">wife</persRef></w> <w>was</w> <w>taken</w> <w>in</w> <w>labour</w> <w>this</w> <w>day&#8212;</w> <w><persRef_ref=\"worthington-charles\">D<hi_rend=\"superscript\">r</hi></w> <w>Worthington</persRef></w> <w>attends</w> <w>her&#8212;</w> <w><persRef_ref=\"u\">M<hi_rend=\"superscript\">rs</hi></w> <w>Nowland</persRef></w> <w>is</w> <w>her</w> <w>Nurse&#8212;</p></w> <w></w>'\n",
    "\n",
    "entity_match = re.search('(New-York)(.*?</w>)', converted_encoding)\n",
    "\n",
    "\n",
    "revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                          f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                          converted_encoding)\n",
    "        \n",
    "xml_cleanup(revised_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b831f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
