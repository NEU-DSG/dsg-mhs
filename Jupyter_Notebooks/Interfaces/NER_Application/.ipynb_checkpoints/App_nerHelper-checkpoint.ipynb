{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerHelper\n",
    "## Application for Reading & Updating XML with NER\n",
    "\n",
    "Once names authority is ready use following encoding to link to external documents:\n",
    "\n",
    "https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-listPrefixDef.html\n",
    "\n",
    "\n",
    "```xml\n",
    "<listPrefixDef>\n",
    "    <prefixDef ident='psc' matchPattern=\"([a-z]+[a-z0-9]*)\" replacementPattern=\"personography.xml#$1\">\n",
    "        <p>Private URIs using the <code>bibl</code> prefix can be\n",
    "         expanded to form URIs which retrieve the relevant\n",
    "         bibliographical reference from www.example.com.</p>\n",
    "    </prefixDef>\n",
    "</listPrefixDef>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/GitHub/dsg-mhs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(child):\n",
    "    if child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "        ancestor = child.getparent().tag\n",
    "        xml_id = child.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "\n",
    "        abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "        return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "#     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "    try:\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "        \n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            return \"Already Encoded\"\n",
    "        \n",
    "#         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "\n",
    "            return new_encoding\n",
    "\n",
    "        else:\n",
    "            return 'Error Making NER Suggestions'\n",
    "    \n",
    "#     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "#     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "#     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(child, df, ns, subset_ner, filename, descendant_order):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    entities, previous_encoding = get_contents(child, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "#         'abridged_xpath':abridged_xpath,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    # Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "            \n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                    df['abridged_xpath'] = abridged_xpath\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "            \n",
    "            df['new_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entity'],\n",
    "                                                        row['label'],\n",
    "                                                        subset_ner, 4, banned_list),\n",
    "                       axis = 1)\n",
    "\n",
    "            \n",
    "            # Add additional columns for user input.\n",
    "            df['uniq_id'] = ''\n",
    "            \n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "            df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return filename, date, df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('<[/]?w>', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reading Pane: Highlight Found Entity\n",
    "\"\"\"\n",
    "def highlighter(previous_encoding, entity):\n",
    "    highlighted_text = etree.fromstring(previous_encoding)\n",
    "    highlighted_text = etree.tostring(highlighted_text, method = 'text', encoding = 'utf-8').decode('utf-8')\n",
    "    \n",
    "    entity_match = re.search(f'(.*)({entity})(.*)', highlighted_text)\n",
    "    \n",
    "    highlighted_text = html.P([entity_match.group(1), html.Mark(entity_match.group(2)), entity_match.group(3)])\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_with_uniq_id(label_dict, uniq_id, \n",
    "                           label, entity, previous_encoding, new_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     Up convert PREVIOUS ENCODING: assumes encoder will supply new encoding and attribute with value.\n",
    "    converted_encoding = up_convert_encoding(previous_encoding)\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "\n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id != '':\n",
    "\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "        revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label} ref=\"{uniq_id}\" type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_without_uniq_id(label_dict, uniq_id, \n",
    "                           label, entity, previous_encoding, new_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     Up convert PREVIOUS ENCODING: assumes encoder will supply new encoding and attribute with value.\n",
    "    converted_encoding = up_convert_encoding(previous_encoding)\n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "\n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id == '':\n",
    "\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "\n",
    "        revised_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    descendant_order\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def inherit_changes(label_dict, dataframe):\n",
    "    \n",
    "    dataframe = dataframe.fillna('')\n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "#         If HAND changes are accepted...\n",
    "        if row['uniq_id'] != '':\n",
    "        \n",
    "            revised_by_hand = revise_with_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                  row['label'], row['entity'], \n",
    "                                                  row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "            \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "                    \n",
    "                else:\n",
    "                    dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "                    \n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "        \n",
    "#         If NER suggestions are accepted as-is...\n",
    "        elif row['label'] != '' and row['uniq_id'] == '':\n",
    "        \n",
    "            revised_no_uniq_id = revise_without_uniq_id(label_dict, row['uniq_id'],\n",
    "                                                        row['label'], row['entity'],\n",
    "                                                        row['previous_encoding'], row['new_encoding'])\n",
    "\n",
    "            dataframe.loc[index, 'new_encoding'] = revised_no_uniq_id\n",
    "        \n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = row['new_encoding']\n",
    "                \n",
    "                else:\n",
    "                    dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = row['new_encoding']\n",
    "                \n",
    "#         If changes are rejected...\n",
    "        else:\n",
    "            try:\n",
    "                if dataframe.loc[index + 1, 'abridged_xpath'] == row['abridged_xpath'] \\\n",
    "                and dataframe.loc[index + 1, 'descendant_order'] == row['descendant_order']:\n",
    "                    dataframe.loc[index + 1, 'previous_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "                    \n",
    "            except KeyError as e:\n",
    "                dataframe.loc[index, 'new_encoding'] = dataframe.loc[index, 'previous_encoding']\n",
    "\n",
    "#     Subset dataframe with finalized revisions.\n",
    "    dataframe = dataframe.groupby(['abridged_xpath', 'descendant_order']).tail(1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write <change> to <revisionDesc>\n",
    "Expects:\n",
    "    XML File (xml_contents in revise_xml())\n",
    "    \n",
    "Output:\n",
    "    Writes changes directly to xml structure (root)\n",
    "\"\"\"\n",
    "def append_change_to_revisionDesc(root, ns):\n",
    "#     Create a change element for revisionDesc.\n",
    "#     If revisionDesc already exists...\n",
    "    if root.find('.//ns:teiHeader/ns:revisionDesc', ns):\n",
    "        revision_desc = root.find('.//ns:teiHeader/ns:revisionDesc', ns)\n",
    "\n",
    "        new_change = etree.SubElement(revision_desc, 'change',\n",
    "                                      when = str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "                                      who = '#nerHelper')\n",
    "                                      \n",
    "        new_change.text = f\"Entities added by NER (spaCy: {spacy.__version__}) application.\"\n",
    "#     Else, create revisionDesc with SubElement, then change.\n",
    "    else:\n",
    "        teiHeader = root.find('.//ns:teiHeader', ns)\n",
    "        revision_desc = etree.SubElement(teiHeader, 'revisionDesc')\n",
    "        new_change = etree.SubElement(revision_desc, 'change',\n",
    "                                      when = str(datetime.datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "                                      who = '#nerHelper')\n",
    "        new_change.text = f\"Entities added by NER (spaCy: {spacy.__version__}) application.\"\n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write <application> to <appInfo>\n",
    "Expects:\n",
    "    XML File (xml_contents in revise_xml())\n",
    "    \n",
    "Output:\n",
    "    Writes changes directly to xml structure (root)\n",
    "\"\"\"\n",
    "def append_app_to_appInfo(root, ns):\n",
    "#     If <appInfo> already exists...\n",
    "    if root.find('.//ns:teiHeader//ns:appInfo', ns):\n",
    "        app_info = root.find('.//ns:teiHeader//ns:appInfo', ns)\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                        ident = 'nerHelper',\n",
    "                                        version = \"0.1\")\n",
    "\n",
    "        # Without saving a variable.\n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n",
    "        \n",
    "#     If <appInfo> missing BUT <encodingDesc> exists...\n",
    "    elif root.find('.//ns:teiHeader/ns:encodingDesc', ns):\n",
    "        encoding_desc = root.find('.//ns:teiHeader/ns:encodingDesc', ns)\n",
    "        \n",
    "        app_info = etree.SubElement(encoding_desc, 'appInfo')\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                ident = 'nerHelper',\n",
    "                                version = \"0.1\")\n",
    "        \n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n",
    "        \n",
    "#     Else <appInfo> and <encodingDesc> missing...\n",
    "    else:\n",
    "        teiHeader = root.find('.//ns:teiHeader', ns)\n",
    "        \n",
    "        encoding_desc = etree.SubElement(teiHeader, 'encodingDesc')\n",
    "        \n",
    "        app_info = etree.SubElement(encoding_desc, 'appInfo')\n",
    "\n",
    "        ner_app_info = etree.SubElement(app_info, 'application',\n",
    "                                ident = 'nerHelper',\n",
    "                                version = \"0.1\")\n",
    "        \n",
    "        etree.SubElement(ner_app_info, 'label').text = 'nerHelper App'\n",
    "        etree.SubElement(ner_app_info, 'p').text = f'Entities added with spaCy-{spacy.__version__}.'\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Write New XML File with Accepted Revisions\n",
    "Expects:\n",
    "    XML File with Original Encoding\n",
    "    CSV File with Accepted Changes\n",
    "    Label Dictionary\n",
    "\"\"\"\n",
    "def revise_xml(xml_contents, csv_df):\n",
    "#     Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "#     First, update data to reflect accepted changes.\n",
    "    new_data = inherit_changes(label_dict, csv_df)\n",
    "    \n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    xml_file = xml_decoded.encode('utf-8')\n",
    "    \n",
    "    root = etree.fromstring(xml_file)\n",
    "    ns = get_namespace(root)\n",
    "    \n",
    "#     Add <change> to <revisionDesc> and add <application> to <appInfo>\n",
    "    append_change_to_revisionDesc(root, ns)\n",
    "    append_app_to_appInfo(root, ns) # Does not need to save as variable; changes written to root.\n",
    "    \n",
    "    \n",
    "#     Convert XML structure to string for regex processing.\n",
    "    tree_as_string = etree.tostring(root, pretty_print = True).decode('utf-8')\n",
    "    tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "    \n",
    "#     Write accepted code into XML tree.\n",
    "    for index, row in new_data.iterrows():\n",
    "        original_encoding_as_string = row['previous_encoding']\n",
    "        \n",
    "        # Remove namespaces within tags to ensure regex matches accurately.\n",
    "        original_encoding_as_string = re.sub('^<(.*?)( xmlns.*?)>(.*)$',\n",
    "                                             '<\\\\1>\\\\3',\n",
    "                                             original_encoding_as_string)\n",
    "        \n",
    "        accepted_encoding_as_string = row['new_encoding']\n",
    "        accepted_encoding_as_string = re.sub('<(.*?)( xmlns.*?)>(.*)$',\n",
    "                                             '<\\\\1>\\\\3',\n",
    "                                             accepted_encoding_as_string) # Remove namespaces within tags.\n",
    "        \n",
    "        tree_as_string = re.sub(original_encoding_as_string,\n",
    "                                accepted_encoding_as_string,\n",
    "                                tree_as_string)\n",
    "\n",
    "        \n",
    "#     Check well-formedness (will fail if not well-formed)\n",
    "    doc = etree.fromstring(tree_as_string)\n",
    "    et = etree.ElementTree(doc)\n",
    "    \n",
    "#     Convert to string.\n",
    "    et = etree.tostring(et, encoding='unicode', method='xml', pretty_print = True)\n",
    "    return et\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write Schema Information before Root\n",
    "Input: \n",
    "    - Revised XML document (return variable from revise_xml())\n",
    "    - XML File with Original Encoding\n",
    "\"\"\"\n",
    "def write_schema_information(xml_contents, final_revisions):\n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    \n",
    "    xml_file = xml_decoded.encode('utf-8').decode('utf-8')\n",
    "    xml_file = re.sub('\\s+', ' ', xml_file)\n",
    "    \n",
    "    schema_match = re.search('(<?.*)(<TEI.*)', xml_file)\n",
    "    schema_match = schema_match.group(1)\n",
    "    \n",
    "    completed_document = schema_match + final_revisions\n",
    "\n",
    "    return completed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n",
      "CPU times: user 26.3 ms, sys: 11.2 ms, total: 37.5 ms\n",
      "Wall time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # External JavaScript files\n",
    "# external_scripts = [\n",
    "#     'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js',\n",
    "#     {'src':'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js'}\n",
    "# ]\n",
    "\n",
    "app = JupyterDash(__name__) \n",
    "#                   external_scripts = external_scripts)\n",
    "\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "\n",
    "# Preset variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW','DATE']\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef', 'date']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "    \n",
    "#     Title\n",
    "    html.Header(\n",
    "        className=\"app-header\",\n",
    "        children = [\n",
    "            html.Div('nerHelper Application', className = \"app-header--title\")\n",
    "        ]),\n",
    "\n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('NER Labels & Definitions'),\n",
    "    \n",
    "#     Add legend & checklist for ner_labels.\n",
    "    html.Table([\n",
    "        html.Thead([\n",
    "            html.Tr([\n",
    "                html.Th('Label'),\n",
    "                html.Th('Definition'),\n",
    "            ]),\n",
    "        ]),\n",
    "        html.Tbody([\n",
    "            html.Tr([\n",
    "                html.Td('PERSON'),\n",
    "                html.Td('A person\\'s name (proper noun)' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('LOC'),\n",
    "                html.Td('Non-GPE locations, mountain ranges, bodies of water.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('GPE'),\n",
    "                html.Td('Countries, cities, states.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('FAC'),\n",
    "                html.Td('Buildings, airports, highways, bridges, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('ORG'),\n",
    "                html.Td('Companies, agencies, institutions, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('NORP'),\n",
    "                html.Td('Nationalities or religious or political groups.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('EVENT'),\n",
    "                html.Td('Named hurricanes, battles, wars, sports events, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('WORK_OF_ART'),\n",
    "                html.Td('Titles of books, songs, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('LAW'),\n",
    "                html.Td('Named documents made into laws.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('DATE'),\n",
    "                html.Td('Absolute or relative dates or periods.' ),\n",
    "            ]),\n",
    "        ]),\n",
    "    ]),\n",
    "    \n",
    "    #     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    \n",
    "    dcc.Checklist(\n",
    "        className = 'ner-checklist',\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['PERSON', 'LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        className = 'upload-data',\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    \n",
    "#     Store uploaded data.\n",
    "    dcc.Store(id = 'data-upload-store'),\n",
    "    \n",
    "#     Display pane for file information.\n",
    "    html.Div(className = 'file-information', id = 'file-information'),\n",
    "    \n",
    "#     Display pane for data as table.\n",
    "    dash_table.DataTable(id = 'data-table-container',\n",
    "                         row_selectable=\"single\",\n",
    "                         selected_rows = [0],\n",
    "                         editable = True,\n",
    "                         page_size=1,\n",
    "                        ),\n",
    "    \n",
    "#     Display pane for reading data from selected row & revision options.\n",
    "    html.Div(className = 'reading-container', id = 'reading-container'),\n",
    "    html.Div(id = 'revision-radio-container'),\n",
    "    html.Div(id = 'revision-text-container'),\n",
    "    html.Div(id = 'revision-button-container'),\n",
    "\n",
    "    dcc.ConfirmDialog(\n",
    "        id='confirm',\n",
    "        message='You must include an ID when selecting PERSON.',\n",
    "    ),\n",
    "    \n",
    "#     Store revised data.\n",
    "    dcc.Store(id = 'revisions-store'),\n",
    "    \n",
    "    \n",
    "#     Div to hold button that will write and download XML file.\n",
    "    html.Div(id = 'write-button-container'),\n",
    "    \n",
    "    html.Div(id = 'download-button-container')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "######### Callbacks ################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Upload data & create table.\n",
    "@app.callback([Output('file-information', 'children'),\n",
    "               Output('data-upload-store', 'data')],\n",
    "              [Input('upload-data', 'contents'),\n",
    "               Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'),\n",
    "               State('upload-data', 'last_modified')])\n",
    "def upload_data(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "#     Parse uploaded contents.\n",
    "    children = [\n",
    "        parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "        zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "    ]\n",
    "    data = children[0][2]\n",
    "    \n",
    "#     Extract file information.\n",
    "    file_information = html.Div([html.P(f'File name: {children[0][0]}'),\n",
    "                                 html.P(f'Last Modified: {datetime.datetime.fromtimestamp(children[0][1])}')])    \n",
    "    \n",
    "    return file_information, data.to_dict('rows')\n",
    "\n",
    "\n",
    "\n",
    "# Generate table with data from store.\n",
    "@app.callback([Output('data-table-container', 'data'),\n",
    "               Output('data-table-container', 'columns')],\n",
    "              Input('data-upload-store', 'data'))\n",
    "def populate_data_table(data):\n",
    "\n",
    "    df = pd.DataFrame(data)[['file', 'entity', 'label']]\n",
    "    cols = [{'name':i, 'id': i} for i in df.columns]\n",
    "\n",
    "    return df.to_dict('rows'), cols\n",
    "\n",
    "\n",
    "\n",
    "# Create reading pane & revision options once row from table is selected.\n",
    "@app.callback([Output('reading-container', 'children'),\n",
    "               Output('revision-radio-container', 'children'),\n",
    "               Output('revision-text-container', 'children'),\n",
    "               Output('revision-button-container', 'children')],\n",
    "              [Input('data-upload-store', 'data'),\n",
    "               Input('data-table-container', 'selected_rows')])\n",
    "def create_reading_and_revisions_pane(data, selected_rows):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "        \n",
    "    reading_df = pd.DataFrame(data).iloc[selected_rows]\n",
    "\n",
    "#     Access previous and new encoding and squeeze() them to return only scalar (the text).\n",
    "#     Use highlighter() to re-construct previous_encoding with html.Mark() around found entity.\n",
    "    highlighted_text = highlighter(reading_df['previous_encoding'].squeeze(),\n",
    "                                   reading_df['entity'].squeeze())\n",
    "    \n",
    "    reading_pane = html.Div([\n",
    "        html.H2('Found Entity'),\n",
    "        html.Div(highlighted_text),\n",
    "        html.H2('Revisions Options'),\n",
    "        html.P(\"\"\"\n",
    "        Please confirm the correct label that describes the entity. \n",
    "        If you've selected 'PERSON,' you must also hand-type an reference identifier below. \n",
    "        The reference identifier should match an entity in the names authority database.\n",
    "        \"\"\"),\n",
    "    ])\n",
    "    \n",
    "#     Choose correct entity label with radio buttons.\n",
    "    revision_radio = dcc.RadioItems(\n",
    "        className = 'radio-input',\n",
    "        id = 'radioInput',\n",
    "        options = [{'label':'PERSON', 'value':'PERSON'},\n",
    "                   {'label':'LOC', 'value':'LOC'},\n",
    "                   {'label':'GPE', 'value':'GPE'},\n",
    "                   {'label':'FAC', 'value':'FAC'},\n",
    "                   {'label':'ORG', 'value':'ORG'},\n",
    "                   {'label':'NORP', 'value':'NORP'},\n",
    "                   {'label':'EVENT', 'value':'EVENT'},\n",
    "                   {'label':'WORK_OF_ART', 'value':'WORK_OF_ART'},\n",
    "                   {'label':'LAW', 'value':'LAW'},\n",
    "                   {'label':'DATE', 'value':'DATE'},\n",
    "                   {'label':'No Changes', 'value':''}\n",
    "        ]\n",
    "        ),\n",
    "    \n",
    "#     Create text area for manual changes.\n",
    "    revision_text = dcc.Input(className = 'text-input',\n",
    "                              id = 'textInput', type = 'text', \n",
    "                              placeholder = 'Type a Unique ID here.', value = '', debounce = True)\n",
    "        \n",
    "#     Create button for committing changes.\n",
    "    revision_button = html.Button('Confirm Changes?', id = 'confirm-button',\n",
    "                                  n_clicks = 0, className = 'revision-button'),\n",
    "    \n",
    "    return reading_pane, revision_radio, revision_text, revision_button\n",
    "\n",
    "\n",
    "# Once a revisions is accepted, write row with instructions to dataframe.\n",
    "@app.callback([Output('revisions-store', 'data'),\n",
    "               Output('confirm', 'displayed')],\n",
    "              [Input('revision-button-container', 'n_clicks'),\n",
    "               Input('data-upload-store', 'data'),\n",
    "               Input('data-table-container', 'selected_rows'),\n",
    "               Input('revision-radio-container', 'children'), \n",
    "               Input('revision-text-container', 'children')],\n",
    "              State('revisions-store', 'data'))\n",
    "def commit_revisions_to_dataframe(n_clicks, data, selected_rows,\n",
    "                                  radio_children, text_children, revisions):\n",
    "    \n",
    "#     Only run if the n_click 'id' is triggered by the revision-button-container.\n",
    "    changed_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "        \n",
    "    if changed_id != 'revision-button-container.n_clicks':\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    revised_row = pd.DataFrame(data).iloc[selected_rows]\n",
    "        \n",
    "#     Check if radio_ and text_children each have a value by seeing if a 'value' key is nested in 'props'.\n",
    "    if 'value' in radio_children[0]['props']:\n",
    "        radio_value = radio_children[0]['props']['value']\n",
    "    else:\n",
    "        radio_value = ''\n",
    "    \n",
    "    if 'value' in text_children['props']:\n",
    "        text_value = text_children['props']['value']\n",
    "    else:\n",
    "        text_value = ''\n",
    "    \n",
    "#     Change individual cell value according to selected row & user-input.\n",
    "    revised_row['uniq_id'] = text_value\n",
    "    revised_row['label'] = radio_value\n",
    "    \n",
    "#     Check for xml:id and send error msg if missing.\n",
    "    if radio_value == 'PERSON' and text_value == '':\n",
    "        error_msg = True\n",
    "        return None, error_msg\n",
    "    \n",
    "    else:\n",
    "        error_msg = False\n",
    "#         Create or update revisions dataframe to store revisions.\n",
    "        if revisions is None:\n",
    "            revisions = pd.DataFrame(revised_row)\n",
    "        else:\n",
    "            revisions = pd.DataFrame(revisions)\n",
    "            revisions = revisions.append(revised_row, ignore_index = True)\n",
    "    \n",
    "        return revisions.to_dict('rows'), error_msg\n",
    "\n",
    "    \n",
    "\n",
    "# After last revision (or whenever one change completed), provide button to commit changes to XML.\n",
    "@app.callback(Output('write-button-container', 'children'),\n",
    "              Input('revisions-store', 'data'))\n",
    "def provide_button_to_download_revisions(data):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    return html.Button('Finished? Download Revised XML.', \n",
    "                       id = 'write-xml-button', className = 'write-button')\n",
    "\n",
    "\n",
    "# Run functions to revise XML and download new document.\n",
    "@app.callback(Output('download-button-container', 'children'),\n",
    "              [Input('write-button-container', 'n_clicks'),\n",
    "               Input('upload-data', 'contents'),\n",
    "               Input('revisions-store', 'data')],\n",
    "              State('upload-data', 'filename'))\n",
    "def provide_download_link(n_clicks, contents, revisions, filename):\n",
    "    write_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "    \n",
    "    if write_id != 'write-button-container.n_clicks':\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    xml_contents = contents[0]\n",
    "    revisions = pd.DataFrame(revisions)\n",
    "    \n",
    "    final_revisions = revise_xml(xml_contents, revisions)\n",
    "    \n",
    "    completed_file = write_schema_information(xml_contents, final_revisions)\n",
    "    \n",
    "    path = f\"revised-{filename[0]}\"\n",
    "    with open(path, \"w\") as file:\n",
    "        file.write(completed_file)\n",
    "\n",
    "    return html.P(f'{filename[0]} downloaded here {path}! Please review the XML document for well-formedness.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash\n",
    "    app.run_server(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
