{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application for Reading & Updating XML with NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/GitHub/dsg-mhs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Retrieve XPaths\n",
    "\"\"\"\n",
    "def get_abridged_xpath(child):\n",
    "    if child.getparent().get('{http://www.w3.org/XML/1998/namespace}id') is not None:    \n",
    "        ancestor = child.getparent().tag\n",
    "        xml_id = child.getparent().get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "\n",
    "        abridged_xpath = f'.//ns:body//{ancestor}[@xml:id=\"{xml_id}\"]/{child.tag}'\n",
    "        return abridged_xpath\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Write New Encoding with Up-Conversion\n",
    "\"\"\"\n",
    "def make_ner_suggestions(previous_encoding, entity, label, subset_ner, kwic_range, banned_list):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', previous_encoding, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, previous_encoding):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-convert entity (label remains unchanged).\n",
    "    label = subset_ner[label]    \n",
    "    converted_entity = ' '.join(['<w>' + e + '</w>' for e in entity.split(' ')])\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "        \n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "#     Find converted entities and kwic-converted entities, even if there's additional encoding within entity.\n",
    "    try:\n",
    "        entity_regex = re.sub('<w>(.*)</w>', '(\\\\1)(.*?</w>)', converted_entity)\n",
    "        entity_match = re.search(entity_regex, converted_encoding)\n",
    "        \n",
    "        ban_decision = []\n",
    "        for i in banned_list:\n",
    "            if i in entity_match.group(0):\n",
    "                ban_decision.append('y')\n",
    "                \n",
    "        if 'y' in ban_decision:\n",
    "            return \"Already Encoded\"\n",
    "        \n",
    "#         If expanded regex is in previous encoding, find & replace it with new encoding.\n",
    "        elif entity_match:\n",
    "            new_encoding = re.sub(f'{entity_match.group(0)}',\n",
    "                                  f'<{label}>{entity_match.group(1)}</{label}>{entity_match.group(2)}',\n",
    "                                  converted_encoding)\n",
    "            \n",
    "#             Remove <w> tags to return to well-formed xml.\n",
    "            new_encoding = re.sub('<[/]?w>', '', new_encoding)\n",
    "#             Remove underscores.\n",
    "            new_encoding = re.sub('_', ' ', new_encoding)\n",
    "\n",
    "            return new_encoding\n",
    "\n",
    "        else:\n",
    "            return 'Error Making NER Suggestions'\n",
    "    \n",
    "#     Up-conversion works well because it 'breaks' if an entity already has been encoded:\n",
    "#     <w>Abel</w> (found entity) does not match <w><persRef_ref=\"abel-mary\">Mrs</w> <w>Abel</persRef></w>\n",
    "#     <persRef> breaks function and avoids duplicating entities.\n",
    "    \n",
    "    except:\n",
    "        return 'Error Occurred with Regex.'\n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(child, df, ns, subset_ner, filename, descendant_order):\n",
    "    abridged_xpath = get_abridged_xpath(child)\n",
    "    entities, previous_encoding = get_contents(child, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "#         'abridged_xpath':abridged_xpath,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    label_dict = {'PERSON':'persName',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'abridged_xpath', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body//ns:div[@type=\"docbody\"]', ns):\n",
    "            \n",
    "                abridged_xpath = get_abridged_xpath(child)\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                    df['abridged_xpath'] = abridged_xpath\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "            \n",
    "            df['new_encoding'] = df \\\n",
    "                .apply(lambda row: make_ner_suggestions(row['previous_encoding'],\n",
    "                                                        row['entity'],\n",
    "                                                        row['label'],\n",
    "                                                        subset_ner, 4, banned_list),\n",
    "                       axis = 1)\n",
    "\n",
    "            \n",
    "            # Add additional columns for user input.\n",
    "            df['accept_changes'] = ''\n",
    "            df['make_hand_edits'] = ''\n",
    "            \n",
    "#             Drop rows if 'new_encoding' value equals 'Already Encoded'.\n",
    "            df = df[df['new_encoding'] != 'Already Encoded']\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "\n",
    "\n",
    "#     Return HTML with outputs.\n",
    "    return filename, date, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd0f2fad610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 3.49 ms, total: 26.2 ms\n",
      "Wall time: 334 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "# Preset Variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW']\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "#     Title\n",
    "    html.H1('NER + XML Reader'),\n",
    "\n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    dcc.Checklist(\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['PERSON', 'LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    \n",
    "#     Hidden div for storing data.\n",
    "#     https://stackoverflow.com/questions/58990502/dashpython-cant-display-dataframe-in-datatable-after-calculations\n",
    "    html.Div(id = 'data-upload-store', style = {'display':'none'}),\n",
    "    \n",
    "#     File Info Area.\n",
    "    html.Div(id = 'file-information'),\n",
    "    \n",
    "#     Datatable Area.\n",
    "    html.Div(id = 'datatable-container'),\n",
    "    \n",
    "#     Revisions Area.\n",
    "    html.Div(id = 'revisions-container')\n",
    "])\n",
    "\n",
    "# Callbacks.\n",
    "# Upload data.\n",
    "@app.callback([Output('file-information', 'children'), Output('data-upload-store', 'data')],\n",
    "              [Input('upload-data', 'contents'), Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'), State('upload-data', 'last_modified')])\n",
    "\n",
    "def upload_data(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents is None:\n",
    "        raise PreventUpdate\n",
    "        \n",
    "    children = [\n",
    "        parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "        zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "    ]\n",
    "\n",
    "    filename = children[0][0]\n",
    "    date = children[0][1]\n",
    "\n",
    "    file_information = html.Div([html.P(f'File name: {filename}'),\n",
    "                                 html.P(f'Last Modified: {datetime.datetime.fromtimestamp(date)}')])\n",
    "\n",
    "    data = children[0][2].to_dict('records')\n",
    "\n",
    "    return file_information, data\n",
    "    \n",
    "\n",
    "# Update app to display uploaded data information.\n",
    "@app.callback(Output('datatable-container', 'children'),\n",
    "              [Input('data-upload-store', 'data')])\n",
    "\n",
    "def update_file_info(data):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    cols = [{\"name\": i, \"id\": i} for i in df.columns]\n",
    "    \n",
    "    child = html.Div([\n",
    "        dash_table.DataTable(\n",
    "                id = 'datatable',\n",
    "                data = df.to_dict('records'),\n",
    "                columns = cols,\n",
    "                row_selectable=\"single\",\n",
    "                selected_rows = [],\n",
    "                editable = True,\n",
    "                page_size=1,\n",
    "                export_format = 'csv', # Eventually, I'll remove this and put export with finalized datatable.\n",
    "\n",
    "                style_data_conditional=[\n",
    "                    {\n",
    "                        'if': {'row_index': 'odd'},\n",
    "                        'backgroundColor': 'rgb(248, 248, 248)'\n",
    "                    }\n",
    "                ],\n",
    "                style_header={\n",
    "                    'backgroundColor': 'rgb(230, 230, 230)',\n",
    "                    'fontWeight': 'bold'\n",
    "                }\n",
    "            ),\n",
    "    ])\n",
    "    return child\n",
    "\n",
    "# Work with one row of data table.\n",
    "@app.callback(Output('revisions-container', 'children'),\n",
    "              [Input('data-upload-store', 'data'),  Input('datatable', 'selected_rows')])\n",
    "def update_row_with_revisions(data, derived_virtual_selected_rows):\n",
    "    if derived_virtual_selected_rows is None:\n",
    "        raise PreventUpdate\n",
    "\n",
    "    dff = pd.DataFrame(data).iloc[derived_virtual_selected_rows]\n",
    "    prev_encoding = dff['previous_encoding']\n",
    "    new_encoding = dff['new_encoding']\n",
    "    entity = dff['entity']\n",
    "    label = dff['label']\n",
    "#     accept_changes = dff['accept_changes']\n",
    "#     make_hand_edits = dff['make_hand_edits']\n",
    "\n",
    "    return html.Div([\n",
    "        html.Table(\n",
    "            html.Tr(\n",
    "#                 html.Th('Previous Encoding'),\n",
    "#                 html.Th('New Encoding')\n",
    "#                 html.Td(prev_encoding),\n",
    "                html.Td(new_encoding)\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# filename = abs_dir + \"Data/TestEncoding/EditingData/test_xml-before.xml\"\n",
    "filename = abs_dir + \"TestEncoding/EditingData/JQADiaries-v33-1821-12-p001_copy.xml\"\n",
    "xml_file = open(filename).read()\n",
    "root = etree.fromstring(xml_file.encode('utf-8'))\n",
    "ns = get_namespace(root)\n",
    "\n",
    "subset_ner = {'PERSON':'persName', 'LOC':'placeName'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct data table as is (using parse_contents until p_c() returns HTML).\n",
    "# Build data table in top region of screen.\n",
    "#     Primary purpose: show progress and provide navigatability\n",
    "#         Visualize Progess/Steps: highlight row when viewed (color for previous rows, current row, white for unseen)\n",
    "\n",
    "\n",
    "def upload_data(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents:\n",
    "        children = [\n",
    "            parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "        ]\n",
    "    \n",
    "        filename = children[0][0]\n",
    "        date = children[0][1]\n",
    "        df = children[0][2]\n",
    "        \n",
    "        return html.Div([\n",
    "        \n",
    "#             Print file info.\n",
    "            html.Div([\n",
    "                html.H4('File Information'),\n",
    "                html.P(f'{filename}, {datetime.datetime.fromtimestamp(date)}'),\n",
    "            ]),\n",
    "\n",
    "            html.Br(),\n",
    "\n",
    "#             Return data table of element and attribute info.\n",
    "            dash_table.DataTable(\n",
    "                id = 'datatable',\n",
    "                data = df.to_dict('records'),\n",
    "                columns = [{'name':i, 'id':i} for i in df.columns],\n",
    "                row_selectable=\"single\",\n",
    "                editable = True,\n",
    "                page_size=1,\n",
    "                export_format = 'csv',\n",
    "\n",
    "                style_cell_conditional=[\n",
    "                    {\n",
    "                        'if': {'column_id': c},\n",
    "                        'textAlign': 'left'\n",
    "                    } for c in ['Date', 'Region']\n",
    "                ],\n",
    "                style_data_conditional=[\n",
    "                    {\n",
    "                        'if': {'row_index': 'odd'},\n",
    "                        'backgroundColor': 'rgb(248, 248, 248)'\n",
    "                    }\n",
    "                ],\n",
    "                style_header={\n",
    "                    'backgroundColor': 'rgb(230, 230, 230)',\n",
    "                    'fontWeight': 'bold'\n",
    "                }\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "\n",
    "    \n",
    "# # Necessary Functionality.\n",
    "# # The interface \"visualizes\" the changes; the app actualizes these changes by writing them to data table.\n",
    "\n",
    "# # Pretty print old encoding in left-central region.\n",
    "# # Pretty print new encoding in central-central region.\n",
    "# #     \"new\" encoding is automated changes at first; then updated to user-changes\n",
    "# @app.callback(\n",
    "#     Output('datatable-interactivity-container', \"children\"),\n",
    "#     [Input('output-data-upload', 'derived_virtual_selected_rows')])\n",
    "# #     [Input('datatable', \"derived_virtual_selected_rows\")])\n",
    "# def update_texts(derived_virtual_selected_rows):\n",
    "    \n",
    "#     if derived_virtual_selected_rows is None:\n",
    "#         derived_virtual_selected_rows = []\n",
    "    \n",
    "#     dff = df if rows is None else pd.DataFrame(rows)\n",
    "    \n",
    "#     return [\n",
    "#         html.H2('Encoding Revisions'),\n",
    "# #         dash_table.DataTable(\n",
    "# #             data = dff.to_dict('records')\n",
    "# #         )\n",
    "#     ]\n",
    "\n",
    "# # Provide accept/reject radio buttons in top-right region.\n",
    "# # Provide text box for manual changes in center-right region.\n",
    "# # Provide \"commit\" button in lower-right region."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
