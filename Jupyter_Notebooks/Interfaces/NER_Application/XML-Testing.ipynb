{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import xml.etree.ElementTree as ET\n",
    "from lxml import etree, isoschematron\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/GitHub/dsg-mhs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making revisions with and without IDs + Highlighting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_dict = {'PERSON':'persName',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Intersperse Entity with Likely TEI Information for Capacious Regex\n",
    "\"\"\"\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 - 0)\n",
    "    result[0::2] = lst\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Function: Build KWIC of Found Entities in Up Converted Encoding\n",
    "\"\"\"\n",
    "def get_kwic_encoding(entity, encoding, banned_list, kwic_range):\n",
    "#     Up convert arguments.\n",
    "    converted_encoding = up_convert_encoding(encoding)\n",
    "    converted_entity = up_convert_encoding(entity)\n",
    "\n",
    "#     Intersperse & 'up convert' by hand entity.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['(<.*?>)']) + ']*'\n",
    "\n",
    "    expanded_regex = r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    expanded_entity = re.sub('\\s', '</w> <w>', expanded_regex)\n",
    "    \n",
    "#     <w>(?:(?!<w>).)*\n",
    "#     'Tempered greedy token solution', <w> cannot appear after a <w>, unless within expanded_entity\n",
    "#     entity_regex = re.compile('(<w>(?:(?!<w>).)*' + expanded_entity + '.*?</w>)')\n",
    "    entity_regex = re.compile('([^\\s]*' + expanded_entity + '[^\\s]*)')\n",
    "    \n",
    "    \n",
    "    # Use regex match as final conv. entity.\n",
    "    try:\n",
    "        kwic_dict = {entity: []}\n",
    "        for m in entity_regex.finditer(converted_encoding):\n",
    "            \n",
    "            if any(item in m.group() for item in banned_list):\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "#                 Gather context:\n",
    "#                 Start of match (m.start()) minus kwic_range through end of match plus kwic_range.\n",
    "                context = converted_encoding[ m.start() - kwic_range : m.end() + kwic_range]\n",
    "                kwic_dict[entity].append(context)\n",
    "        \n",
    "        \n",
    "#         For each item in entity list, create new regex and expand until reaches preceeding </w> and trailing <w>.\n",
    "        for n, i in enumerate(kwic_dict[entity]):\n",
    "            complete_kwic = re.search(f'([^\\s]*{i}[^\\s]*)', converted_encoding).group()\n",
    "            kwic_dict[entity][n] = complete_kwic\n",
    "        \n",
    "#         Return values only\n",
    "        return kwic_dict[entity]\n",
    "            \n",
    "    except AttributeError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(descendant, df, ns, subset_ner, filename, descendant_order):\n",
    "    entities, previous_encoding = get_contents(descendant, './/ns:.', ns, subset_ner)\n",
    "\n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    # Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body', ns): # Change this line to specify where to look for entities.\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "                \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "        \n",
    "            # Add additional columns for user input.\n",
    "            df['uniq_id'] = ''\n",
    "            \n",
    "#             Replace 'previous_encoding' with a KWIC version containing entity.\n",
    "            df['previous_encoding'] = df.apply(lambda row: get_kwic_encoding(row['entity'],\n",
    "                                                                             row['previous_encoding'], \n",
    "                                                                             banned_list,\n",
    "                                                                             30),\n",
    "                                               axis = 1)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "    \n",
    "#     Explode lists within more than one item.\n",
    "    df = df.explode('previous_encoding').dropna().drop_duplicates()\n",
    "    \n",
    "#     Return HTML with outputs.\n",
    "    return filename, date, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('(<[/]?w>)', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reading Pane: Highlight Found Entity\n",
    "\"\"\"\n",
    "def highlighter(previous_encoding, entity):\n",
    "#     Remove all tags.\n",
    "    highlighted_text = re.sub('(<.*?>)', '', previous_encoding) \n",
    "    \n",
    "    entity_match = re.search(f'(.*)({entity})(.*)', highlighted_text)\n",
    "    \n",
    "    highlighted_text = html.P([entity_match.group(1), html.Mark(entity_match.group(2)), entity_match.group(3)])\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_with_selections(label_dict, label, uniq_id, entity, previous_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     <w>(?:(?!<w>).)*\n",
    "#     'Tempered greedy token solution', <w> cannot appear after a <w>, unless it's within expanded_entity.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['(<.*?>)']) + ']*'\n",
    "    expanded_regex = r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    expanded_entity = re.sub('\\s', '</w> <w>', expanded_regex)\n",
    "    \n",
    "#     [^\\s]*(</w>)?\n",
    "#     Match anything except for whitespace until first </w> appears.\n",
    "#     expanded_entity = f'(<w>(?:(?!<w>).)*{expanded_entity}[^\\s]*(</w>)?)'\n",
    "    expanded_entity = f'([^\\s]*{expanded_entity}[^\\s]*)'\n",
    "    \n",
    "    matched_entity = re.search(expanded_entity, previous_encoding).group()\n",
    "    \n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id != '':\n",
    "        \n",
    "        revised_encoding = re.sub(f'{matched_entity}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{matched_entity}</{label}>',\n",
    "                                  previous_encoding)        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "\n",
    "        return revised_encoding\n",
    "    \n",
    "    elif uniq_id == '':\n",
    "        \n",
    "        revised_encoding = re.sub(f'{matched_entity}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{matched_entity}</{label}>',\n",
    "                                  previous_encoding)\n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    descendant_order\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def commit_revisions(label_dict, dataframe):\n",
    "    \n",
    "    dataframe = dataframe.fillna('').reset_index()\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        revised_by_hand = revise_with_selections(label_dict, row['label'], row['uniq_id'],\n",
    "                                                 row['entity'], row['previous_encoding'])\n",
    "        \n",
    "        dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "    \n",
    "#     Clean up previous_encoding (remove word tags)\n",
    "    dataframe['previous_encoding'] = dataframe.apply(lambda row: xml_cleanup(row['previous_encoding']), axis = 1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Write New XML File with Accepted Revisions\n",
    "Expects:\n",
    "    XML File with Original Encoding\n",
    "    CSV File with Accepted Changes\n",
    "    Label Dictionary\n",
    "\"\"\"\n",
    "def revise_xml(xml_contents, csv_df):\n",
    "#     Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    new_data = commit_revisions(label_dict, csv_df)\n",
    "    \n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    xml_file = xml_decoded.encode('utf-8')\n",
    "    \n",
    "    root = etree.fromstring(xml_file)\n",
    "    ns = get_namespace(root)    \n",
    "    \n",
    "#     Convert XML structure to string for regex processing.\n",
    "    tree_as_string = etree.tostring(root, pretty_print = True).decode('utf-8')\n",
    "    tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "    \n",
    "#     Write accepted code into XML tree.\n",
    "    for index, row in new_data.iterrows():\n",
    "        tree_as_string = re.sub(f'(.*)({row.previous_encoding})(.*)',\n",
    "                                f'\\\\1{row.new_encoding}\\\\3',\n",
    "                                tree_as_string)\n",
    "        \n",
    "#     Check well-formedness (will fail if not well-formed)\n",
    "    doc = etree.fromstring(tree_as_string)\n",
    "    et = etree.ElementTree(doc)\n",
    "    \n",
    "#     Convert to string.\n",
    "    et = etree.tostring(et, encoding='unicode', method='xml', pretty_print = True)\n",
    "    return et\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write Schema Information before Root\n",
    "Input: \n",
    "    - Revised XML document (return variable from revise_xml())\n",
    "    - XML File with Original Encoding\n",
    "\"\"\"\n",
    "def write_schema_information(xml_contents, final_revisions):\n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    \n",
    "    xml_file = xml_decoded.encode('utf-8').decode('utf-8')\n",
    "    xml_file = re.sub('\\s+', ' ', xml_file)\n",
    "    \n",
    "    schema_match = re.search('(<?.*)(<TEI.*)', xml_file)\n",
    "    schema_match = schema_match.group(1)\n",
    "    \n",
    "    completed_document = schema_match + final_revisions\n",
    "\n",
    "    return completed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "previous_encoding = \"\"\"<div xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:mhs=\"http://www.masshist.org/ns/1.0\" xmlns:tei=\"http://www.tei-c.org/ns/1.0\" type=\"entry\" xml:id=\"jqadiaries-v23-1821-05-01\"> <head>1 May 1821</head> <bibl> <author>JQA</author> <date type=\"creation\" when=\"1821-05-01\"/> <editor role=\"transcription\">Neal Millikan</editor> </bibl> <div type=\"docbody\"> <opener> <dateline>Washington May 22 1832</dateline> <salute>My Dear Sir</salute> </opener> <dateline><hi rend=\"italic\">May 1821.</hi></dateline> <p><date>1 V:15.</date> Tuesday. W. A. Schoolfield at the Office. About Slaves. Weekly tea-party resumed. I send you a list of the stockholders of the Bank of the U. States &#8211; foreign and domestic and am very sorry that I <unclear cert=\"low\">suffered</unclear> the promise I made to you in Baltimore to escape from my mind until it was brought back by your letter &#8211; my apology is the pressure of many engagements &#8211; and I hope the delay has not produced inconvenience. I am often accused by <persRef ref=\"richards-robert\">my husband</persRef> of being very deep in my plans and of not always telling at once what they are. <persRef ref=\"abel-mary\">Mrs Abel</persRef> is winning the confidence of people so that we shall have no difficulty in introducing our wares.</p> <p>Another paragraph here that discusses the daily errands of W. A. Schoolfield. Schoolfield's work was in Baltimore.</p> <closer> <salute>I am dear sir / very truly your / friend </salute> <signed>R. B. Taney</signed> </closer> </div> </div>\n",
    "\"\"\"\n",
    "\n",
    "# entity = 'R. B. Taney'\n",
    "entity = 'W. A. Schoolfield'\n",
    "\n",
    "label = 'PERSON'\n",
    "\n",
    "xml_id = \"schoolfied01\"\n",
    "\n",
    "#### Subset label_dict with input values from Checklist *****\n",
    "ner_values = ['PERSON']\n",
    "subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>previous_encoding</th>\n",
       "      <th>entities</th>\n",
       "      <th>descendant_order</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "      <th>uniq_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake.xml</td>\n",
       "      <td>&lt;w&gt;V:15.&lt;/date&gt;&lt;/w&gt; &lt;w&gt;Tuesday.&lt;/w&gt; &lt;w&gt;W.&lt;/w&gt; ...</td>\n",
       "      <td>(W. A. Schoolfield, PERSON)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake.xml</td>\n",
       "      <td>&lt;w&gt;daily&lt;/w&gt; &lt;w&gt;errands&lt;/w&gt; &lt;w&gt;of&lt;/w&gt; &lt;w&gt;W.&lt;/w...</td>\n",
       "      <td>(W. A. Schoolfield, PERSON)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W. A. Schoolfield</td>\n",
       "      <td>PERSON</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake.xml</td>\n",
       "      <td>&lt;w&gt;friend&lt;/w&gt; &lt;w&gt;&lt;/salute&gt;&lt;/w&gt; &lt;w&gt;&lt;signed&gt;R.&lt;/...</td>\n",
       "      <td>(R. B. Taney, PERSON)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>R. B. Taney</td>\n",
       "      <td>PERSON</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file                                  previous_encoding  \\\n",
       "0  fake.xml  <w>V:15.</date></w> <w>Tuesday.</w> <w>W.</w> ...   \n",
       "0  fake.xml  <w>daily</w> <w>errands</w> <w>of</w> <w>W.</w...   \n",
       "0  fake.xml  <w>friend</w> <w></salute></w> <w><signed>R.</...   \n",
       "\n",
       "                      entities  descendant_order             entity   label  \\\n",
       "0  (W. A. Schoolfield, PERSON)               1.0  W. A. Schoolfield  PERSON   \n",
       "0  (W. A. Schoolfield, PERSON)               1.0  W. A. Schoolfield  PERSON   \n",
       "0        (R. B. Taney, PERSON)               1.0        R. B. Taney  PERSON   \n",
       "\n",
       "  uniq_id  \n",
       "0          \n",
       "0          \n",
       "0          "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename = 'fake.xml'\n",
    "df = pd.DataFrame(columns = ['file', 'previous_encoding', 'entities'])\n",
    "\n",
    "# xml_file = open(abs_dir + \"TestEncoding/EditingData/ESR-EDA-1890-03-08 copy.xml\").read()\n",
    "xml_file = open(abs_dir + \"TestEncoding/EditingData/test_xml-before.xml\").read()\n",
    "xml_file = xml_file.encode('utf-8')\n",
    "    \n",
    "root = etree.fromstring(xml_file)\n",
    "ns = get_namespace(root)\n",
    "\n",
    "\n",
    "#             Search through elements for entities.\n",
    "desc_order = 0\n",
    "for child in root.findall('.//ns:body', ns): # Change this line to specify where to look for entities.\n",
    "\n",
    "    for descendant in child:\n",
    "        desc_order = desc_order + 1\n",
    "        df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "        \n",
    "#             Join data\n",
    "df = df \\\n",
    "    .explode('entities') \\\n",
    "    .dropna()\n",
    "\n",
    "df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "\n",
    "# Add additional columns for user input.\n",
    "df['uniq_id'] = ''\n",
    "\n",
    "df['previous_encoding'] = df.apply(lambda row: get_kwic_encoding(row['entity'], \n",
    "                                                                 row['previous_encoding'], banned_list,\n",
    "                                                                 30),\n",
    "                                   axis = 1)\n",
    "\n",
    "#     Explode lists within more than one item.\n",
    "df = df.explode('previous_encoding').dropna().drop_duplicates()\n",
    "\n",
    "\n",
    "# print (up_convert_encoding(df['previous_encoding'].head(1)[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:mhs=\"http://www.masshist.org/ns/1.0\" xmlns:tei=\"http://www.tei-c.org/ns/1.0\" xml:id=\"v23-1821-05\"> <teiHeader> <fileDesc> <titleStmt> <title>Test Encoding: Copy-Pasted Contents from Three Editions</title> </titleStmt> <publicationStmt><p/></publicationStmt> <sourceDesc><p/></sourceDesc> </fileDesc> </teiHeader> <text> <body> <div type=\"entry\" xml:id=\"jqadiaries-v23-1821-05-01\"> <head>1 May 1821</head> <bibl> <author>JQA</author> <date type=\"creation\" when=\"1821-05-01\"/> <editor role=\"transcription\">Neal Millikan</editor> </bibl> <div type=\"docbody\"> <opener> <dateline>Washington May 22 1832</dateline> <salute>My Dear Sir</salute> </opener> <dateline><hi rend=\"italic\">May 1821.</hi></dateline> <p><date>1 V:15.</date> Tuesday. <persName type=\"nerHelper-added\">W. A. Schoolfield</persName> at the Office. About Slaves. Weekly tea-party resumed. I send you a list of the stockholders of the Bank of the U. States – foreign and domestic and am very sorry that I <unclear cert=\"low\">suffered</unclear> the promise I made to you in Baltimore to escape from my mind until it was brought back by your letter – my apology is the pressure of many engagements – and I hope the delay has not produced inconvenience. I am often accused by <persRef ref=\"richards-robert\">my husband</persRef> of being very deep in my plans and of not always telling at once what they are. <persRef ref=\"abel-mary\">Mrs Abel</persRef> is winning the confidence of people so that we shall have no difficulty in introducing our wares.</p> <p>Another paragraph here that discusses the daily errands of <persName type=\"nerHelper-added\">W. A. Schoolfield.</persName> Schoolfield\\'s work was in Baltimore.</p> <closer> <salute>I am dear sir / very truly your / friend </salute> <persName type=\"nerHelper-added\"><signed>R. B. Taney</signed></persName> </closer> </div> </div> </body> </text> </TEI> '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xml_file = open(abs_dir + \"TestEncoding/EditingData/ESR-EDA-1890-03-08 copy.xml\").read()\n",
    "xml_file = open(abs_dir + \"TestEncoding/EditingData/test_xml-before.xml\").read()\n",
    "xml_file = xml_file.encode('utf-8')\n",
    "\n",
    "\n",
    "root = etree.fromstring(xml_file)\n",
    "ns = get_namespace(root)    \n",
    "\n",
    "#     Convert XML structure to string for regex processing.\n",
    "tree_as_string = etree.tostring(root, pretty_print = True).decode('utf-8')\n",
    "\n",
    "csv_df = df\n",
    "new_data = commit_revisions(label_dict, csv_df)\n",
    "\n",
    "# new_data\n",
    "\n",
    "\n",
    "tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "\n",
    "for index, row in new_data.iterrows():\n",
    "    tree_as_string = re.sub(f'(.*)({row.previous_encoding})(.*)',\n",
    "                            f'\\\\1{row.new_encoding}\\\\3',\n",
    "                            tree_as_string)\n",
    "    \n",
    "#     Check well-formedness (will fail if not well-formed)\n",
    "doc = etree.fromstring(tree_as_string)\n",
    "et = etree.ElementTree(doc)\n",
    "\n",
    "#     Convert to string.\n",
    "et = re.sub('\\s+', ' ',\n",
    "            etree.tostring(et, encoding='unicode', method='xml', pretty_print = True))\n",
    "\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<string>:17:0:ERROR:RELAXNGV:RELAXNG_ERR_ELEMWRONG: Did not expect element head there\n",
       "<string>:18:0:ERROR:RELAXNGV:RELAXNG_ERR_EXTRACONTENT: Element div has extra content: head\n",
       "<string>:16:0:ERROR:RELAXNGV:RELAXNG_ERR_ELEMWRONG: Did not expect element div there\n",
       "<string>:16:0:ERROR:RELAXNGV:RELAXNG_ERR_CONTENTVALID: Element body failed to validate content"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_file = open(abs_dir + \"TestEncoding/EditingData/test_xml-before.xml\").read()\n",
    "xml_file = xml_file.encode('utf-8').decode('utf-8')\n",
    "doc = etree.fromstring(bytes(xml_file, encoding = 'utf8'))\n",
    "\n",
    "relaxng_doc = open(abs_dir + \"TestEncoding/EditingData/primarysourcecoop.rng\").read()\n",
    "relaxng_doc = etree.XML(relaxng_doc.encode('utf-8'))\n",
    "relaxng = etree.RelaxNG(relaxng_doc)\n",
    "\n",
    "relaxng.validate(doc)\n",
    "\n",
    "log = relaxng.error_log\n",
    "\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
