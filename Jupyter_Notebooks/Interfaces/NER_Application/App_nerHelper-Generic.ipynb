{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerHelper\n",
    "## Application for Reading & Updating XML with NER\n",
    "\n",
    "Once names authority is ready use following encoding to link to external documents:\n",
    "\n",
    "https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-listPrefixDef.html\n",
    "\n",
    "\n",
    "```xml\n",
    "<listPrefixDef>\n",
    "    <prefixDef ident='psc' matchPattern=\"([a-z]+[a-z0-9]*)\" replacementPattern=\"personography.xml#$1\">\n",
    "        <p>Private URIs using the <code>bibl</code> prefix can be\n",
    "         expanded to form URIs which retrieve the relevant\n",
    "         bibliographical reference from www.example.com.</p>\n",
    "    </prefixDef>\n",
    "</listPrefixDef>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, re, glob, datetime, csv, sys, os, base64, io, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "import dash, dash_table\n",
    "import dash_core_components as dcc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_html_components as html\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "# Import spaCy language model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ignore simple warnings.\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Declare directory location to shorten filepaths later.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/GitHub/dsg-mhs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_dict = {'PERSON':'persName',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Get Namespaces\n",
    "\"\"\"\n",
    "def get_namespace(root):\n",
    "    namespace = re.match(r\"{(.*)}\", str(root.tag))\n",
    "    ns = {\"ns\":namespace.group(1)}\n",
    "    return ns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Convert to String\n",
    "\"\"\"\n",
    "def get_text(elem):\n",
    "    text_list = []\n",
    "    text = ''.join(etree.tostring(elem, encoding='unicode', method='text', with_tail=False))\n",
    "    text_list.append(re.sub(r'\\s+', ' ', text))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "XML Parsing Function: Get Encoded Content\n",
    "\"\"\"    \n",
    "def get_encoding(elem):\n",
    "    encoding = etree.tostring(elem, pretty_print = True).decode('utf-8')\n",
    "    encoding = re.sub('\\s+', ' ', encoding) # remove additional whitespace\n",
    "    return encoding\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "NER Function\n",
    "\"\"\"\n",
    "# spaCy\n",
    "def get_spacy_entities(text, subset_ner):\n",
    "    sp_entities_l = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in subset_ner.keys():\n",
    "            sp_entities_l.append((str(ent), ent.label_))\n",
    "        else:\n",
    "            pass\n",
    "    return sp_entities_l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Retrieve Contents\n",
    "\"\"\"\n",
    "def get_contents(ancestor, xpath_as_string, namespace, subset_ner):\n",
    "    \n",
    "    textContent = get_text(ancestor) # Get plain text.\n",
    "    encodedContent = get_encoding(ancestor) # Get encoded content.\n",
    "    sp_entities_l = get_spacy_entities(textContent, subset_ner) # Get named entities from plain text.\n",
    "\n",
    "    return (sp_entities_l, encodedContent)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & Regex: Up Conversion\n",
    "\n",
    "Function replaces all spaces between beginning and end tags with underscores.\n",
    "Then, function wraps each token (determined by whitespace) with word tags (<w>...</w>)\n",
    "\"\"\"\n",
    "def up_convert_encoding(column):\n",
    "#     Regularize spacing & store data as new variable ('converted_encoding').\n",
    "    converted_encoding = re.sub('\\s+', ' ', column, re.MULTILINE)\n",
    "    \n",
    "#     Create regex that replaces spaces with underscores if spaces occur within tags.\n",
    "#     This regex treats tags as a single token later.\n",
    "    tag_regex = re.compile('<(.*?)>')\n",
    "\n",
    "#     Accumulate underscores through iteration\n",
    "    for match in re.findall(tag_regex, column):\n",
    "        replace_space = re.sub('\\s', '_', match)\n",
    "        converted_encoding = re.sub(match, replace_space, converted_encoding)\n",
    "    \n",
    "#     Up-Converstion\n",
    "#     Tokenize encoding and text, appending <w> tags, and re-join.\n",
    "    converted_encoding = converted_encoding.split(' ')\n",
    "    for idx, item in enumerate(converted_encoding):\n",
    "        item = '<w>' + item + '</w>'\n",
    "        converted_encoding[idx] = item\n",
    "    converted_encoding = ' '.join(converted_encoding)\n",
    "    \n",
    "    return converted_encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Intersperse Entity with Likely TEI Information for Capacious Regex\n",
    "\"\"\"\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 - 0)\n",
    "    result[0::2] = lst\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML Function: Build KWIC of Found Entities in Up Converted Encoding\n",
    "\"\"\"\n",
    "def get_kwic_encoding(entity, encoding, banned_list, kwic_range):\n",
    "#     Up convert arguments.\n",
    "    converted_encoding = up_convert_encoding(encoding)\n",
    "    converted_entity = up_convert_encoding(entity)\n",
    "\n",
    "#     Intersperse & 'up convert' by hand entity.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['(<.*?>)']) + ']*'\n",
    "\n",
    "    expanded_regex = r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    expanded_entity = re.sub('\\s', '</w> <w>', expanded_regex)\n",
    "    \n",
    "#     <w>(?:(?!<w>).)*\n",
    "#     'Tempered greedy token solution', <w> cannot appear after a <w>, unless within expanded_entity\n",
    "#     entity_regex = re.compile('(<w>(?:(?!<w>).)*' + expanded_entity + '.*?</w>)')\n",
    "    entity_regex = re.compile('([^\\s]*' + expanded_entity + '[^\\s]*)')\n",
    "    \n",
    "    \n",
    "    # Use regex match as final conv. entity.\n",
    "    try:\n",
    "        kwic_dict = {entity: []}\n",
    "        for m in entity_regex.finditer(converted_encoding):\n",
    "            \n",
    "            if any(item in m.group() for item in banned_list):\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "#                 Gather context:\n",
    "#                 Start of match (m.start()) minus kwic_range through end of match plus kwic_range.\n",
    "                context = converted_encoding[ m.start() - kwic_range : m.end() + kwic_range]\n",
    "                kwic_dict[entity].append(context)\n",
    "        \n",
    "        \n",
    "#         For each item in entity list, create new regex and expand until reaches preceeding </w> and trailing <w>.\n",
    "        for n, i in enumerate(kwic_dict[entity]):\n",
    "            complete_kwic = re.search(f'([^\\s]*{i}[^\\s]*)', converted_encoding).group()\n",
    "            kwic_dict[entity][n] = complete_kwic\n",
    "        \n",
    "#         Return values only\n",
    "        return kwic_dict[entity]\n",
    "            \n",
    "    except AttributeError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: & NER: Create Dataframe of Entities\n",
    "\"\"\"\n",
    "def make_dataframe(descendant, df, ns, subset_ner, filename, descendant_order):\n",
    "    entities, previous_encoding = get_contents(descendant, './/ns:.', ns, subset_ner)\n",
    "    \n",
    "    df = df.append({\n",
    "        'file':re.sub('.*/(.*.xml)', '\\\\1', filename),\n",
    "        'descendant_order': descendant_order,\n",
    "        'previous_encoding': previous_encoding,\n",
    "        'entities':entities,\n",
    "    },\n",
    "        ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parse Contents: XML Structure (ouput-data-upload)\n",
    "\"\"\"\n",
    "def parse_contents(contents, filename, date, ner_values):\n",
    "    ner_values = ner_values.split(',')\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string).decode('utf-8')\n",
    "    \n",
    "    # Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    #### Subset label_dict with input values from Checklist *****\n",
    "    subset_ner = {k: label_dict[k] for k in ner_values}\n",
    "    \n",
    "#     Run XML Parser + NER here.\n",
    "    try:\n",
    "#         Assume that the user uploaded a CSV file\n",
    "        if 'csv' in filename:\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded)\n",
    "            )\n",
    "            \n",
    "#         Assume that the user uploaded an XML file\n",
    "        elif 'xml' in filename:\n",
    "            xml_file = decoded.encode('utf-8')\n",
    "            \n",
    "            df = pd.DataFrame(columns = ['file', 'previous_encoding', 'entities'])\n",
    "            \n",
    "            root = etree.fromstring(xml_file)\n",
    "            ns = get_namespace(root)\n",
    "            \n",
    "#             Search through elements for entities.\n",
    "            desc_order = 0\n",
    "            for child in root.findall('.//ns:body', ns): # Change this line to specify where to look for entities.\n",
    "                \n",
    "                for descendant in child:\n",
    "                    desc_order = desc_order + 1\n",
    "                    df = make_dataframe(descendant, df, ns, subset_ner, filename, desc_order)\n",
    "            \n",
    "#             Join data\n",
    "            df = df \\\n",
    "                .explode('entities') \\\n",
    "                .dropna()\n",
    "\n",
    "            df[['entity', 'label']] = pd.DataFrame(df['entities'].tolist(), index = df.index)\n",
    "        \n",
    "            # Add additional columns for user input.\n",
    "            df['uniq_id'] = ''\n",
    "            \n",
    "#             Replace 'previous_encoding' with a KWIC version containing entity.\n",
    "            df['previous_encoding'] = df.apply(lambda row: get_kwic_encoding(row['entity'],\n",
    "                                                                             row['previous_encoding'], \n",
    "                                                                             banned_list,\n",
    "                                                                             30),\n",
    "                                               axis = 1)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div([\n",
    "            f'There was an error processing this file: {e}.'\n",
    "    ])\n",
    "    \n",
    "#     Explode lists within more than one item.\n",
    "    df = df.explode('previous_encoding').dropna().drop_duplicates()\n",
    "    \n",
    "#     Return HTML with outputs.\n",
    "    return filename, date, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Remove word tags and clean up\n",
    "\"\"\"\n",
    "def xml_cleanup(encoding):\n",
    "#     Clean up any additional whitespace and remove word tags.\n",
    "    encoding = re.sub('\\s+', ' ', encoding, re.MULTILINE)\n",
    "    encoding = re.sub('(<[/]?w>)', '', encoding)\n",
    "\n",
    "    encoding = re.sub('_', ' ', encoding) # Remove any remaining underscores in tags.\n",
    "    encoding = re.sub('“', '\"', encoding) # Change quotation marks to correct unicode.\n",
    "    encoding = re.sub('”', '\"', encoding)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reading Pane: Highlight Found Entity\n",
    "\"\"\"\n",
    "def highlighter(previous_encoding, entity):\n",
    "#     Remove all tags.\n",
    "    highlighted_text = re.sub('(<.*?>)', '', previous_encoding) \n",
    "    \n",
    "    entity_match = re.search(f'(.*)({entity})(.*)', highlighted_text)\n",
    "    \n",
    "    highlighted_text = html.P([entity_match.group(1), html.Mark(entity_match.group(2)), entity_match.group(3)])\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "\"\"\"\n",
    "XML Parsing Function: Suggest New Encoding with Hand Edits\n",
    "\n",
    "Similar to make_ner_suggestions(), this function folds in revision using regular expressions.\n",
    "The outcome is the previous encoding with additional encoded information determined by user input.\n",
    "\n",
    "Expected Columns:\n",
    "    previous_encoding\n",
    "    entities\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def revise_with_selections(label_dict, label, uniq_id, entity, previous_encoding):\n",
    "    \n",
    "    label = label_dict[label]\n",
    "    \n",
    "#     <w>(?:(?!<w>).)*\n",
    "#     'Tempered greedy token solution', <w> cannot appear after a <w>, unless it's within expanded_entity.\n",
    "    expanded_entity = [c for c in entity]\n",
    "    expanded_regex = '[' + \"|\".join(['(<.*?>)']) + ']*'\n",
    "    expanded_regex = r''.join(intersperse(expanded_entity, expanded_regex))\n",
    "    expanded_entity = re.sub('\\s', '</w> <w>', expanded_regex)\n",
    "    \n",
    "#     [^\\s]*(</w>)?\n",
    "#     Match anything except for whitespace until first </w> appears.\n",
    "#     expanded_entity = f'(<w>(?:(?!<w>).)*{expanded_entity}[^\\s]*(</w>)?)'\n",
    "    expanded_entity = f'([^\\s]*{expanded_entity}[^\\s]*)'\n",
    "    \n",
    "    matched_entity = re.search(expanded_entity, previous_encoding).group()\n",
    "    \n",
    "#     If there is a unique id to add & hand edits...\n",
    "    if uniq_id != '':\n",
    "        revised_encoding = re.sub(f'{matched_entity}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{matched_entity}</{label}>',\n",
    "                                  previous_encoding)        \n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "\n",
    "        return revised_encoding\n",
    "    \n",
    "    elif uniq_id == '':\n",
    "        revised_encoding = re.sub(f'{matched_entity}',\n",
    "                                  f'<{label} type=\"nerHelper-added\">{matched_entity}</{label}>',\n",
    "                                  previous_encoding)\n",
    "        revised_encoding = xml_cleanup(revised_encoding)\n",
    "        \n",
    "        return revised_encoding\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Update/Inherit Accepted Changes\n",
    "Expects a dataframe (from a .csv) with these columns:\n",
    "    file\n",
    "    abridged_xpath\n",
    "    descendant_order\n",
    "    previous_encoding\n",
    "    entities\n",
    "    new_encoding\n",
    "    uniq_id\n",
    "\"\"\"\n",
    "def commit_revisions(label_dict, dataframe):\n",
    "    \n",
    "    dataframe = dataframe.fillna('').reset_index()\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        revised_by_hand = revise_with_selections(label_dict, row['label'], row['uniq_id'],\n",
    "                                                 row['entity'], row['previous_encoding'])\n",
    "        dataframe.loc[index, 'new_encoding'] = revised_by_hand\n",
    "    \n",
    "#     Clean up previous_encoding (remove word tags)\n",
    "    dataframe['previous_encoding'] = dataframe.apply(lambda row: xml_cleanup(row['previous_encoding']), axis = 1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML & NER: Write New XML File with Accepted Revisions\n",
    "Expects:\n",
    "    XML File with Original Encoding\n",
    "    CSV File with Accepted Changes\n",
    "    Label Dictionary\n",
    "\"\"\"\n",
    "def revise_xml(xml_contents, csv_df):\n",
    "#     Label dictionary.\n",
    "    label_dict = {'PERSON':'persRef',\n",
    "                  'LOC':'placeName', # Non-GPE locations, mountain ranges, bodies of water.\n",
    "                  'GPE':'placeName', # Countries, cities, states.\n",
    "                  'FAC':'placeName', # Buildings, airports, highways, bridges, etc.\n",
    "                  'ORG':'orgName', # Companies, agencies, institutions, etc.\n",
    "                  'NORP':'name', # Nationalities or religious or political groups.\n",
    "                  'EVENT':'name', # Named hurricanes, battles, wars, sports events, etc.\n",
    "                  'WORK_OF_ART':'name', # Titles of books, songs, etc.\n",
    "                  'LAW':'name', # Named documents made into laws.\n",
    "                  'DATE':'date' # Absolute or relative dates or periods.\n",
    "                 }\n",
    "    \n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    xml_file = xml_decoded.encode('utf-8')\n",
    "    \n",
    "    root = etree.fromstring(xml_file)\n",
    "    ns = get_namespace(root)    \n",
    "    \n",
    "#     Convert XML structure to string for regex processing.\n",
    "    tree_as_string = etree.tostring(root, pretty_print = True).decode('utf-8')\n",
    "    tree_as_string = re.sub('\\s+', ' ', tree_as_string) # remove additional whitespace\n",
    "    \n",
    "    new_data = commit_revisions(label_dict, csv_df)\n",
    "    \n",
    "#     Write accepted code into XML tree.\n",
    "    for index, row in new_data.iterrows():\n",
    "        tree_as_string = re.sub(f'(.*)({row.previous_encoding})(.*)',\n",
    "                                f'\\\\1{row.new_encoding}\\\\3',\n",
    "                                tree_as_string)\n",
    "        \n",
    "#     Check well-formedness (will fail if not well-formed)\n",
    "    doc = etree.fromstring(tree_as_string)\n",
    "    et = etree.ElementTree(doc)\n",
    "    \n",
    "#     Convert to string.\n",
    "    et = etree.tostring(et, encoding='unicode', method='xml')\n",
    "    return et\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "XML: Write Schema Information before Root\n",
    "Input: \n",
    "    - Revised XML document (return variable from revise_xml())\n",
    "    - XML File with Original Encoding\n",
    "\"\"\"\n",
    "def write_schema_information(xml_contents, final_revisions):\n",
    "    xml_content_type, xml_content_string = xml_contents.split(',')\n",
    "    xml_decoded = base64.b64decode(xml_content_string).decode('utf-8')\n",
    "    \n",
    "    xml_file = xml_decoded.encode('utf-8').decode('utf-8')\n",
    "    xml_file = re.sub('\\s+', ' ', xml_file)\n",
    "    \n",
    "    schema_match = re.search('(<?.*)(<TEI.*)', xml_file)\n",
    "    schema_match = schema_match.group(1)\n",
    "    \n",
    "    completed_document = schema_match + final_revisions\n",
    "\n",
    "    return completed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n",
      "CPU times: user 26.2 ms, sys: 11.1 ms, total: 37.3 ms\n",
      "Wall time: 49.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# External JavaScript files\n",
    "external_scripts = [\n",
    "    'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js',\n",
    "    {'src':'https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js'}\n",
    "]\n",
    "\n",
    "app = JupyterDash(__name__, \n",
    "                  external_scripts = external_scripts)\n",
    "\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "\n",
    "# Preset variables.\n",
    "ner_labels = ['PERSON','LOC','GPE','FAC','ORG','NORP','EVENT','WORK_OF_ART','LAW','DATE']\n",
    "\n",
    "# Banned List (list of elements that already encode entities)\n",
    "banned_list = ['persRef', 'date']\n",
    "\n",
    "# Layout.\n",
    "app.layout = html.Div([\n",
    "    \n",
    "#     Title\n",
    "    html.Div(\n",
    "        className=\"app-header\",\n",
    "        children = [\n",
    "            html.Div('nerHelper Application', className = \"app-header--title\")\n",
    "        ]),\n",
    "\n",
    "#     Add or substract labels to list for NER to find. Complete list of NER labels: https://spacy.io/api/annotation\n",
    "    html.H2('Select Entities to Search For'),\n",
    "    \n",
    "#     Add legend & checklist for ner_labels.\n",
    "    html.Table([\n",
    "        html.Thead([\n",
    "            html.Tr([\n",
    "                html.Th('Label'),\n",
    "                html.Th('Definition'),\n",
    "            ]),\n",
    "        ]),\n",
    "        html.Tbody([\n",
    "            html.Tr([\n",
    "                html.Td('PERSON'),\n",
    "                html.Td('A person\\'s name (proper noun)' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('LOC'),\n",
    "                html.Td('Non-GPE locations, mountain ranges, bodies of water.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('GPE'),\n",
    "                html.Td('Countries, cities, states.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('FAC'),\n",
    "                html.Td('Buildings, airports, highways, bridges, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('ORG'),\n",
    "                html.Td('Companies, agencies, institutions, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('NORP'),\n",
    "                html.Td('Nationalities or religious or political groups.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('EVENT'),\n",
    "                html.Td('Named hurricanes, battles, wars, sports events, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('WORK_OF_ART'),\n",
    "                html.Td('Titles of books, songs, etc.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('LAW'),\n",
    "                html.Td('Named documents made into laws.' ),\n",
    "            ]),\n",
    "            html.Tr([\n",
    "                html.Td('DATE'),\n",
    "                html.Td('Absolute or relative dates or periods.' ),\n",
    "            ]),\n",
    "        ]),\n",
    "    ]),\n",
    "    \n",
    "    dcc.Checklist(\n",
    "        id = 'ner-checklist',\n",
    "        options = [{\n",
    "            'label': i,\n",
    "            'value': i\n",
    "        } for i in ner_labels],\n",
    "        value = ['PERSON', 'LOC', 'GPE']\n",
    "    ),\n",
    "    \n",
    "    \n",
    "#     Upload Data Area.\n",
    "    html.H2('Upload File'),\n",
    "    dcc.Upload(\n",
    "        id = 'upload-data',\n",
    "        children = html.Div([\n",
    "            'Drag and Drop or ', html.A('Select File')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '95%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        multiple=True # Allow multiple files to be uploaded\n",
    "    ),\n",
    "    \n",
    "#     Store uploaded data.\n",
    "    dcc.Store(id = 'data-upload-store'),\n",
    "    \n",
    "#     Display pane for file information.\n",
    "    html.Div(id = 'file-information'),\n",
    "    \n",
    "#     Display pane for data as table.\n",
    "    dash_table.DataTable(id = 'data-table-container',\n",
    "                         row_selectable=\"single\",\n",
    "                         selected_rows = [0],\n",
    "                         editable = True,\n",
    "                         page_size=1,\n",
    "                        ),\n",
    "    \n",
    "#     Display pane for reading data from selected row & revision options.\n",
    "    html.Div(id = 'reading-container'),\n",
    "    html.Div(id = 'revision-radio-container'),\n",
    "    html.Div(id = 'revision-text-container'),\n",
    "    html.Div(id = 'revision-button-container'),\n",
    "    \n",
    "#     Store revised data.\n",
    "    dcc.Store(id = 'revisions-store'),\n",
    "    \n",
    "    \n",
    "#     Div to hold button that will write and download XML file.\n",
    "    html.Div(id = 'write-button-container'),\n",
    "    \n",
    "    html.Div(id = 'download-button-container')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "#####                             ##################################################################################\n",
    "#####     Callbacks               ##################################################################################\n",
    "#####                             ##################################################################################\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Upload data & create table.\n",
    "@app.callback([Output('file-information', 'children'),\n",
    "               Output('data-upload-store', 'data')],\n",
    "              [Input('upload-data', 'contents'),\n",
    "               Input('ner-checklist', 'value')],\n",
    "              [State('upload-data', 'filename'),\n",
    "               State('upload-data', 'last_modified')])\n",
    "def upload_data(list_of_contents, ner_values, list_of_names, list_of_dates):\n",
    "    if list_of_contents is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "#     Parse uploaded contents.\n",
    "    children = [\n",
    "        parse_contents(c, n, d, ner) for c, n, d, ner in\n",
    "        zip(list_of_contents, list_of_names, list_of_dates, ner_values)\n",
    "    ]\n",
    "    data = children[0][2]\n",
    "    \n",
    "#     Extract file information.\n",
    "    file_information = html.Div([html.P(f'File name: {children[0][0]}'),\n",
    "                                 html.P(f'Last Modified: {datetime.datetime.fromtimestamp(children[0][1])}')])    \n",
    "    \n",
    "    return file_information, data.to_dict('rows')\n",
    "\n",
    "\n",
    "\n",
    "# Generate table with data from store.\n",
    "@app.callback([Output('data-table-container', 'data'),\n",
    "               Output('data-table-container', 'columns')],\n",
    "              Input('data-upload-store', 'data'))\n",
    "def populate_data_table(data):\n",
    "\n",
    "    df = pd.DataFrame(data)[['file', 'entity', 'label']]\n",
    "    cols = [{'name':i, 'id': i} for i in df.columns]\n",
    "\n",
    "    return df.to_dict('rows'), cols\n",
    "\n",
    "\n",
    "\n",
    "# Create reading pane & revision options once row from table is selected.\n",
    "@app.callback([Output('reading-container', 'children'),\n",
    "               Output('revision-radio-container', 'children'),\n",
    "               Output('revision-text-container', 'children'),\n",
    "               Output('revision-button-container', 'children')],\n",
    "              [Input('data-upload-store', 'data'),\n",
    "               Input('data-table-container', 'selected_rows')])\n",
    "def create_reading_and_revisions_pane(data, selected_rows):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "        \n",
    "    reading_df = pd.DataFrame(data).iloc[selected_rows]\n",
    "\n",
    "#     Access previous and new encoding and squeeze() them to return only scalar (the text).\n",
    "#     Use highlighter() to re-construct previous_encoding with html.Mark() around found entity.\n",
    "    highlighted_text = highlighter(reading_df['previous_encoding'].squeeze(),\n",
    "                                   reading_df['entity'].squeeze())\n",
    "    \n",
    "    reading_pane = html.Div([\n",
    "        html.H2('Found Entity'),\n",
    "        html.Div(highlighted_text),\n",
    "        html.H2('Revisions Options'),\n",
    "        html.P('Please confirm the correct label that describes the entity.'),\n",
    "        html.P('If you\\'ve selected \"PERSON,\" you must also hand-type an xml:id below. The xml:id should match an entity in the names authority database.'),\n",
    "    ])\n",
    "    \n",
    "#     Choose correct entity label with radio buttons.\n",
    "    revision_radio = dcc.RadioItems(\n",
    "            id = 'radioInput',\n",
    "            options = [{'label':'PERSON', 'value':'PERSON'},\n",
    "                       {'label':'LOC', 'value':'LOC'},\n",
    "                       {'label':'GPE', 'value':'GPE'},\n",
    "                       {'label':'FAC', 'value':'FAC'},\n",
    "                       {'label':'ORG', 'value':'ORG'},\n",
    "                       {'label':'NORP', 'value':'NORP'},\n",
    "                       {'label':'EVENT', 'value':'EVENT'},\n",
    "                       {'label':'WORK_OF_ART', 'value':'WORK_OF_ART'},\n",
    "                       {'label':'LAW', 'value':'LAW'},\n",
    "                       {'label':'DATE', 'value':'DATE'},\n",
    "                       {'label':'No Changes', 'value':''}\n",
    "            ]\n",
    "        ),\n",
    "    \n",
    "#     Create text area for manual changes.\n",
    "    revision_text = dcc.Input(id = 'textInput', type = 'text', \n",
    "                              placeholder = 'Type a Unique ID here.', value = '', debounce = True)\n",
    "        \n",
    "#     Create button for committing changes.\n",
    "    revision_button = html.Button('Confirm Changes?', id = 'confirm-button', n_clicks = 0),\n",
    "    \n",
    "    return reading_pane, revision_radio, revision_text, revision_button\n",
    "\n",
    "\n",
    "# Once a revisions is accepted, write row with instructions to dataframe.\n",
    "@app.callback(Output('revisions-store', 'data'),\n",
    "              [Input('revision-button-container', 'n_clicks'),\n",
    "               Input('data-upload-store', 'data'),\n",
    "               Input('data-table-container', 'selected_rows'),\n",
    "               Input('revision-radio-container', 'children'), \n",
    "               Input('revision-text-container', 'children')],\n",
    "              State('revisions-store', 'data'))\n",
    "def commit_revisions_to_dataframe(n_clicks, data, selected_rows,\n",
    "                                  radio_children, text_children, revisions):\n",
    "    \n",
    "#     Only run if the n_click 'id' is triggered by the revision-button-container.\n",
    "    changed_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "        \n",
    "    if changed_id != 'revision-button-container.n_clicks':\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    revised_row = pd.DataFrame(data).iloc[selected_rows]\n",
    "        \n",
    "#     Check if radio_ and text_children each have a value by seeing if a 'value' key is nested in 'props'.\n",
    "    if 'value' in radio_children[0]['props']:\n",
    "        radio_value = radio_children[0]['props']['value']\n",
    "    else:\n",
    "        radio_value = ''\n",
    "    \n",
    "    if 'value' in text_children['props']:\n",
    "        text_value = text_children['props']['value']\n",
    "    else:\n",
    "        text_value = ''\n",
    "    \n",
    "    revised_row['uniq_id'] = text_value\n",
    "    revised_row['label'] = radio_value\n",
    "    \n",
    "    if revisions is None:\n",
    "        revisions = pd.DataFrame(revised_row)\n",
    "    else:\n",
    "        revisions = pd.DataFrame(revisions)\n",
    "        revisions = revisions.append(revised_row, ignore_index = True)\n",
    "    \n",
    "    return revisions.to_dict('rows')\n",
    "\n",
    "    \n",
    "\n",
    "# After last revision (or whenever one change completed), provide button to commit changes to XML.\n",
    "@app.callback(Output('write-button-container', 'children'),\n",
    "              Input('revisions-store', 'data'))\n",
    "def provide_button_to_download_revisions(data):\n",
    "    if data is None:\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    return html.Button('Finished? Download Revised XML.', id = 'write-xml-button')\n",
    "\n",
    "\n",
    "# Run functions to revise XML and download new document.\n",
    "@app.callback(Output('download-button-container', 'children'),\n",
    "              [Input('write-button-container', 'n_clicks'),\n",
    "               Input('upload-data', 'contents'),\n",
    "               Input('revisions-store', 'data')],\n",
    "              State('upload-data', 'filename'))\n",
    "def provide_download_link(n_clicks, contents, revisions, filename):\n",
    "    write_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "    \n",
    "    if write_id != 'write-button-container.n_clicks':\n",
    "        raise PreventUpdate\n",
    "    \n",
    "    xml_contents = contents[0]\n",
    "    revisions = pd.DataFrame(revisions)\n",
    "    \n",
    "    final_revisions = revise_xml(xml_contents, revisions)\n",
    "    \n",
    "    completed_file = write_schema_information(xml_contents, final_revisions)\n",
    "    \n",
    "    path = f\"revised-{filename[0]}\"\n",
    "    with open(path, \"w\") as file:\n",
    "        file.write(completed_file)\n",
    "\n",
    "    return f'{filename[0]} downloaded! Please review the XML document for well-formedness.'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     app.run_server(mode = 'inline', debug = True) # mode = 'inline' for JupyterDash\n",
    "    app.run_server(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
