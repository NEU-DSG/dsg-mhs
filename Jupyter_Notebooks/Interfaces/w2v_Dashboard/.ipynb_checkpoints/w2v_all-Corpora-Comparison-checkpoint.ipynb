{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3af7a4",
   "metadata": {},
   "source": [
    "# word2vec Corpus Comparison\n",
    "\n",
    "> After building models and saving them, only the second half of this notebook will be necessary for examining corpora.\n",
    "\n",
    "This notebook first builds w2v models for each corpus separately. Then, it combines texts from all corpora and creates a single model. \n",
    "\n",
    "[Following the same sequence, word vectors can be projected onto 2-dimensional space.]\n",
    "\n",
    "The second half of this notebook includes a Dash application to compare word vectors across space.\n",
    "\n",
    "The results of these word vectors are for exploratory purposes. The way that word2vec weighs and projects vectors into space can make cross-corpora comparisons tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374c4e7",
   "metadata": {},
   "source": [
    "## Read In Libraries & Function\n",
    "\n",
    "The method for inputting texts into gensim is from Laura Nelson (link?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba9785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, string, glob, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('JQA_XML_parser.py'), '../../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "from JQA_XML_parser import *\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('Correspondence_XML_parser.py'), '../../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "from Correspondence_XML_parser import *\n",
    "\n",
    "# Declare absolute path.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/\"\n",
    "\n",
    "# Define tokenizer.\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    punct = string.punctuation + '“' + '”' + '‘' + \"’\"\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    lower_case = lower_case.replace('—', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punct])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4523055",
   "metadata": {},
   "source": [
    "### Richards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19925061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Richards\n",
    "# Declare directory location to shorten filepaths later.\n",
    "input_directory = \"Data/PSC/Richards/ESR-XML-Files-MHS/*.xml\"\n",
    "files = glob.glob(abs_dir + input_directory)\n",
    "\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_richards.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96e91d",
   "metadata": {},
   "source": [
    "### Sedgwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6373b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Sedgwick\n",
    "input_directory = \"Data/PSC/Sedgwick/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_sedgwick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a80ee",
   "metadata": {},
   "source": [
    "### Taney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30fc20af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Taney\n",
    "input_directory = \"Data/PSC/Taney/RBT_RawXML/*/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_taney.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bdaf1",
   "metadata": {},
   "source": [
    "### JQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31c31ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_textContent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_textContent' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Declare variables.\n",
    "\"\"\"\n",
    "\n",
    "# Declare regex to simplify file paths below\n",
    "regex = re.compile(r'.*/.*/(.*.xml)')\n",
    "\n",
    "# Declare document level of file. Requires root starting point ('.').\n",
    "doc_as_xpath = './/ns:div/[@type=\"entry\"]'\n",
    "\n",
    "# Declare text level within each document.\n",
    "text_path = './ns:div/[@type=\"docbody\"]/ns:p'\n",
    "\n",
    "\"\"\"\n",
    "Build dataframe.\n",
    "\"\"\"\n",
    "\n",
    "dataframe = []\n",
    "\n",
    "for file in glob.glob(abs_dir + 'Data/PSC/JQA/*/*.xml'):\n",
    "    reFile = str(regex.search(file).group(1))\n",
    "#         Call functions to create necessary variables and grab content.\n",
    "    root = get_root(file)\n",
    "    ns = get_namespace(root)\n",
    "\n",
    "    for eachDoc in root.findall(doc_as_xpath, ns):\n",
    "#             Call functions.\n",
    "        text = get_textContent(eachDoc, text_path, ns)\n",
    "\n",
    "        dataframe.append([text])\n",
    "\n",
    "dataframe = pd.DataFrame(dataframe, columns = ['text'])\n",
    "\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in dataframe['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_jqa.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e44e8",
   "metadata": {},
   "source": [
    "### All Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b42ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 )\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Richards\n",
    "# Declare directory location to shorten filepaths later.\n",
    "input_directory = \"Data/PSC/Richards/ESR-XML-Files-MHS/*.xml\"\n",
    "files = glob.glob(abs_dir + input_directory)\n",
    "\n",
    "# Sedgwick\n",
    "input_directory = \"Data/PSC/Sedgwick/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "# Taney\n",
    "input_directory = \"Data/PSC/Taney/RBT_RawXML/*/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "dataframe = pd.concat([dataframe, df], ignore_index=True) # dataframe overwrites JQA dataframe with rest.\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in dataframe['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_allCorpora.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ce272",
   "metadata": {},
   "source": [
    "## w2v Corpus Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa96e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
