{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273e6b53",
   "metadata": {},
   "source": [
    "# word2vec Corpus Comparison\n",
    "\n",
    "> After building models and saving them, only the second half of this notebook will be necessary for examining corpora.\n",
    "\n",
    "This notebook first builds w2v models for each corpus separately. Then, it combines texts from all corpora and creates a single model. \n",
    "\n",
    "[Following the same sequence, word vectors can be projected onto 2-dimensional space.]\n",
    "\n",
    "The second half of this notebook includes a Dash application to compare word vectors across space.\n",
    "\n",
    "The results of these word vectors are for exploratory purposes. The way that word2vec weighs and projects vectors into space can make cross-corpora comparisons tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391d523",
   "metadata": {},
   "source": [
    "## Read In Libraries & Function\n",
    "\n",
    "The method for inputting texts into gensim is from Laura Nelson (link?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f468ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, string, glob, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('JQA_XML_parser.py'), '../../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "from JQA_XML_parser import *\n",
    "\n",
    "# Import project-specific functions. \n",
    "# Python files (.py) have to be in same folder to work.\n",
    "lib_path = os.path.abspath(os.path.join(os.path.dirname('Correspondence_XML_parser.py'), '../../Scripts'))\n",
    "sys.path.append(lib_path)\n",
    "from Correspondence_XML_parser import *\n",
    "\n",
    "# Declare absolute path.\n",
    "abs_dir = \"/Users/quinn.wi/Documents/\"\n",
    "\n",
    "# Define tokenizer.\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    punct = string.punctuation + '“' + '”' + '‘' + \"’\"\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    lower_case = lower_case.replace('—', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punct])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10458a67",
   "metadata": {},
   "source": [
    "### Richards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d958c1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "Word total: 3658\n",
      "Unique word total 989\n",
      "CPU times: user 55.8 ms, sys: 14.2 ms, total: 70 ms\n",
      "Wall time: 77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Richards\n",
    "# Declare directory location to shorten filepaths later.\n",
    "input_directory = \"Data/PSC/Richards/ESR-XML-Files-MHS/*.xml\"\n",
    "files = glob.glob(abs_dir + input_directory)\n",
    "\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_richards.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82c944",
   "metadata": {},
   "source": [
    "### Sedgwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7050c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "Word total: 71745\n",
      "Unique word total 7107\n",
      "CPU times: user 2.83 s, sys: 43.4 ms, total: 2.88 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Sedgwick\n",
    "input_directory = \"Data/PSC/Sedgwick/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_sedgwick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c208f1",
   "metadata": {},
   "source": [
    "### Taney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1367f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "Word total: 132090\n",
      "Unique word total 9147\n",
      "CPU times: user 5.81 s, sys: 70.8 ms, total: 5.88 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Taney\n",
    "input_directory = \"Data/PSC/Taney/RBT_RawXML/*/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in df['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_taney.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8746a85",
   "metadata": {},
   "source": [
    "### JQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c99291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word total: 6309943\n",
      "Unique word total 61130\n",
      "CPU times: user 6min 6s, sys: 1.84 s, total: 6min 8s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Declare variables.\n",
    "\"\"\"\n",
    "\n",
    "# Declare regex to simplify file paths below\n",
    "regex = re.compile(r'.*/.*/(.*.xml)')\n",
    "\n",
    "# Declare document level of file. Requires root starting point ('.').\n",
    "doc_as_xpath = './/ns:div/[@type=\"entry\"]'\n",
    "\n",
    "# Declare text level within each document.\n",
    "text_path = './ns:div/[@type=\"docbody\"]/ns:p'\n",
    "\n",
    "\"\"\"\n",
    "Build dataframe.\n",
    "\"\"\"\n",
    "\n",
    "dataframe = []\n",
    "\n",
    "for file in glob.glob(abs_dir + 'Data/PSC/JQA/*/*.xml'):\n",
    "    reFile = str(regex.search(file).group(1))\n",
    "#         Call functions to create necessary variables and grab content.\n",
    "    root = get_root(file)\n",
    "    ns = get_namespace(root)\n",
    "\n",
    "    for eachDoc in root.findall(doc_as_xpath, ns):\n",
    "#             Call functions.\n",
    "        text = get_textContent(eachDoc, text_path, ns)\n",
    "\n",
    "        dataframe.append([text])\n",
    "\n",
    "dataframe = pd.DataFrame(dataframe, columns = ['text'])\n",
    "\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in dataframe['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_jqa.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611cf6f2",
   "metadata": {},
   "source": [
    "### All Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb480d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinn.wi/Documents/Data/PSC/Richards/ESR-XML-Files-MHS/ESR-EDA-1893-09-24.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1803-10-06-toPamelaDwightSedgwickF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1809-01-27-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-25-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-29-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-04-26-toFSWF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1800-01-12-toTheodoreSedgwickIF.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1805-11-15-toPamelaDwightSedgwickFD (1).xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-12-28-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1807-03-24-toFrancesSedgwickWatsonFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1808-11-22-toTheodoreSedgwickIFD.xml \n",
      "\n",
      "/Users/quinn.wi/Documents/Data/PSC/Sedgwick/CMS1806-01-17-toPamelaDwightSedgwickFD.xml \n",
      "\n",
      "Word total: 6442033\n",
      "Unique word total 62201\n",
      "CPU times: user 6min, sys: 1.76 s, total: 6min 1s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Richards\n",
    "# Declare directory location to shorten filepaths later.\n",
    "input_directory = \"Data/PSC/Richards/ESR-XML-Files-MHS/*.xml\"\n",
    "files = glob.glob(abs_dir + input_directory)\n",
    "\n",
    "# Sedgwick\n",
    "input_directory = \"Data/PSC/Sedgwick/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "# Taney\n",
    "input_directory = \"Data/PSC/Taney/RBT_RawXML/*/*.xml\"\n",
    "files = files + glob.glob(abs_dir + input_directory)\n",
    "\n",
    "df = build_dataframe(files)\n",
    "df = df[['text']]\n",
    "\n",
    "dataframe = pd.concat([dataframe, df], ignore_index=True) # dataframe overwrites JQA dataframe with rest.\n",
    "\n",
    "\n",
    "# Convert dataframe text field to list of sentences.\n",
    "sentences = [sentence for text in dataframe['text'] for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "\n",
    "# Get total number of words and unique words.\n",
    "single_list_of_words = []\n",
    "for l in words_by_sentence:\n",
    "    for w in l:\n",
    "        single_list_of_words.append(w)\n",
    "print (f'Word total: {len(single_list_of_words)}\\nUnique word total {len(set(single_list_of_words))}')\n",
    "\n",
    "# Build model.\n",
    "model = gensim.models.Word2Vec(words_by_sentence, window=10, vector_size=300,\n",
    "                               min_count=10, sg=1, alpha=0.025, batch_words=10000, workers=4)\n",
    "\n",
    "# Unused arguments:\n",
    "# size=100, iter=5,\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format(abs_dir + '/Data/Output/WordVectors/w2v_allCorpora.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e014bba",
   "metadata": {},
   "source": [
    "## Dash App. — w2v Corpus Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a851e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
